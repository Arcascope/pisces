{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set discovery using Prefix Trees\n",
    "\n",
    "Data sets are discovered based on being folders within the provided data set root directory which contain subdirectories that start with `cleaned_`.  \n",
    "\n",
    "Once the data sets are discovered, we take the `cleaned_<feature>` subdirectories and use the `<feature>` as the feature name. \n",
    "\n",
    "Then we take the files within the `cleaned_<feature>` subdirectories and discover the ids that data set has for that feature. These do not need to be the same across features, hence all of our data getters might also return `None`.\n",
    "\n",
    "Automagic ID discovery is done using a prefix tree, which is a data structure that allows for efficient searching of strings based on their prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "\n",
    "class SimplifiablePrefixTree:\n",
    "    \"\"\"A standard prefix tree with the ability to \"simplify\" itself by combining nodes with only one child.\n",
    "\n",
    "    These also have the ability to \"flatten\" themselves, which means to convert all nodes at and below a certain depth into leaves on the most recent ancestor of that depth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    delimiter : str\n",
    "        The delimiter to use when splitting words into characters. If empty, the words are treated as sequences of characters.\n",
    "    key : str\n",
    "        The key of the current node in its parent's `.children` dictionary. If empty, the node is (likely) the root of the tree.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    key : str\n",
    "        The key of the current node in its parent's `.children` dictionary. If empty, the node is (likely) the root of the tree.\n",
    "    children : Dict[str, SimplifiablePrefixTree]\n",
    "        The children of the current node, stored in a dictionary with the keys being the children's keys.\n",
    "    is_end_of_word : bool\n",
    "        Whether the current node is the end of a word. Basically, is this a leaf node?\n",
    "    delimiter : str\n",
    "        The delimiter to use when splitting words into characters. If empty, the words are treated as sequences of characters.\n",
    "    print_spacer : str\n",
    "        The string to use to indent the printed tree.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    chars_from(word: str) -> List[str]\n",
    "        Splits a word into characters, using the `delimiter` attribute as the delimiter.\n",
    "    insert(word: str) -> None\n",
    "        Inserts a word into the tree.\n",
    "    search(word: str) -> bool\n",
    "        Searches for a word in the tree.\n",
    "    simplified() -> SimplifiablePrefixTree\n",
    "        Returns a simplified copy of the tree. The original tree is not modified.\n",
    "    simplify() -> SimplifiablePrefixTree\n",
    "        Simplifies the tree in place.\n",
    "    reversed() -> SimplifiablePrefixTree\n",
    "        Returns a reversed copy of the tree, except with with `node.key` reversed versus the node in `self.children`. The original tree is not modified.\n",
    "    flattened(max_depth: int = 1) -> SimplifiablePrefixTree\n",
    "        Returns a Tree identical to `self` up to the given depth, but with all nodes at + below `max_depth` converted into leaves on the most recent acestor of lepth `max_depth - 1`.\n",
    "    _pushdown() -> List[SimplifiablePrefixTree]\n",
    "        Returns a list corresponding to the children of `self`, with `self.key` prefixed to each child's key.\n",
    "    print_tree(indent=0) -> str\n",
    "        Prints the tree, with indentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, delimiter: str = \"\", key: str = \"\"):\n",
    "        self.key = key\n",
    "        self.children: Dict[str, SimplifiablePrefixTree] = {}\n",
    "        self.is_end_of_word = False\n",
    "        self.delimiter = delimiter\n",
    "        self.print_spacer = \"++\"\n",
    "    \n",
    "    def chars_from(self, word: str):\n",
    "        return word.split(self.delimiter) if self.delimiter else word\n",
    "\n",
    "    def insert(self, word: str):\n",
    "        node = self\n",
    "        for char in self.chars_from(word):\n",
    "            if char not in node.children:\n",
    "                node.children[char] = SimplifiablePrefixTree(self.delimiter, key=char)\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "\n",
    "    def search(self, word: str) -> bool:\n",
    "        node = self\n",
    "        for char in self.chars_from(word):\n",
    "            if char not in node.children:\n",
    "                return False\n",
    "            node = node.children[char]\n",
    "        return node.is_end_of_word\n",
    "    \n",
    "    def simplified(self) -> 'SimplifiablePrefixTree':\n",
    "        self_copy = deepcopy(self)\n",
    "        return self_copy.simplify()\n",
    "    \n",
    "    def simplify(self):\n",
    "        if len(self.children) == 1 and not self.is_end_of_word:\n",
    "            child_key = list(self.children.keys())[0]\n",
    "            self.key += child_key\n",
    "            self.children = self.children[child_key].children\n",
    "            self.simplify()\n",
    "        else:\n",
    "            current_keys = list(self.children.keys())\n",
    "            for key in current_keys:\n",
    "                child = self.children.pop(key)\n",
    "                child.simplify()\n",
    "                self.children[child.key] = child\n",
    "        return self\n",
    "    \n",
    "    def reversed(self) -> 'SimplifiablePrefixTree':\n",
    "        rev_self = SimplifiablePrefixTree(self.delimiter, key=self.key[::-1])\n",
    "        rev_self.children = {k[::-1]: v.reversed() for k, v in self.children.items()}\n",
    "        return rev_self\n",
    "    \n",
    "    def flattened(self, max_depth: int = 1) -> 'SimplifiablePrefixTree':\n",
    "        \"\"\"Returns a Tree identical to `self` up to the given depth, but with all nodes at + below `max_depth` converted into leaves on the most recent acestor of lepth `max_depth - 1`.\n",
    "        \"\"\"\n",
    "        flat_self = SimplifiablePrefixTree(self.delimiter, key=self.key)\n",
    "        if max_depth == 0:\n",
    "            if not self.is_end_of_word:\n",
    "                warnings.warn(f\"max_depth is 0, but {self.key} is not a leaf.\")\n",
    "            return flat_self\n",
    "        if max_depth == 1:\n",
    "            for k, v in self.children.items():\n",
    "                if v.is_end_of_word:\n",
    "                    flat_self.children[k] = SimplifiablePrefixTree(self.delimiter, key=k)\n",
    "                else:\n",
    "                    # flattened_children = v._pushdown()\n",
    "                    for flattened_child in v._pushdown():\n",
    "                        flat_self.children[flattened_child.key] = flattened_child\n",
    "        else:\n",
    "            for k, v in self.children.items():\n",
    "                flat_self.children[k] = v.flattened(max_depth - 1)\n",
    "        return flat_self\n",
    "    \n",
    "    def _pushdown(self) -> List['SimplifiablePrefixTree']:\n",
    "        \"\"\"Returns a list corresponding to the children of `self`, with `self.key` prefixed to each child's key.\n",
    "        \"\"\"\n",
    "        pushed_down = [\n",
    "            c\n",
    "            for k in self.children.values()\n",
    "            for c in k._pushdown()\n",
    "        ]\n",
    "        for i in range(len(pushed_down)):\n",
    "            pushed_down[i].key = self.key + self.delimiter + pushed_down[i].key\n",
    "\n",
    "        if not pushed_down:\n",
    "            return [SimplifiablePrefixTree(self.delimiter, key=self.key)]\n",
    "        else:\n",
    "            return pushed_down\n",
    "            \n",
    "\n",
    "    def __str__(self):\n",
    "        # prints .children recursively with indentation\n",
    "        return self.key + \"\\n\" + self.print_tree()\n",
    "\n",
    "    def print_tree(self, indent=0) -> str:\n",
    "        result = \"\"\n",
    "        for key, child in self.children.items():\n",
    "            result +=  self.print_spacer * indent + \"( \" + child.key + \"\\n\"\n",
    "            result += SimplifiablePrefixTree.print_tree(child, indent + 1)\n",
    "        return result\n",
    "\n",
    "\n",
    "class IdExtractor(SimplifiablePrefixTree):\n",
    "    \"\"\"Class extending the prefix trees that incorporates the algorithm for extracting IDs from a list of file names. The algorithm is somewhat oblique, so it's better to just use the `extract_ids` method versus trying to use the prfix trees directly at the call site.\n",
    "    \n",
    "    The algorithm is based on the assumption that the IDs are the same across all file names, but that the file names may have different suffixes. The algorithm reverses the file names, inserts them into the tree, and then simplifes and flattens that tree in order to find the IDs as leaves of that simplified tree.\n",
    "\n",
    "    1. Insert the file name string into the tree, but with each string **reversed**.\n",
    "    2. Simplify the tree, combining nodes with only one child.\n",
    "    3. There may be unexpected suffix matches for these IDs, so we flatten the tree to depth 1, meaning all children of the root are combined to make leaves.\n",
    "    4. The leaves are the IDs we want to extract. However, we must reverse these leaf keys to get the original IDs, since we reversed the file names in step 1.\n",
    "\n",
    "    TODO:\n",
    "    * If we want to find IDs for files with differing prefixes instead, we should instead insert the file names NOT reversed and then NOT reverse in the last step.\n",
    "\n",
    "    * To handle IDs that appear in the middle of file names, we can use both methods to come up with a list of potential IDs based on prefix and suffix, then figure out the \"intersection\" of those lists. (Maybe using another prefix tree?)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, delimiter: str = \"\", key: str = \"\"):\n",
    "        super().__init__(delimiter, key)\n",
    "\n",
    "    def extract_ids(self, files: List[str]) -> List[str]:\n",
    "        for file in files:\n",
    "            self.insert(file[::-1])\n",
    "        return sorted([\n",
    "            c.key for c in self\n",
    "                .prefix_flattened()\n",
    "                .children\n",
    "                .values()\n",
    "        ])\n",
    "    \n",
    "    def prefix_flattened(self) -> 'IdExtractor':\n",
    "        return self.simplified().flattened(1).reversed()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test\n",
    "entries = [\n",
    "    '3XYZabc12',\n",
    "    '3XY&abc12',\n",
    "    '3XYAabc12',\n",
    "    '3XYBabc12',\n",
    "    'MMVQabc12',\n",
    "    'NMVQabc12',\n",
    "]\n",
    "\n",
    "expected_ids = sorted([\n",
    "    '3XYZ',\n",
    "    '3XY&',\n",
    "    '3XYA',\n",
    "    '3XYB',\n",
    "    'MMVQ',\n",
    "    'NMVQ',\n",
    "])\n",
    "\n",
    "id_extractor = IdExtractor()\n",
    "\n",
    "ids = id_extractor.extract_ids(entries)\n",
    "\n",
    "for i, (expected, actual) in enumerate(zip(expected_ids, ids)):\n",
    "    assert expected == actual, f\"Expected {expected}, but got {actual} at index {i}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( 2\n",
      "++( 1\n",
      "++++( c\n",
      "++++++( b\n",
      "++++++++( a\n",
      "++++++++++( Z\n",
      "++++++++++++( Y\n",
      "++++++++++++++( X\n",
      "++++++++++++++++( 3\n",
      "++++++++++( &\n",
      "++++++++++++( Y\n",
      "++++++++++++++( X\n",
      "++++++++++++++++( 3\n",
      "++++++++++( A\n",
      "++++++++++++( Y\n",
      "++++++++++++++( X\n",
      "++++++++++++++++( 3\n",
      "++++++++++( B\n",
      "++++++++++++( Y\n",
      "++++++++++++++( X\n",
      "++++++++++++++++( 3\n",
      "++++++++++( Q\n",
      "++++++++++++( V\n",
      "++++++++++++++( M\n",
      "++++++++++++++++( M\n",
      "++++++++++++++++( N\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "print(id_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc12\n",
      "( 3XYZ\n",
      "( 3XY&\n",
      "( 3XYA\n",
      "( 3XYB\n",
      "( MMVQ\n",
      "( NMVQ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "print(id_extractor.prefix_flattened())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import DefaultDict, Iterable\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "from pisces.utils import determine_header_rows_and_delimiter\n",
    "\n",
    "LOG_LEVEL = logging.INFO\n",
    "\n",
    "class DataSetObject:\n",
    "    FEATURE_PREFIX = \"cleaned_\"\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(LOG_LEVEL)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    def __init__(self, name: str, path: Path):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.ids: List[str] = []\n",
    "\n",
    "        # keeps track of the files for each feature and user\n",
    "        self._feature_map: DefaultDict[str, Dict[str, str]] = defaultdict(dict)\n",
    "        self._feature_cache: DefaultDict[str, Dict[str, pl.DataFrame]] = defaultdict(dict)\n",
    "    \n",
    "    @property\n",
    "    def features(self) -> List[str]:\n",
    "        return list(self._feature_map.keys())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.name}: {self.path}\"\n",
    "\n",
    "    def get_feature_data(self, feature: str, id: str) -> pl.DataFrame | None:\n",
    "        if feature not in self.features:\n",
    "            warnings.warn(f\"Feature {feature} not found in {self.name}. Returning None.\")\n",
    "            return None\n",
    "        if id not in self.ids:\n",
    "            warnings.warn(f\"ID {id} not found in {self.name}\")\n",
    "            return None\n",
    "        if (df := self._feature_cache[feature].get(id)) is None:\n",
    "            file = self.get_filename(feature, id)\n",
    "            if not file:\n",
    "                return None\n",
    "            self.logger.debug(f\"Loading {file}\")\n",
    "            try:\n",
    "                n_rows, delimiter = determine_header_rows_and_delimiter(file)\n",
    "                # self.logger.debug(f\"n_rows: {n_rows}, delimiter: {delimiter}\")\n",
    "                df = pl.read_csv(file, has_header=True if n_rows > 0 else False,\n",
    "                                 skip_rows=max(n_rows-1, 0), \n",
    "                                 separator=delimiter)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Error reading {file}:\\n{e}\")\n",
    "                return None\n",
    "            # sort by time when loading\n",
    "            df.sort(df.columns[0])\n",
    "            self._feature_cache[feature][id] = df\n",
    "        return df\n",
    "\n",
    "    def get_filename(self, feature: str, id: str) -> Path | None:\n",
    "        feature_ids = self._feature_map.get(feature)\n",
    "        if feature_ids is None:\n",
    "            # raise ValueError(f\"Feature {feature_ids} not found in {self.name}\")\n",
    "            print(f\"Feature {feature_ids} not found in {self.name}\")\n",
    "            return None\n",
    "        file = feature_ids.get(id)\n",
    "        if file is None:\n",
    "            # raise ValueError\n",
    "            print(f\"ID {id} not found in {self.name}\")\n",
    "            return None\n",
    "        return self.get_feature_path(feature)\\\n",
    "            .joinpath(file)\n",
    "    \n",
    "    def get_feature_path(self, feature: str) -> Path:\n",
    "        return self.path.joinpath(self.FEATURE_PREFIX + feature)\n",
    "    \n",
    "    def _extract_ids(self, files: List[str]) -> List[str]:\n",
    "        return IdExtractor().extract_ids(files)\n",
    "    \n",
    "    def add_feature_files(self, feature: str, files: Iterable[str]):\n",
    "        if feature not in self.features:\n",
    "            self.logger.debug(f\"Adding feature {feature} to {self.name}\")\n",
    "            self._feature_map[feature] = {}\n",
    "        # use a set for automatic deduping\n",
    "        deduped_ids = set(self.ids)\n",
    "        extracted_ids = sorted(self._extract_ids(files))\n",
    "        files = sorted(list(files))\n",
    "        # print('# extracted_ids:', len(extracted_ids))\n",
    "        for id, file in zip(extracted_ids, files):\n",
    "            # print('adding data for id:', id, 'file:', file)\n",
    "            self._feature_map[feature][id] = file\n",
    "            # set.add only adds the value if it's not already in the set\n",
    "            deduped_ids.add(id)\n",
    "        self.ids = sorted(list(deduped_ids))\n",
    "    \n",
    "    def get_feature_files(self, feature: str) -> Dict[str, str]:\n",
    "        return {k: v for k, v in self._feature_map[feature].items()}\n",
    "    \n",
    "    def get_id_files(self, id: str) -> Dict[str, str]:\n",
    "        return {k: v[id] for k, v in self._feature_map.items()}\n",
    "    \n",
    "    def load_feature_data(self, feature: str | None, id: str | None) -> Dict[str, np.ndarray]:\n",
    "        if feature not in self.features:\n",
    "            raise ValueError(f\"Feature {feature} not found in {self.name}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def find_data_sets(cls, root: str | Path) -> Dict[str, 'DataSetObject']:\n",
    "        set_dir_regex = r\".*\" + cls.FEATURE_PREFIX + r\"(.+)\"\n",
    "        # this regex matches the feature directory name and the data set name\n",
    "        # but doesn't work on Windows (? maybe, cant test) because of the forward slashes\n",
    "        feature_dir_regex = r\".*/(.+)/\" + cls.FEATURE_PREFIX + r\"(.+)\"\n",
    "\n",
    "        data_sets: Dict[str, DataSetObject] = {}\n",
    "        for root, dirs, files in os.walk(root, followlinks=True):\n",
    "            # check to see if the root is a feature directory,\n",
    "            # if it is, add that feature data to the data set object,\n",
    "            # creating a new data set object if necessary.\n",
    "            if (root_match := re.match(feature_dir_regex, root)):\n",
    "                cls.logger.debug(f\"Feature directory: {root}\")\n",
    "                cls.logger.debug(f\"data set name: {root_match.group(1)}\")\n",
    "                cls.logger.debug(f\"feature is: {root_match.group(2)}\", )\n",
    "                data_set_name = root_match.group(1)\n",
    "                feature_name = root_match.group(2)\n",
    "                if (data_set := data_sets.get(data_set_name)) is None:\n",
    "                    data_set = DataSetObject(root_match.group(1), Path(root).parent)\n",
    "                    data_sets[data_set.name] = data_set\n",
    "                files = [f for f in files if not f.startswith(\".\") and not f.endswith(\".tmp\")]\n",
    "                data_set.add_feature_files(feature_name, files)\n",
    "        \n",
    "        return data_sets\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def psg_to_sleep_wake(psg: pl.DataFrame) -> np.array:\n",
    "    # map all positive classes to 1 (sleep)\n",
    "    # retain all 0 (wake) and -1 (mask) classes\n",
    "    return np.where(psg[:, 1] > 0, 1, psg[:, 1])\n",
    "\n",
    "def get_activity_X_PSG_y(data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "    activity_0 = data_set.get_feature_data(\"activity\", id)\n",
    "    psg_0 = data_set.get_feature_data(\"psg\", id)\n",
    "\n",
    "    if activity_0 is None or psg_0 is None:\n",
    "        return None\n",
    "\n",
    "    # trim the activity and psg data to both end when the 0th column (time) of either ends\n",
    "    end_time = min(activity_0[-1, 0], psg_0[-1, 0])\n",
    "    rows_retained = sum(activity_0[:, 0] <= end_time)\n",
    "    activity_0 = activity_0.filter(activity_0[:, 0] <= end_time)\n",
    "    psg_0 = psg_0.filter(psg_0[:, 0] <= end_time)\n",
    "\n",
    "    X = activity_0[:, 1].to_numpy()\n",
    "    # make the reshape(-1, 2) non-ragged \n",
    "    # remove the last element if the length is odd\n",
    "    if res := X.shape[0] % 2:\n",
    "        #! X[:-0] is empty, so don't do this if res == 0\n",
    "        X = X[:-res] \n",
    "    X = X.reshape((-1, 2)).sum(axis=1)\n",
    "    y = psg_to_sleep_wake(psg_0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def rolling_window(arr, window_size):\n",
    "    strided_axis_0 = max(arr.shape[0] - window_size + 1, 0)\n",
    "    arr_strided = as_strided(arr, shape=(strided_axis_0, window_size), strides=(arr.strides[0], arr.strides[0]))\n",
    "    return arr_strided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test\n",
    "input_vec = np.arange(10)\n",
    "expected_rolling = np.array([[0, 1, 2],\n",
    "       [1, 2, 3],\n",
    "       [2, 3, 4],\n",
    "       [3, 4, 5],\n",
    "       [4, 5, 6],\n",
    "       [5, 6, 7],\n",
    "       [6, 7, 8],\n",
    "       [7, 8, 9]])\n",
    "rolled = rolling_window(input_vec, 3)\n",
    "\n",
    "\n",
    "assert np.array_equal(rolled, expected_rolling), f\"Expected {expected_rolling}, but got {rolled}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def apply_gausian_filter(df: pl.DataFrame, sigma: float = 1.0, overwrite: bool = False) -> pl.DataFrame:\n",
    "    data_columns = df.columns[1:]  # Adjust this to match your data column indices\n",
    "\n",
    "    # Specify the standard deviation of the Gaussian kernel\n",
    "    sigma = 1.0  # This controls the smoothing. Adjust based on your data's sampling rate and desired smoothing\n",
    "\n",
    "    # Apply Gaussian smoothing to each data column\n",
    "    for col in data_columns:\n",
    "        new_col_name = f\"{col}_smoothed\" if not overwrite else col\n",
    "        df = df.with_columns(\n",
    "            pl.Series(gaussian_filter1d(df[col].to_numpy(), sigma)).alias(new_col_name)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def fill_gaps_in_accelerometer_data(acc: pl.DataFrame, smooth: bool = False) -> pl.DataFrame:\n",
    "    # Calculate the time difference in seconds to get the 50 Hz rate\n",
    "    sampling_rate_hz = 50\n",
    "    sampling_period_s = 1 / sampling_rate_hz\n",
    "    # Step 0: Save the original 'timestamp' column as 'timestamp_raw'\n",
    "    acc = acc.with_columns(acc[acc.columns[0]].alias('timestamp_raw'))\n",
    "\n",
    "    # Step 1: Calculate the start time and round down each 'timestamp_raw' to the nearest 0.02 sec from the start time\n",
    "    start_time = acc['timestamp_raw'].min()\n",
    "    acc = acc.with_columns(\n",
    "        (((acc['timestamp_raw'] - start_time) / sampling_period_s).floor() * sampling_period_s + start_time)\n",
    "        .alias('timestamp')\n",
    "    )\n",
    "\n",
    "\n",
    "    # Generate the new timestamps starting from the first timestamp to the last, at 50 Hz\n",
    "    start_time = acc['timestamp'].min()\n",
    "    end_time = acc['timestamp'].max() + sampling_period_s  # Add one more period to include the last timestamp\n",
    "    new_timestamps = np.arange(start_time, end_time, sampling_period_s)\n",
    "\n",
    "    # Create a new DataFrame with these timestamps\n",
    "    new_df = pl.DataFrame({'timestamp': new_timestamps})\n",
    "\n",
    "    # Join the original DataFrame with the new one based on the closest timestamp\n",
    "    # This assumes timestamps are unique. If not, you might need to handle duplicates.\n",
    "    acc_resampled = new_df.join(acc, on='timestamp', how='left').with_columns([\n",
    "        # Fill missing data with zeros\n",
    "        pl.col('*').exclude('timestamp').fill_null(0)\n",
    "    ])\n",
    "\n",
    "    if smooth:\n",
    "        acc_resampled = apply_gausian_filter(acc_resampled, overwrite=True)\n",
    "\n",
    "    return acc_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10bf5c2b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA46klEQVR4nO3deXRU9f3/8ddkQhJCMmExK4ZVBAFBo5AT3AADESlKDxVKKaCldYNvxSglVPlGvrYlCEVrtUgpirXKIhX1pwgSFGwBQQxQWUR2UMhCgCRsgczc3x8xU8YkkEnurHk+zpkj87mfO/f9mYm5r9x753MthmEYAgAAMEGIrwsAAADBg2ABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADBNqLc36HA4dPToUUVHR8tisXh78wAAoB4Mw1BZWZmSkpIUElL7cQmvB4ujR48qOTnZ25sFAAAmOHLkiK6++upal3s9WERHR0uqLMxms3l78wAAoB5KS0uVnJzs3I/XxuvBour0h81mI1gAABBgrnQZAxdvAgAA0xAsAACAaQgWAADANF6/xqIu7Ha7Ll686Osy4CVNmjSR1Wr1dRkAABP4XbA4ffq0vv32WxmG4etS4CUWi0VXX321oqKifF0KAKCB/CpY2O12ffvtt4qMjFRsbCwTaDUChmGoqKhI3377rTp16sSRCwAIcH4VLC5evCjDMBQbG6umTZv6uhx4SWxsrA4ePKiLFy8SLAAgwPnlxZscqWhc+LwBIHj41RELAABwBQ67dGi9dLpA9mZx2mTvosIzFxUXHaHe7VvKGuLbP9b88ohFY3Pw4EFZLBZt3bq1zussWLBAzZs393kdAAAv2vm+9EJ36fUfSf8cJ+vfh6jtP1K1fMlfNXLe57p1xidasf2YT0skWJjoyJEj+sUvfqGkpCSFhYWpbdu2euyxx1RcXHzZ9ZKTk3Xs2DF17969ztsaMWKEvvnmm4aWDAAIFDvfl5aMkUqPujQn6ITmNHlBGSGblF9yXo/8I8+n4YJgYZL9+/fr5ptv1p49e7Rw4ULt3btXr7zyilavXq20tDSdOHGixvUuXLggq9WqhIQEhYbW/cxU06ZNFRcXZ1b5AAB/5rBLKyZLqj4VQ9WZj+wmb8gihyRp2v/bKbvDN9M2BGWwsDsMbdhXrPe2fqcN+4q98uaOHz9eYWFh+vjjj3XHHXeoTZs2GjRokHJzc/Xdd9/pqaeekiS1a9dOzz77rMaMGSObzaYHH3ywxlMQ77//vjp16qSIiAj169dPr7/+uiwWi06dOiWp+qmQZ555RjfccIPeeOMNtWvXTjExMfrpT3+qsrIyZ58VK1bo1ltvVfPmzdWqVSv96Ec/0r59+zz+3gAAGujQ+mpHKi4VYpGSLMXqHfK1DEnHSs5r04Ga/6D1tKALFiu2H9OtMz7RyHmf67FFW71yzunEiRNauXKlHn300Wpfk01ISNCoUaO0ePFi56Rfs2bNUs+ePbVlyxZNnTq12usdOHBAP/nJTzR06FBt27ZNDz30kDOYXM6+ffv07rvv6oMPPtAHH3ygtWvXKicnx7n8zJkzyszM1ObNm7V69WqFhIToxz/+sRwORwPfAQCAR50uqFO3OJ1y/ruw7LyHirm8oPpWyIrtx/TIP/KqHSiqOuc05+cpuqt7ounb3bNnjwzD0HXXXVfj8uuuu04nT55UUVGRJKl///564oknnMsPHjzo0n/u3Lnq3LmzZs6cKUnq3Lmztm/frt///veXrcPhcGjBggWKjo6WJI0ePVqrV692rjds2DCX/q+++qpiY2O1c+dOt67vAAB4WVR8nboVqrnz33HRER4q5vKC5oiF3WFo2v/bWcPZp/+ekfL0Oae6TkN+8803X3b57t271atXL5e23r17X/F127Vr5wwVkpSYmKjCwkLn8z179mjkyJHq0KGDbDab2rVrJ0k6fPhwneoGAPhI2z6SLUlSzV8ldRjSUaOVNjm6yCIpMabyq6e+EDTBYtOBEzpWUvthH0+ec7rmmmtksVi0a9euGpfv2rVLLVq0UGxsrCSpWbNmptcgVd7M61IWi8XlNMeQIUN04sQJzZs3Txs3btTGjRslVV5ACgDwYyFW6a4Z3z9xDRdVfy9Puzhaxve79ewhXX02n0XQBIu6nkvyxDmnVq1aacCAAfrLX/6ic+fOuSzLz8/Xm2++qREjRtR5hsnOnTtr8+bNLm1ffPFFg2osLi7W7t279fTTT+vOO+90np4BAASIrvdIw/8u2VxP6eerlR65OFErHb2VEBPhsdP+dRU011jU9VySp845vfTSS+rTp48yMjL0u9/9Tu3bt9eOHTs0adIktW7d+orXR1zqoYce0uzZszV58mSNGzdOW7du1YIFCyTVf/rrFi1aqFWrVvrrX/+qxMREHT58WFlZWfV6LQCAj3S9R+oy2GXmzUP2Lrr7zEXdz8yb5urdvqUSYyJqOfskj59z6tSpkzZv3qwOHTpo+PDh6tixox588EH169dPGzZsUMuWdd9u+/bttXTpUr3zzjvq0aOH5syZ4/xWSHh4eL3qCwkJ0aJFi/Tll1+qe/fuevzxx50XhwIAAkiIVWp/m3T9T2TtcLvSOsXp3htaK61jK5+HCkmyGHW94tAkpaWliomJUUlJiWw2m8uy8+fP68CBA2rfvr0iItw/slD1rRDJdQqRqrfZ14eHGuL3v/+9XnnlFR05csTXpZiuoZ87AMDzLrf/vlTQHLGQpLu6J2rOz1OUEOO6c/KHc07u+stf/qIvvvhC+/fv1xtvvKGZM2dq7Nixvi4LAIDLCpprLKrc1T1RA7omaNOBEyosO+83d3tz1549e/S73/1OJ06cUJs2bfTEE09oypQpvi4LAIDLCrpgIUnWEIvSOrbydRkN8vzzz+v555/3dRkAALglqE6FAAAA3yJYAAAA0xAsAACAaQgWAADANG4Fi2eeeUYWi8Xl0aVLF0/VBgAAAozb3wrp1q2bcnNz//sCoUH5xRIAAFAPbp8KCQ0NVUJCgvNx1VVXeaIu+IGDBw/KYrFo69atDXqdvn37auLEiabUBADwb24Hiz179igpKUkdOnTQqFGjdPjw4cv2Ly8vV2lpqcsj2Nx///3OU0NNmjRR+/bt9Zvf/Ebnz5t/J1UAAPyZW8EiNTVVCxYs0IoVKzRnzhwdOHBAt912m8rKympdZ/r06YqJiXE+kpOTG1z0FTns0oF/SV8trfyvw+7xTd511106duyY9u/fr+eff15z585Vdna2x7cLAIA/cStYDBo0SPfdd5969OihjIwMLV++XKdOndKSJUtqXWfKlCkqKSlxPjx+E62d70svdJde/5H0z3GV/32he2W7B4WHhyshIUHJyckaOnSo0tPTtWrVKkmSw+HQ9OnT1b59ezVt2lQ9e/bU0qVLneuePHlSo0aNUmxsrJo2bapOnTrptddecy6fPHmyrr32WkVGRqpDhw6aOnWqLl686Fz+zDPP6IYbbtCrr76qNm3aKCoqSo8++qjsdruee+45JSQkKC4urtqt2y0Wi+bMmaNBgwapadOm6tChg0tdNdm+fbsGDRqkqKgoxcfHa/To0Tp+/Lhz+ZkzZzRmzBhFRUUpMTFRf/zjHxv0vgIAAkuDvm7avHlzXXvttdq7d2+tfcLDw2Wz2VweHrPzfWnJGKn0qGt76bHKdg+Hiyrbt2/X+vXrFRYWJqnyqM3f//53vfLKK9qxY4cef/xx/fznP9fatWslSVOnTtXOnTv10UcfadeuXZozZ47LtSvR0dFasGCBdu7cqT/96U+aN29etem+9+3bp48++kgrVqzQwoULNX/+fA0ePFjffvut1q5dqxkzZujpp5/Wxo0bXdabOnWqhg0bpm3btmnUqFH66U9/ql27dtU4rlOnTql///668cYbtXnzZq1YsUIFBQUaPny4s8+kSZO0du1avffee/r444+1Zs0a5eXlmfK+AgACgNEAZWVlRosWLYw//elPdV6npKTEkGSUlJRUW3bu3Dlj586dxrlz59wvxl5hGH/sYhjZtloeMYbxx+sq+5ls7NixhtVqNZo1a2aEh4cbkoyQkBBj6dKlxvnz543IyEhj/fr1LuuMGzfOGDlypGEYhjFkyBDjgQceqPP2Zs6cadx0003O59nZ2UZkZKRRWlrqbMvIyDDatWtn2O12Z1vnzp2N6dOnO59LMh5++GGX105NTTUeeeQRwzAM48CBA4YkY8uWLYZhGMazzz5rDBw40KX/kSNHDEnG7t27jbKyMiMsLMxYsmSJc3lxcbHRtGlT47HHHqt1PA363AEAXnG5/fel3Pqu6JNPPqkhQ4aobdu2Onr0qLKzs2W1WjVy5EjzE4+7Dq2vfqTChSGVflfZr/1tpm++X79+mjNnjs6cOaPnn39eoaGhGjZsmHbs2KGzZ89qwIABLv0vXLigG2+8UZL0yCOPaNiwYcrLy9PAgQM1dOhQ9enTx9l38eLFevHFF7Vv3z6dPn1aFRUV1Y78tGvXTtHR0c7n8fHxslqtCgkJcWkrLCx0WS8tLa3a89q+BbJt2zZ9+umnioqKqrZs3759OnfunC5cuKDU1FRne8uWLdW5c+caXw8AEHzcChbffvutRo4cqeLiYsXGxurWW2/V559/rtjYWE/VV3enC8zt56ZmzZrpmmuukSS9+uqr6tmzp+bPn6/u3btLkj788EO1bt3aZZ3w8HBJldeuHDp0SMuXL9eqVat05513avz48Zo1a5Y2bNigUaNGadq0acrIyFBMTIwWLVpU7dqFJk2auDyv+obKD9scDke9x3j69GkNGTJEM2bMqLYsMTHxsqfEAACNg1vBYtGiRZ6qo+Gi4s3t1wAhISH67W9/q8zMTH3zzTcKDw/X4cOHdccdd9S6TmxsrMaOHauxY8fqtttu06RJkzRr1iytX79ebdu21VNPPeXse+jQIdNq/fzzzzVmzBiX51VHUn4oJSVF//znP9WuXbsaJ0br2LGjmjRpoo0bN6pNmzaSKi9M/eabby47dgBA8AieaTPb9pFsSZUXasqooYOlcnnbPjUsM999992nSZMmae7cuXryySf1+OOPy+Fw6NZbb1VJSYnWrVsnm82msWPH6n//93910003qVu3biovL9cHH3yg6667TpLUqVMnHT58WIsWLVKvXr304YcfatmyZabV+fbbb+vmm2/WrbfeqjfffFObNm3S/Pnza+w7fvx4zZs3TyNHjtRvfvMbtWzZUnv37tWiRYv0t7/9TVFRURo3bpwmTZqkVq1aKS4uTk899ZTL6RgAQHALnmARYpXumlH57Q9Z5BouLJX/uSunsp8XhIaGasKECXruued04MABxcbGavr06dq/f7+aN2+ulJQU/fa3v5UkhYWFacqUKTp48KCaNm2q2267zXl06J577tHjjz+uCRMmqLy8XIMHD9bUqVP1zDPPmFLntGnTtGjRIj366KNKTEzUwoUL1bVr1xr7JiUlad26dZo8ebIGDhyo8vJytW3bVnfddZczPMycOdN5yiQ6OlpPPPGESkpKTKkVAOD/LIZh1PTnvceUlpYqJiZGJSUl1S5APH/+vA4cOKD27dsrIiKifhvY+b60YrLrhZy21pWhous9Dag8+FgsFi1btkxDhw71aR2mfO4AAI+63P77UsFzxKJK13ukLoMrv/1xuqDymoq2fbx2pAIAgMYs+IKFVBkiPPCVUgAAcHnBGSxQJ14+CwYAaAS4XB8AAJiGYAEAAEzDqRAAAIKAvaJCX29cqXMnv1PTFq3VJTVD1homM/Q0ggUAAAFuy8rXlbRhmrqp2NlWsKqVjqZl68aMsV6thWABAEAA27LydfVc/+vKJ5b/tscaxYpd/2ttkbwaLrjGAgCAAGWvqFDShmmSpBCL67Kq54kbpsleUeG1mggWjZTFYtG7775r+uv27dtXEydONP11AQDVfb1xpeJVXC1UVAmxSAkq1tcbV3qtJoKFSYqKivTII4+oTZs2Cg8PV0JCgjIyMrRu3Tqf1vXMM8/ohhtu8GkNAADPOHfyO1P7mSEor7GwO+zKK8xT0dkixUbGKiUuRVYPT+k9bNgwXbhwQa+//ro6dOiggoICrV69WsXFxVdeGQCAemjaorWp/cwQdEcscg/lKuOfGfrFyl9o8r8m6xcrf6GMf2Yo91Cux7Z56tQp/etf/9KMGTPUr18/tW3bVr1799aUKVN0zz2VNz6zWCyaO3eufvSjHykyMlLXXXedNmzYoL1796pv375q1qyZ+vTpo3379rm89pw5c9SxY0eFhYWpc+fOeuONN1yWHz58WPfee6+ioqJks9k0fPhwFRQUSJIWLFigadOmadu2bbJYLLJYLFqwYIFz3ePHj+vHP/6xIiMj1alTJ73//vsur719+3YNGjRIUVFRio+P1+jRo3X8+HHn8jNnzmjMmDGKiopSYmKi/vjHP5r5tgIArqBLaoYK1EoOQ7JL+iIiXMubReqLiHDZJTkMKV+t1CU1w2s1BVWwyD2Uq8w1mSo4W+DSXni2UJlrMj0WLqKiohQVFaV3331X5eXltfZ79tlnNWbMGG3dulVdunTRz372Mz300EOaMmWKNm/eLMMwNGHCBGf/ZcuW6bHHHtMTTzyh7du366GHHtIDDzygTz/9VJLkcDh077336sSJE1q7dq1WrVql/fv3a8SIEZKkESNG6IknnlC3bt107NgxHTt2zLlMqrxl+vDhw/Wf//xHd999t0aNGqUTJ05IqgxL/fv314033qjNmzdrxYoVKigo0PDhw53rT5o0SWvXrtV7772njz/+WGvWrFFeXp6p7y0AoHbW0FAdTctWbmRTZSQn6ReJ8Zocd5V+kRivjOQk5UY21bG0bO/OZ2F4WUlJiSHJKCkpqbbs3Llzxs6dO41z5865/boV9grjziV3Gt0XdK/xcf2C6430JelGhb3CjGFUs3TpUqNFixZGRESE0adPH2PKlCnGtm3bnMslGU8//bTz+YYNGwxJxvz5851tCxcuNCIiIpzP+/TpY/zqV79y2c59991n3H333YZhGMbHH39sWK1W4/Dhw87lO3bsMCQZmzZtMgzDMLKzs42ePXtWq/eH9Zw+fdqQZHz00UeGYRjGs88+awwcONBlnSNHjhiSjN27dxtlZWVGWFiYsWTJEufy4uJio2nTpsZjjz12xffrUg353AGgsVt1cFXlvu61bq77vu+frzq4ypTtXG7/famgOWKRV5hX7UjFpQwZyj+br7xCz/xFPWzYMB09elTvv/++7rrrLq1Zs0YpKSkupx569Ojh/Hd8fLwk6frrr3dpO3/+vEpLSyVJu3bt0i233OKynVtuuUW7du1yLk9OTlZycrJzedeuXdW8eXNnn8u5tJ5mzZrJZrOpsLBQkrRt2zZ9+umnzqMxUVFR6tKliyRp37592rdvny5cuKDU1FTna7Rs2VKdO3e+4nYBAOawO+zK2ZRT+cTyg6+GWCyyyKIZm2bI7rB7raagCRZFZ4tM7VcfERERGjBggKZOnar169fr/vvvV3Z2tnN5kyZNnP+2fP8DUFObw+HwWI2XunTbVduv2vbp06c1ZMgQbd261eWxZ88e3X777V6pDwBweb7+o7omQRMsYiNjTe1nhq5du+rMmTP1Xv+6666r9nXVdevWqWvXrs7lR44c0ZEjR5zLd+7cqVOnTjn7hIWFyW53P6mmpKRox44dateuna655hqXR7NmzdSxY0c1adJEGzdudK5z8uRJffPNN/UZKgCgHvzhj+ofCppgkRKXovjIeFlU8ywhFlmUEJmglLgU07ddXFys/v376x//+If+85//6MCBA3r77bf13HPP6d577633606aNEkLFizQnDlztGfPHs2ePVvvvPOOnnzySUlSenq6rr/+eo0aNUp5eXnatGmTxowZozvuuEM333yzJKldu3Y6cOCAtm7dquPHj1/24tJLjR8/XidOnNDIkSP1xRdfaN++fVq5cqUeeOAB2e12RUVFady4cZo0aZI++eQTbd++Xffff79CQoLmRwoA/J4//lEdNHsBa4hVWb2zJKlauKh6Prn3ZI/MZxEVFaXU1FQ9//zzuv3229W9e3dNnTpVv/rVr/TSSy/V+3WHDh2qP/3pT5o1a5a6deumuXPn6rXXXlPfvn0lVZ66eO+999SiRQvdfvvtSk9PV4cOHbR48WLnawwbNkx33XWX+vXrp9jYWC1cuLBO205KStK6detkt9s1cOBAXX/99Zo4caKaN2/uDA8zZ87UbbfdpiFDhig9PV233nqrbrrppnqPFwDgHl/+UV0bi2EYhte2Jqm0tFQxMTEqKSmRzWZzWXb+/HkdOHBA7du3V0RERL1eP/dQrnI25bicc0qITNDk3pOV3ja9QbXDM8z43AGgsaqaakGqvKaiSlXYmN13tin7v8vtvy8VdDNvprdNV7/kfl6feRMAAF9Ib5uu2X1nV/ujOj4y3id/VAddsJAqT4v0Sujl6zIAAPAKf/qjOiiDBQAAjY2//FEdNBdvAgAA3yNYAAAA0/hlsPDyF1XgY3zeABA8/CpYWK2VF5lcuHDBx5XAm6o+76rPHwAQuPzq4s3Q0FBFRkaqqKhITZo0YRbHRsDhcKioqEiRkZEK9eZtfQEAHuFXv8ktFosSExN14MABHTp0yNflwEtCQkLUpk0b503YAACBy6+ChVR506xOnTpxOqQRCQsL4+gUAAQJvwsWUuVfsEztDABA4OHPRAAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADBNg4JFTk6OLBaLJk6caFI5AAAgkNU7WHzxxReaO3euevToYWY9AAAggNUrWJw+fVqjRo3SvHnz1KJFC7NrAgAAAapewWL8+PEaPHiw0tPTr9i3vLxcpaWlLg8AABCcQt1dYdGiRcrLy9MXX3xRp/7Tp0/XtGnT3C4MAAAEHreOWBw5ckSPPfaY3nzzTUVERNRpnSlTpqikpMT5OHLkSL0KBQAA/s9iGIZR187vvvuufvzjH8tqtTrb7Ha7LBaLQkJCVF5e7rKsJqWlpYqJiVFJSYlsNlv9KwcAAF5T1/23W6dC7rzzTn311VcubQ888IC6dOmiyZMnXzFUAACA4OZWsIiOjlb37t1d2po1a6ZWrVpVawcAAI0PM28CAADTuP2tkB9as2aNCWUAAIBgwBELAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANwQIAAJiGYAEAAExDsAAAAKYhWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAME2orwsAAAB1Z3fYlVeYp6KzRYqNjFVKXIqsIVZfl+VEsAAAIEDkHspVzqYcFZwtcLbFR8Yrq3eW0tum+7Cy/+JUCAAAASD3UK4y12S6hApJKjxbqMw1mco9lOujylwRLAAA8HN2h105m3JkyKi2rKptxqYZsjvs3i6tGoIFAAB+Lq8wr9qRiksZMpR/Nl95hXlerKpmBAsAAPxc0dkiU/t5EsECAAA/FxsZa2o/TyJYAADg51LiUhQfGS+LLDUut8iihMgEpcSleLmy6oI6WNgrKrRj3Yfa/MFftWPdh7JXVPi6JAAA3GYNsSqrd5YkVQsXVc8n957sF/NZWAzDqH6JqQeVlpYqJiZGJSUlstlsHtvOlpWvK2nDNMWr2NlWoFY6mpatGzPGemy7AAB4Sk3zWCREJmhy78ken8eirvvvoAwWW1a+rp7rfy1JCrkk2Dm+H+m2Pi8SLgAAAclXM2/Wdf8ddDNv2isqlLRhmiTXUFH13GFIiRumyX7nKFlDg274AIAgZw2xqldCL1+XUaugu8bi640rFa/iaqGiSohFSlCxvt640ruFAQDQCARdsDh38jtT+wEAgLoLumDRtEVrU/sBAIC6C7pg0SU1QwVq5bxQ84cchpSvVuqSmuHdwgAAaASCLlhYQ0N1NC1bkqqFi6rnx9KyuXATAAAPCLpgIUk3ZozVtj4vqsjSyqW90NKKr5oCAOBBQTmPRRV7RYW+3rhS505+p6YtWqtLagZHKgAAqIdGO4/Fpayhoep2y2BflwEAQKMRlKdCAACAbxAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANwQIAAJjGrWAxZ84c9ejRQzabTTabTWlpafroo488VRsAAAgwbgWLq6++Wjk5Ofryyy+1efNm9e/fX/fee6927NjhqfoAAEAAsRiGYTTkBVq2bKmZM2dq3LhxdepfWlqqmJgYlZSUyGazNWTTAADAS+q6/w6t7wbsdrvefvttnTlzRmlpabX2Ky8vV3l5uUthAAAgOLl98eZXX32lqKgohYeH6+GHH9ayZcvUtWvXWvtPnz5dMTExzkdycnKDCgYAAP7L7VMhFy5c0OHDh1VSUqKlS5fqb3/7m9auXVtruKjpiEVycjKnQgAACCB1PRXS4Gss0tPT1bFjR82dO9fUwgAAgP+o6/67wfNYOBwOlyMSAACg8XLr4s0pU6Zo0KBBatOmjcrKyvTWW29pzZo1WrlypafqAwAAAcStYFFYWKgxY8bo2LFjiomJUY8ePbRy5UoNGDDAU/UBAIAA4lawmD9/vqfqAAAAQYB7hQAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANwQIAAJiGYAEAAExDsAAAAKYhWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwTaivC/Aku8OuvMI8FZ0tUmxkrFLiUmQNsfq6LAAAglbQBovcQ7nK2ZSjgrMFzrb4yHhl9c5Sett0H1YGAEDwCspTIbmHcpW5JtMlVEhS4dlCZa7JVO6hXB9VBgBAcAu6YGF32JWzKUeGjGrLqtpmbJohu8Pu7dIAAAh6QRcs8grzqh2puJQhQ/ln85VXmOfFqgAAaByCLlgUnS0ytR8AAKi7oAsWsZGxpvYDAAB1F3TBIiUuRfGR8bLIUuNyiyxKiExQSlyKlysDACD4BV2wsIZYldU7S5KqhYuq55NuelJfb1ihzR/8VTvWfSh7RYXX6wQAIBhZDMOo/vUJDyotLVVMTIxKSkpks9k8tp2a5rFIiEzQfWG9de+WhYpXsbO9QK10NC1bN2aM9Vg9AAAEsrruv4M2WEjVZ94M2bpdN26YKEkKueRghuP7d2BbnxcJFwAA1KCu+++gnXlTqjwt0iuhlyTJXlGh45+PkOQaKqqeOwwpccM02e8cJWtoUL8tAAB4TNBdY1GbrzeuVLyKq4WKKiEWKUHF+nrjSu8WBgBAEGk0weLcye9M7QcAAKprNMGiaYvWpvYDAADVNZpg0SU1QwVq5bxQ84cchpSvVuqSmuHdwgAACCJuBYvp06erV69eio6OVlxcnIYOHardu3d7qjZTWUNDdTQtW5KqhYuq58fSsrlwEwCABnArWKxdu1bjx4/X559/rlWrVunixYsaOHCgzpw546n6THVjxlht6/OiiiytXNoLLa34qikAACZo0DwWRUVFiouL09q1a3X77bfXaR1vzmNRG3tFhb7euFLnTn6npi1aq0tqBkcqAAC4DK/MY1FSUiJJatmyZa19ysvLVV5e7lKYr1lDQ9XtlsG+LgMAgKBT74s3HQ6HJk6cqFtuuUXdu3evtd/06dMVExPjfCQnJ9d3kwAAwM/V+1TII488oo8++kj//ve/dfXVV9far6YjFsnJyaaeCvnh1N0pcSmyhlhNeW0AAODhUyETJkzQBx98oM8+++yyoUKSwsPDFR4eXp/N1ElNNxuLj4xXVu8spbdN99h2AQBAdW6dCjEMQxMmTNCyZcv0ySefqH379p6qq05yD+Uqc02mS6iQpMKzhcpck6ncQ7k+qgwAgMbJrWAxfvx4/eMf/9Bbb72l6Oho5efnKz8/X+fOnfNUfbWyO+zK2ZQjQ9XP5FS1zdg0Q3aH3dulAQDQaLkVLObMmaOSkhL17dtXiYmJzsfixYs9VV+t8grzqh2puJQhQ/ln85VXmOfFqgAAaNzcusaiAVNemK7obJGp/QAAQMMF7L1CYiNjTe0HAAAaLmCDRUpciuIj42WRpcblFlmUEJmglLgUL1cGAEDjFbDBwhpiVVbvLEmqFi6qnk/uPZn5LAAA8KKADRaSlN42XbP7zlZcZJxLe3xkvGb3nc08FgAAeFnA33krvW26+iX3Y+ZNAAD8QMAHC6nytEivhF517s8U4AAAeEZQBAt3MAU4AACeE9DXWLiLKcABAPCsRhMsmAIcAADPazTBginAAQDwvEYTLJgCHAAAz2s0wYIpwAEA8LxGEyyYAhwAAM9rNMGCKcABAPC8RhMsJKYABwDA0xrdBFlMAQ4AgOc0umAhuT8FOAAA3haot59olMECAAB/Fsi3n2hU11gAAODvAv32EwQLAAD8RDDcfoJgAQCAnwiG208QLAAA8BPBcPsJggUAAH4iGG4/QbAAAMBPBMPtJwgWAAD4iWC4/QTBAgAAPxLot59ggiwAAPxMIN9+gmABAIAfCtTbT3AqBAAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMExRTetsrKvT1xpU6d/I7NW3RWl1SM2QNDYqhAQAQUAJ+77tl5etK2jBN3VTsbCtY1UpH07J1Y8ZYH1YGAEDjE9DBYsvK19Vz/a8rn1xy2/pYo1ix63+tLRLhAgAALwrYayzsFRVK2jBNkhRicV1W9TxxwzTZKyq8XBkAAI1XwAaLrzeuVLyKq4WKKiEWKUHF+nrjSu8WBgBAIxawweLcye9M7QcAABouYINF0xatTe0HAAAaLmCDRZfUDBWolRxGzcsdhpSvVuqSmuHdwgAAaMQCNlhYQ0N1NC1bkqqFi6rnx9Kymc8CAAAvCthgIVV+lXRbnxdVZGnl0l5oaaVtfV7kq6YAAHiZxTCMWk4meEZpaaliYmJUUlIim81mymsy8yYAAJ5V1/13UOx9raGh6nbLYF+XAQBAoxfQp0IAAIB/IVgAAADTuB0sPvvsMw0ZMkRJSUmyWCx69913PVAWAAAIRG4HizNnzqhnz556+eWXPVEPAAAIYG5fvDlo0CANGjTIE7UAAIAA5/FvhZSXl6u8vNz5vLS01NObBAAAPuLxizenT5+umJgY5yM5OdnTmwQAAD7i8WAxZcoUlZSUOB9Hjhzx9CYBAICPePxUSHh4uMLDwz29GQAA4AeYxwIAAJjG7SMWp0+f1t69e53PDxw4oK1bt6ply5Zq06aNqcUBAIDA4naw2Lx5s/r16+d8npmZKUkaO3asFixYYFphAAAg8LgdLPr27Ssv3xAVAAAECK6xAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYxuP3CgkkdoddeYV5KjpbpNjIWKXEpcgaYvV1WQAABAyCxfdyD+UqZ1OOCs4WONviI+OV1TtL6W3TfVgZAACBg1MhqgwVmWsyXUKFJBWeLVTmmkzlHsr1UWUAAASWRh8s7A67cjblyFD1acqr2mZsmiG7w+7t0gAACDiNPljkFeZVO1JxKUOG8s/mK68wz4tVAQAQmBp9sCg6W2RqPwAAGrNGHyxiI2NN7QcAQGPW6INFSlyK4iPjZZGlxuUWWZQQmaCUuBQvVwYAQOBp9MHCGmJVVu8sSaoWLqqeT+49mfksAACog0YfLCQpvW26ZvedrbjIOJf2+Mh4ze47m3ksAACoIybI+l5623T1S+7HzJsAADQAweIS1hCreiX08nUZAAAELE6FAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANdzcFAMCH7A678grzVHS2SLGRsUqJS5E1xOrrsuqNYAEAgI/kHspVzqYcFZwtcLbFR8Yrq3eW0tum+7Cy+uNUCAAAPpB7KFeZazJdQoUkFZ4tVOaaTOUeyvVRZQ1DsAAAwMvsDrtyNuXIkFFtWVXbjE0zZHfYvV1agxEsAADwsrzCvGpHKi5lyFD+2XzlFeZ5sSpzECwAAPCyorNFpvbzJwQLAAC8LDYy1tR+/oRgAQCAl6XEpSg+Ml4WWWpcbpFFCZEJSolL8XJlDUewAADAy6whVmX1zpKkauGi6vnk3pMDcj4LggUAAD6Q3jZds/vOVlxknEt7fGS8ZvedHbDzWDBBFgAAPpLeNl39kvsx82ZjE2zTrQIA/Ic1xKpeCb18XYZpCBZXEIzTrQIA4ClcY3EZwTrdKgAAnkKwqEUwT7cKAICnECxqEczTrQIA4CkEi1oE83SrAAB4CsGiFsE83SoAAJ7Ct0JqUTXdauHZwhqvs7DIovjI+ICcbhUA4H2NZeoCgsWlHHbp0HrpdIGsUfHK6vUbZa59UhZZXMJF1XSrk25+QnnbFqio9LBibW2Ucv1oWUPDZK+4oLyv3vBIuySPvTbtvMe8x8HTznvsX+/xp999VvvUBcn9nPseRcVLbftIIVaXfZJLu5+zGIZR/c/xK3j55Zc1c+ZM5efnq2fPnvrzn/+s3r1712nd0tJSxcTEqKSkRDabze2CPWbn+9KKyVLp0f+22ZKU27GPcoo3qsD637ncE+yGBkV30vKyPS7t8XZDd3uwPcbukCxSSUiI6a9NO+8x73HwtPMe+9d7HGM3VGK1SIYhWf7bbvn++ewyu9KPf+dsly1J6v4TafvSavsk3TVD6nqPfKGu+2+3g8XixYs1ZswYvfLKK0pNTdULL7ygt99+W7t371ZcXNwV1/fLYLHzfWnJGKmGUx6SZJeUFxGuIqtVsXa7ToaE6Mm4qyp7X/JDokvfSk+2+2Kbja3dn2oJ1nZ/qiVY2/2plmBtr2/fqm6GoXi7XSuOHNWVj0V8v/7wv/skXHgsWKSmpqpXr1566aWXJEkOh0PJycn6n//5H2VlZZlWmNc47NIL3V1T4WXYJWUkJ6nAaq3xh+SHidT09pp4epuNrb0m/lZjoLfXxN9qDPT2mvhbjYHeXhN3+l7i1WMF6nW+vA49LZVHLiZ+5fXTInXdf7v1rZALFy7oyy+/VHr6f6eyDgkJUXp6ujZs2FDjOuXl5SotLXV5+JVD6+scKqTKIxcFoaG1/+B4ut0X22xs7f5US7C2+1MtwdruT7UEa3tD+16iyFrXkGBIpd9V7rv8lFvB4vjx47Lb7YqPj3dpj4+PV35+fo3rTJ8+XTExMc5HcnJy/av1hNO1T4JVk7p/+AAA1E2s3c1ZnN3cd3mTx+exmDJlikpKSpyPI0eOeHqT7omKv3KfS7j94QMAUAuLYSihokIpdToNcgk3913e5FawuOqqq2S1WlVQ4JqUCgoKlJCQUOM64eHhstlsLg+/0rZP5fkq1e3wVcr5csVXVFRezVsTT7f7YpuNrd2fagnWdn+qJVjb/amWYG139zV+sKxqPzK5+GQdLtx0riXZWlfuu/yUW8EiLCxMN910k1avXu1sczgcWr16tdLS0kwvzitCrJVf35FUl3BhlZRVfLKy9w9/gKqee7LdF9tsbO3+VEuwtvtTLcHa7k+1BGt7HftW7StiHA6X9ni7XbMLjyv97DnVzff7qLty/Ho+C7dPhWRmZmrevHl6/fXXtWvXLj3yyCM6c+aMHnjgAU/U5x1d76n8+o4t0bXd1lrq8+vvj2j8V3poS81ukao4158RJTikB5p1UryH2ps7DMX84AfW09tsbO28x7zHwdDOe+xf73G8Q3q+RarWnjL06rECzSg8rlePFWhFiUXpN/yq2j6mtn2PbEk++6qpO+o1QdZLL73knCDrhhtu0IsvvqjU1NQ6ret3Xze9VG2znNXS7u8zvdHOe+yv7bzHvMfB0O7ue+zuPsbfZt702DwWDeXXwQIAANTII/NYAAAAXA7BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwTai3N1g10Wdpaam3Nw0AAOqpar99pQm7vR4sysrKJEnJycne3jQAAGigsrIyxcTE1Lrc6/cKcTgcOnr0qKKjo2WxXPk25XVVWlqq5ORkHTlyJOjvQdKYxio1rvEy1uDVmMbLWIOTYRgqKytTUlKSQkJqv5LC60csQkJCdPXVV3vs9W02W9B/uFUa01ilxjVexhq8GtN4GWvwudyRiipcvAkAAExDsAAAAKYJmmARHh6u7OxshYeH+7oUj2tMY5Ua13gZa/BqTONlrI2b1y/eBAAAwStojlgAAADfI1gAAADTECwAAIBpCBYAAMA0ARUsXn75ZbVr104RERFKTU3Vpk2bLtv/7bffVpcuXRQREaHrr79ey5cv91KlDefOWOfNm6fbbrtNLVq0UIsWLZSenn7F98bfuPvZVlm0aJEsFouGDh3q2QJN5O5YT506pfHjxysxMVHh4eG69tprA+Zn2d2xvvDCC+rcubOaNm2q5ORkPf744zp//ryXqq2/zz77TEOGDFFSUpIsFovefffdK66zZs0apaSkKDw8XNdcc40WLFjg8TrN4u5433nnHQ0YMECxsbGy2WxKS0vTypUrvVNsA9Xns62ybt06hYaG6oYbbvBYff4oYILF4sWLlZmZqezsbOXl5alnz57KyMhQYWFhjf3Xr1+vkSNHaty4cdqyZYuGDh2qoUOHavv27V6u3H3ujnXNmjUaOXKkPv30U23YsEHJyckaOHCgvvvuOy9XXj/ujrfKwYMH9eSTT+q2227zUqUN5+5YL1y4oAEDBujgwYNaunSpdu/erXnz5ql169Zertx97o71rbfeUlZWlrKzs7Vr1y7Nnz9fixcv1m9/+1svV+6+M2fOqGfPnnr55Zfr1P/AgQMaPHiw+vXrp61bt2rixIn65S9/GTA7W3fH+9lnn2nAgAFavny5vvzyS/Xr109DhgzRli1bPFxpw7k71iqnTp3SmDFjdOedd3qoMj9mBIjevXsb48ePdz632+1GUlKSMX369Br7Dx8+3Bg8eLBLW2pqqvHQQw95tE4zuDvWH6qoqDCio6ON119/3VMlmqo+462oqDD69Olj/O1vfzPGjh1r3HvvvV6otOHcHeucOXOMDh06GBcuXPBWiaZxd6zjx483+vfv79KWmZlp3HLLLR6t02ySjGXLll22z29+8xujW7duLm0jRowwMjIyPFiZZ9RlvDXp2rWrMW3aNPML8iB3xjpixAjj6aefNrKzs42ePXt6tC5/ExBHLC5cuKAvv/xS6enpzraQkBClp6drw4YNNa6zYcMGl/6SlJGRUWt/f1Gfsf7Q2bNndfHiRbVs2dJTZZqmvuP9v//7P8XFxWncuHHeKNMU9Rnr+++/r7S0NI0fP17x8fHq3r27/vCHP8hut3ur7Hqpz1j79OmjL7/80nm6ZP/+/Vq+fLnuvvtur9TsTYH6+8ksDodDZWVlAfE7qj5ee+017d+/X9nZ2b4uxSe8fhOy+jh+/Ljsdrvi4+Nd2uPj4/X111/XuE5+fn6N/fPz8z1WpxnqM9Yfmjx5spKSkqr94vJH9Rnvv//9b82fP19bt271QoXmqc9Y9+/fr08++USjRo3S8uXLtXfvXj366KO6ePGiX//Sqs9Yf/azn+n48eO69dZbZRiGKioq9PDDDwfEqRB31fb7qbS0VOfOnVPTpk19VJl3zJo1S6dPn9bw4cN9XYrp9uzZo6ysLP3rX/9SaGhA7GJNFxBHLFB3OTk5WrRokZYtW6aIiAhfl2O6srIyjR49WvPmzdNVV13l63I8zuFwKC4uTn/961910003acSIEXrqqaf0yiuv+Lo0061Zs0Z/+MMf9Je//EV5eXl655139OGHH+rZZ5/1dWkw0VtvvaVp06ZpyZIliouL83U5prLb7frZz36madOm6dprr/V1OT4TEHHqqquuktVqVUFBgUt7QUGBEhISalwnISHBrf7+oj5jrTJr1izl5OQoNzdXPXr08GSZpnF3vPv27dPBgwc1ZMgQZ5vD4ZAkhYaGavfu3erYsaNni66n+ny2iYmJatKkiaxWq7PtuuuuU35+vi5cuKCwsDCP1lxf9Rnr1KlTNXr0aP3yl7+UJF1//fU6c+aMHnzwQT311FMKCQmev4Nq+/1ks9mC+mjFokWL9Mtf/lJvv/12QBxRdVdZWZk2b96sLVu2aMKECZIqfz8ZhqHQ0FB9/PHH6t+/v4+r9LyA+D81LCxMN910k1avXu1sczgcWr16tdLS0mpcJy0tzaW/JK1atarW/v6iPmOVpOeee07PPvusVqxYoZtvvtkbpZrC3fF26dJFX331lbZu3ep83HPPPc6r65OTk71Zvlvq89necsst2rt3rzM8SdI333yjxMREvw0VUv3Gevbs2WrhoSpQGUF2S6NA/f3UEAsXLtQDDzyghQsXavDgwb4uxyNsNlu1308PP/ywOnfurK1btyo1NdXXJXqHjy8erbNFixYZ4eHhxoIFC4ydO3caDz74oNG8eXMjPz/fMAzDGD16tJGVleXsv27dOiM0NNSYNWuWsWvXLiM7O9to0qSJ8dVXX/lqCHXm7lhzcnKMsLAwY+nSpcaxY8ecj7KyMl8NwS3ujveHAulbIe6O9fDhw0Z0dLQxYcIEY/fu3cYHH3xgxMXFGb/73e98NYQ6c3es2dnZRnR0tLFw4UJj//79xscff2x07NjRGD58uK+GUGdlZWXGli1bjC1bthiSjNmzZxtbtmwxDh06ZBiGYWRlZRmjR4929t+/f78RGRlpTJo0ydi1a5fx8ssvG1ar1VixYoWvhuAWd8f75ptvGqGhocbLL7/s8jvq1KlTvhpCnbk71h9qjN8KCZhgYRiG8ec//9lo06aNERYWZvTu3dv4/PPPncvuuOMOY+zYsS79lyxZYlx77bVGWFiY0a1bN+PDDz/0csX1585Y27Zta0iq9sjOzvZ+4fXk7md7qUAKFobh/ljXr19vpKamGuHh4UaHDh2M3//+90ZFRYWXq64fd8Z68eJF45lnnjE6duxoREREGMnJycajjz5qnDx50vuFu+nTTz+t8f/BqvGNHTvWuOOOO6qtc8MNNxhhYWFGhw4djNdee83rddeXu+O94447Ltvfn9Xns71UYwwW3DYdAACYJiCusQAAAIGBYAEAAExDsAAAAKYhWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0/x/lETloV3QFwYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'acc' is your existing Polars DataFrame with the first column as 'timestamp'.\n",
    "acc = pl.DataFrame({\n",
    "    \"timestamp\": [0, 0.02, 0.04, 1.51, 1.52],  # Irregular timestamps\n",
    "    \"sensor1\": [1.2, 3.4, 2.1, 5.3, 4.7],\n",
    "    \"sensor2\": [0.5, 0.8, 1.1, 0.3, 1.9]\n",
    "})\n",
    "\n",
    "acc_resampled = fill_gaps_in_accelerometer_data(acc)\n",
    "acc_resampled = apply_gausian_filter(acc_resampled, overwrite=False)\n",
    "\n",
    "# Plot the original and resampled timestamps\n",
    "plt.scatter(acc['timestamp'], acc['sensor1'], label='Original')\n",
    "plt.scatter(acc_resampled['timestamp'], acc_resampled['sensor1'], label='Resampled')\n",
    "plt.scatter(acc_resampled['timestamp'], acc_resampled['sensor1_smoothed'], label='Smoothed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import abc\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "from pisces.enums import KnownModel\n",
    "\n",
    "\n",
    "class SleepWakeClassifier(abc.ABC):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    @abc.abstractmethod\n",
    "    def model_type(self) -> KnownModel:\n",
    "        pass\n",
    "    @classmethod\n",
    "    @abc.abstractmethod\n",
    "    def get_needed_X_y(cls, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        pass\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        pass\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n",
    "    def roc_curve(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        pass \n",
    "    def roc_auc(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> float:\n",
    "        pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SGDLogisticRegression(SleepWakeClassifier):\n",
    "    \"\"\"Uses Sk-Learn's `SGDCLassifier` to train a logistic regression model. The SGD aspect allows for online learning, or custom training regimes through the `partial_fit` method.\n",
    "     \n",
    "    The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a `StandardScaler` before being passed to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr: float = 0.15, input_dim: int = 11, output_dim: int = 1):\n",
    "        self.model = SGDClassifier(loss='log_loss',\n",
    "                                   learning_rate='adaptive',\n",
    "                                   penalty='l1',\n",
    "                                   eta0=lr,\n",
    "                                   class_weight='balanced',\n",
    "                                   warm_start=True)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pipeline = make_pipeline(self.scaler, self.model)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.window_step = 1\n",
    "\n",
    "    @classmethod\n",
    "    def get_needed_X_y(cls, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return get_activity_X_PSG_y(data_set, id)\n",
    "    \n",
    "    @property\n",
    "    def model_type(self) -> KnownModel:\n",
    "        return KnownModel.LOG_REG_SKLEARN\n",
    "\n",
    "    def _prepare_labels(self, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        y_trimmed = self._trim_labels(y)\n",
    "        n_sleep = np.sum(y_trimmed > 0)\n",
    "        n_wake = np.sum(y_trimmed == 0)\n",
    "        N = n_sleep + n_wake\n",
    "        # Want to make a balanced weight loss, along with giving 0.0 * loss for masked values (y < 0)\n",
    "        mask_weights_zero = np.where(y_trimmed < 0, 0.0, 1)\n",
    "        # balancing_weights_ignore_mask = np.where(y_trimmed > 0, n_wake / N, n_sleep / N)\n",
    "        balancing_weights_ignore_mask = np.where(y_trimmed > 0, N / n_sleep, N / n_wake)\n",
    "        sample_weights = mask_weights_zero * balancing_weights_ignore_mask\n",
    "\n",
    "        y_demasked = np.where(y_trimmed < 0, 0, y_trimmed)\n",
    "\n",
    "        return y_demasked, sample_weights\n",
    "\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        if examples_X or examples_y:\n",
    "            assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "        \n",
    "        training = []\n",
    "        training_iterator = iter(pairs_Xy) if pairs_Xy else zip(examples_X, examples_y)\n",
    "        for X, y in training_iterator:\n",
    "            try:\n",
    "                X_folded = self._fold(X)\n",
    "                y_prepped, sample_weights = self._prepare_labels(y)\n",
    "                if (X_folded.shape[0] != y_prepped.shape[0]) \\\n",
    "                    or (X_folded.shape[0] == 0) \\\n",
    "                    or (y_prepped.shape[0] == 0):\n",
    "                    continue\n",
    "                training.append((X_folded, y_prepped, sample_weights))\n",
    "            except Exception as e:\n",
    "                print(f\"Error folding or trimming data: {e}\")\n",
    "                continue\n",
    "        \n",
    "        Xs = [X for X, _, _ in training]\n",
    "        ys = [y for _, y, _ in training]\n",
    "        weights = [w for _, _, w in training]\n",
    "        Xs = np.concatenate(Xs, axis=0)\n",
    "        ys = np.concatenate(ys, axis=0)\n",
    "        weights = np.concatenate(weights, axis=0)\n",
    "\n",
    "        selector = ys >= 0\n",
    "        Xs = Xs[selector]\n",
    "        ys = ys[selector]\n",
    "        weights = weights[selector]\n",
    "\n",
    "        self.pipeline.fit(Xs, ys)\n",
    "    \n",
    "    def _input_preprocessing(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.scaler.transform(self._fold(X))\n",
    "    \n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict(self._input_preprocessing(sample_X))\n",
    "    \n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict_proba(self._input_preprocessing(sample_X))\n",
    "\n",
    "    def roc_auc(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> float:\n",
    "        prediction, labels, balancing_weights = self._evaluation_preprocessing(examples_X_y)\n",
    "        auc = roc_auc_score(labels, prediction)\n",
    "        return auc\n",
    "    \n",
    "    def _evaluation_preprocessing(self, examples_X_y: Tuple[np.ndarray, np.ndarray]\n",
    "                                  ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        labels, balancing_weights = self._prepare_labels(examples_X_y[1])\n",
    "\n",
    "        # get probability of sleep\n",
    "        prediction = self.predict_probabilities(examples_X_y[0])[:, -1]\n",
    "        # drop all -1 (mask) classes\n",
    "        min_matching_length = min(len(labels), len(prediction))\n",
    "        prediction = prediction[:min_matching_length]\n",
    "        if len(prediction) != len(labels):\n",
    "            print(f\"prediction: {prediction.shape}, labels: {labels.shape}\")\n",
    "        labels = labels[:min_matching_length]\n",
    "        balancing_weights = balancing_weights[:min_matching_length]\n",
    "\n",
    "        return prediction, labels, balancing_weights\n",
    "\n",
    "    def roc_curve(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        prediction, labels, balancing_weights = self._evaluation_preprocessing(examples_X_y)\n",
    "        fpr, tpr, _ = roc_curve(labels, prediction, sample_weight=balancing_weights)\n",
    "        return fpr, tpr\n",
    "\n",
    "\n",
    "    def _fold(self, input_X: np.ndarray | pl.DataFrame) -> np.array:\n",
    "        if isinstance(input_X, pl.DataFrame):\n",
    "            xa = input_X.to_numpy()\n",
    "        return rolling_window(input_X, self.input_dim)\n",
    "    \n",
    "    def _trim_labels(self, labels_y: pl.DataFrame) -> np.ndarray:\n",
    "        start, end = self._indices_to_trim()\n",
    "        # return labels_y[self.input_dim:]\n",
    "        return labels_y[start:-end]\n",
    "        \n",
    "    def _indices_to_trim(self) -> Tuple[int, int]:\n",
    "        # ex: input_dim = 8 => (4, 3)\n",
    "        # ex: input_dim = 7 => (3, 3)\n",
    "        # ex: input_dim = 6 => (3, 2)\n",
    "        return (self.input_dim // 2, self.input_dim - (self.input_dim // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mads Olsen et all classifier\n",
    "\n",
    "We have downloaded the saved model weights from a paper by Mads Olsen's group. Thus, we have a TensorFlow Lite model that we can run inference on, and we could train it if we wanted to (I think?).\n",
    "\n",
    "For simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs:\n",
    "1. 3-axis Accelerometer data\n",
    "2. PPG (photoplethysmogram) data\n",
    "Based on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "from pisces.enums import KnownModel\n",
    "import tensorflow as tf\n",
    "import pkg_resources\n",
    "\n",
    "def median(x, fs, window_size):\n",
    "    if isinstance(x, pl.DataFrame):\n",
    "        x = x.to_numpy()\n",
    "\n",
    "    window = (\n",
    "        (fs * window_size + 1) if (fs * window_size) % 2 == 0 else (fs * window_size)\n",
    "    )\n",
    "\n",
    "    reduce_dims = False\n",
    "    if len(x.shape) == 1:\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        reduce_dims = True\n",
    "    x_norm = np.zeros((x.shape))\n",
    "\n",
    "    for idx in range(x.shape[-1]):\n",
    "\n",
    "        x_med = np.ones((x.shape[0])) * np.median(x[:, idx])\n",
    "\n",
    "        x_pd = pd.Series(x[:, idx])\n",
    "        med_ = x_pd.rolling(window).median()\n",
    "        x_med[int(window / 2) : -int(window / 2)] = med_[window - 1 :]\n",
    "        x_med[: int(window / 2)] = med_[window - 1]\n",
    "        x_med[-int(window / 2) :] = med_[-1:]\n",
    "\n",
    "        x_med[np.isnan(x_med)] = 0  # remove nan\n",
    "\n",
    "        x_norm[:, idx] = x[:, idx] - x_med\n",
    "\n",
    "    if reduce_dims:\n",
    "        x_norm = x_norm[:, 0]\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "def clip_by_iqr(x, fs, threshold=20):\n",
    "\n",
    "    x[x > threshold] = threshold\n",
    "    x[x < -threshold] = -threshold\n",
    "\n",
    "    return x\n",
    "\n",
    "def iqr_normalization_adaptive(x, fs, median_window, iqr_window):\n",
    "    def normalize(x, fs, median_window, iqr_window, iqr_upper=0.75, iqr_lower=0.25):\n",
    "\n",
    "        # add noise\n",
    "        x_ = x + np.random.normal(loc=0, scale=sys.float_info.epsilon, size=(x.shape))\n",
    "\n",
    "        # fix window parameters to odd number\n",
    "        med_window = (\n",
    "            (fs * median_window + 1)\n",
    "            if (fs * median_window) % 2 == 0\n",
    "            else (fs * median_window)\n",
    "        )\n",
    "        iqr_window = (\n",
    "            (fs * iqr_window + 1) if (fs * iqr_window) % 2 == 0 else (fs * iqr_window)\n",
    "        )\n",
    "\n",
    "        # preallocation\n",
    "        x_med = np.ones((x.shape)) * np.median(x_)\n",
    "        x_iqr_up = np.ones((x.shape)) * np.quantile(x_, iqr_upper)\n",
    "        x_iqr_lo = np.ones((x.shape)) * np.quantile(x_, iqr_lower)\n",
    "\n",
    "        # find rolling median\n",
    "        x_pd = pd.Series(x_)\n",
    "        med_ = x_pd.rolling(med_window).median()\n",
    "        x_med[int(med_window / 2) : -int(med_window / 2)] = med_[med_window - 1 :]\n",
    "        x_med[np.isnan(x_med)] = 0  # remove nan\n",
    "\n",
    "        # find rolling quantiles\n",
    "        x_iqr_upper = x_pd.rolling(iqr_window).quantile(iqr_upper)\n",
    "        x_iqr_lower = x_pd.rolling(iqr_window).quantile(iqr_lower)\n",
    "\n",
    "        # border padding\n",
    "        x_iqr_up[int(iqr_window / 2) : -int(iqr_window / 2)] = x_iqr_upper[\n",
    "            iqr_window - 1 :\n",
    "        ]\n",
    "        x_iqr_lo[int(iqr_window / 2) : -int(iqr_window / 2)] = x_iqr_lower[\n",
    "            iqr_window - 1 :\n",
    "        ]\n",
    "\n",
    "        # remove nan\n",
    "        x_iqr_up[np.isnan(x_iqr_up)] = 0\n",
    "        x_iqr_lo[np.isnan(x_iqr_lo)] = 0\n",
    "\n",
    "        # return normalize\n",
    "        return (x_ - x_iqr_lo) / (x_iqr_up - x_iqr_lo + sys.float_info.epsilon) * 2 - 1\n",
    "\n",
    "    x_norm = np.zeros((x.shape))\n",
    "    if len(x.shape) == 1:\n",
    "        x_norm[:] = normalize(x, fs, median_window, iqr_window)\n",
    "    else:\n",
    "        for n in range(x.shape[1]):\n",
    "            x_norm[:, n] = normalize(x[:, n], fs, median_window, iqr_window)\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "\n",
    "def cal_psd(x, fs, window, noverlap, nfft, f_min, f_max, f_sub=1):\n",
    "    \"\"\"\n",
    "    https://github.com/MADSOLSEN/SleepStagePrediction/blob/d47ff488f5cedd3b0459593a53fc4f92fc3660a2/signal_processing/spectrogram.py#L91\n",
    "    \"\"\"\n",
    "    from scipy.ndimage import maximum_filter as maxfilt\n",
    "\n",
    "    # border edit\n",
    "    x_ = np.zeros((x.size + window,))\n",
    "    x_[window // 2 : -window // 2] = x\n",
    "    x_ = x_ + np.random.normal(loc=0, scale=sys.float_info.epsilon, size=(x_.shape))\n",
    "\n",
    "    f, t, S = spectrogram(\n",
    "        x=x_,\n",
    "        fs=fs,\n",
    "        window=np.blackman(window),\n",
    "        nperseg=window,\n",
    "        noverlap=noverlap,\n",
    "        nfft=nfft,\n",
    "    )\n",
    "\n",
    "    S = S[(f > f_min) & (f <= f_max), :]\n",
    "    S = maxfilt(np.abs(S), size=(f_sub, 1))\n",
    "    S = S[::f_sub, :]\n",
    "    S = np.swapaxes(S, axis1=1, axis2=0)\n",
    "    S = np.log(S + sys.float_info.epsilon)\n",
    "    if S.shape[1] != 24:\n",
    "        k = 1\n",
    "\n",
    "    assert np.sum(np.isinf(S)) == 0\n",
    "    assert np.sum(np.isnan(S)) == 0\n",
    "    return S\n",
    "\n",
    "def _load_from_tflite():\n",
    "    file_path = pkg_resources.resource_filename('pisces', 'cached_models/mo_resunet.keras')\n",
    "\n",
    "    return tf.keras.models.load_model(file_path, safe_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericcanton/mambaforge/envs/pisces/lib/python3.10/site-packages/keras/src/layers/core/lambda_layer.py:327: UserWarning: CNN.resunet is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(\n"
     ]
    }
   ],
   "source": [
    "mod = _load_from_tflite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "FS = 32\n",
    "CHANNELS = 1\n",
    "DEPTH = 9\n",
    "N_OUT = 2 ** (DEPTH + 1)\n",
    "N_CLASSES = 4\n",
    "\n",
    "MO_UNET_CONFIG = {\n",
    "    \"input_shape\": (15360, 32, CHANNELS),\n",
    "    \"num_classes\": N_CLASSES,\n",
    "    \"num_outputs\": N_OUT,\n",
    "    \"init_filter_num\": 8,  # 16,\n",
    "    \"filter_increment_factor\": 2 ** (1 / 3),\n",
    "    \"max_pool_size\": (2, 2),\n",
    "    \"depth\": DEPTH,\n",
    "    \"kernel_size\": (16, 3),\n",
    "}\n",
    "\n",
    "MO_PREPROCESSING_CONFIG = {\n",
    "    \"preprocessing\": [\n",
    "        {\"args\": {\"window_size\": 30, \"fs\": FS}, \"type\": \"median\"},\n",
    "        {\n",
    "            \"args\": {\"iqr_window\": 300, \"median_window\": 300, \"fs\": FS},\n",
    "            \"type\": \"iqr_normalization_adaptive\",\n",
    "        },\n",
    "        {\"args\": {\"threshold\": 20, \"fs\": FS}, \"type\": \"clip_by_iqr\"},\n",
    "        {\n",
    "            \"args\": {\n",
    "                \"fs\": FS,\n",
    "                \"nfft\": 512,\n",
    "                \"f_max\": 6,\n",
    "                \"f_min\": 0,\n",
    "                \"f_sub\": 3,\n",
    "                \"window\": 320,\n",
    "                \"noverlap\": 256,\n",
    "            },\n",
    "            \"type\": \"cal_psd\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "class MOResUNetPretrained(SleepWakeClassifier):\n",
    "    tf_model = _load_from_tflite()\n",
    "    config = MO_PREPROCESSING_CONFIG\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def prepare_set_for_training(self, \n",
    "                                 data_set: DataSetObject, ids: List[str] | None = None\n",
    "                                 ) -> List[Tuple[np.ndarray, np.ndarray] | None]:\n",
    "        \"\"\"Calls `get_needed_X_y` in parallel for each of the `ids` provided, or all of the IDs in the `data_set` if `ids` is None.\n",
    "\n",
    "        Returns a list of tuples, where each tuple is the result of `get_needed_X_y` for a given ID. An empty list indicates an error occured during pool.map.\n",
    "        \"\"\"\n",
    "        if ids is None:\n",
    "            ids = data_set.ids\n",
    "        results = []\n",
    "        \n",
    "        if ids:\n",
    "            data_set_and_ids = [(data_set, id) for id in ids]\n",
    "            # Get the number of available CPU cores\n",
    "            num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "            # Create a pool of workers\n",
    "            with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "                results = list(executor.map(MOResUNetPretrained.get_needed_X_y_from_pair, data_set_and_ids))\n",
    "            # pool = multiprocessing.Pool(processes=num_cores-1)\n",
    "\n",
    "            # # Map the function to each id in w.ids\n",
    "            # try:\n",
    "            #     results = pool.map(curried_get_needed_X_y, ids)\n",
    "            # except KeyboardInterrupt:\n",
    "            #     pool.terminate()\n",
    "            #     pool.join()\n",
    "            #     raise\n",
    "\n",
    "            # # Close the pool of workers\n",
    "            # pool.close()\n",
    "        else:\n",
    "            warnings.warn(\"No IDs found in the data set.\")\n",
    "            return results\n",
    "        return results\n",
    "    \n",
    "    @classmethod\n",
    "    def get_needed_X_y_from_pair(cls, pair: Tuple[DataSetObject, str] | None = None) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        if pair:\n",
    "            data_set, id = pair\n",
    "        else:\n",
    "            return None\n",
    "        return cls.get_needed_X_y(data_set, id)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_needed_X_y(cls, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        accelerometer = data_set.get_feature_data(\"accelerometer\", id)\n",
    "        psg = data_set.get_feature_data(\"psg\", id)\n",
    "\n",
    "        if accelerometer is None or psg is None:\n",
    "            return None\n",
    "        \n",
    "        accelerometer = fill_gaps_in_accelerometer_data(accelerometer, smooth=False)\n",
    "        stop_time = min(accelerometer[:, 0].max(), psg[:, 0].max())\n",
    "        accelerometer = accelerometer.filter(accelerometer[:, 0] <= stop_time)\n",
    "        psg = psg.filter(psg[:, 0] <= stop_time)\n",
    "\n",
    "\n",
    "        mirrored_spectro = cls._input_preprocessing(accelerometer)\n",
    "\n",
    "        return mirrored_spectro, psg_to_sleep_wake(psg)\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[pl.DataFrame] = [], \n",
    "              examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        \"\"\"This function does nothing, because this is a pre-trained, non-trainable model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return np.argmax(self.predict_probabilities(sample_X), axis=1)\n",
    "\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        if isinstance(sample_X, pl.DataFrame):\n",
    "            sample_X = sample_X.to_numpy()\n",
    "        return self.eval_tflite_interp(sample_X)\n",
    "\n",
    "    def roc_curve(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        raise NotImplementedError\n",
    "    def roc_auc(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def model_type(self) -> KnownModel:\n",
    "        return \n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        self.tflite_model = self._load_from_tflite()\n",
    "\n",
    "    # Called when pickling\n",
    "    def __getstate__(self) -> dict:\n",
    "        selfCopy = self\n",
    "        selfCopy.tf_model = None\n",
    "\n",
    "        return selfCopy.__dict__\n",
    "\n",
    "    @classmethod\n",
    "    def _spectrogram_preprocessing(cls, acc_xyz: np.ndarray) -> np.ndarray:\n",
    "        return cls._preprocessing(acc_xyz)\n",
    "\n",
    "    @classmethod\n",
    "    def _input_preprocessing(\n",
    "        cls,\n",
    "        acc_xyz: pl.DataFrame | np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        spec = cls._spectrogram_preprocessing(acc_xyz)\n",
    "\n",
    "        input_dets = cls.tf_model.get_input_details()\n",
    "\n",
    "        # We will copy the spectrogram to both channels, flipping it on channel 1\n",
    "        input_shape = input_dets[0][\"shape\"]\n",
    "        inputs_len = input_shape[1]\n",
    "\n",
    "        inputs = np.zeros(shape=input_shape, dtype=np.float32)\n",
    "        # We must do some careful work with indices to not overflow arrays\n",
    "        spec = spec[:inputs_len].astype(np.float32) # protect agains spec.len > input_shape\n",
    "\n",
    "        #! careful, order matters here. We first trim spec to make sure it'll fit into inputs,\n",
    "        # then compute the new length which we KNOW is <= inputs_len\n",
    "        spec_len = spec.shape[0]\n",
    "        # THEN we assign only as much inputs as spec covers\n",
    "        inputs[0, : spec_len, :, 0] = spec # protect agains spec_len < input_shape\n",
    "        inputs[0, : spec_len, :, 1] = spec[:, ::-1]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def eval_tflite_interp(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # Boilerplate to run inference on a TensorFlow Lite model interpreter.\n",
    "        input_dets = self.tf_model.get_input_details()\n",
    "        output_dets = self.tf_model.get_output_details()\n",
    "\n",
    "        # set input tensor to FLOAT32\n",
    "        inputs = inputs.astype(np.float32)\n",
    "\n",
    "        # self.tflite_model.set_tensor(input_dets[0][\"index\"], inputs)\n",
    "\n",
    "        # self.tflite_model.invoke()\n",
    "\n",
    "        # preds = self.tflite_model.get_tensor(output_dets[0][\"index\"])\n",
    "        preds = self.tf_model.predict(inputs)\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    @classmethod\n",
    "    def _preprocessing(\n",
    "        cls,\n",
    "        acc: pl.DataFrame | np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The Mads Olsen repo uses a list of transformations\n",
    "        \"\"\"\n",
    "        if isinstance(acc, pl.DataFrame):\n",
    "            acc = acc.to_numpy()\n",
    "        x_ = acc[:, 0]\n",
    "        y_ = acc[:, 1]\n",
    "        z_ = acc[:, 2]\n",
    "        for step in cls.config[\"preprocessing\"]:\n",
    "            fn = eval(step[\"type\"])  # convert string version to function in environment\n",
    "            fn_args = partial(\n",
    "                fn, **step[\"args\"]\n",
    "            )  # fill in the args given, which must be everything besides numerical input\n",
    "\n",
    "            # apply\n",
    "            x_ = fn_args(x_)\n",
    "            y_ = fn_args(y_)\n",
    "            z_ = fn_args(z_)\n",
    "\n",
    "        spec = x_ + y_ + z_\n",
    "        spec /= 3.0\n",
    "\n",
    "        return spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Type\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "def run_split(train_indices, \n",
    "              preprocessed_data_set: List[Tuple[np.ndarray, np.ndarray]], \n",
    "              epochs: int, \n",
    "              swc: SleepWakeClassifier) -> SleepWakeClassifier:\n",
    "    training_pairs = [\n",
    "        preprocessed_data_set[i]\n",
    "        for i in train_indices\n",
    "        if preprocessed_data_set[i]\n",
    "    ]\n",
    "    swc.train(pairs_Xy=training_pairs, epochs=epochs)\n",
    "\n",
    "    return swc\n",
    "\n",
    "class SplitMaker:\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LeaveOneOutSplitter(SplitMaker):\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        loo = LeaveOneOut()\n",
    "        return loo.split(ids)\n",
    "\n",
    "def run_splits(split_maker: SplitMaker, w: DataSetObject, epochs: int, swc_class: Type[SleepWakeClassifier]) -> Tuple[\n",
    "        List[SleepWakeClassifier], \n",
    "        List[np.ndarray],\n",
    "        List[List[List[int]]]]:\n",
    "    split_models: List[SGDLogisticRegression] = []\n",
    "    test_indices = []\n",
    "    splits = []\n",
    "\n",
    "    preprocessed_data = [swc_class.get_needed_X_y(w, i) for i in w.ids]\n",
    "\n",
    "    for train_index, test_index in tqdm(split_maker.split(w.ids)):\n",
    "        if preprocessed_data[test_index[0]] is None:\n",
    "            continue\n",
    "        model = run_split(train_indices=train_index,\n",
    "                        preprocessed_data_set=preprocessed_data,\n",
    "                        epochs=500, swc=swc_class())\n",
    "        split_models.append(model)\n",
    "        test_indices.append(test_index[0])\n",
    "        splits.append([train_index, test_index])\n",
    "        # break\n",
    "    \n",
    "    return split_models, preprocessed_data, splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
