{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "> A _Pisces experiment_ is a combined specification of data sets, validation method(s), and model architectures to use.\n",
    "\n",
    "Let's go through the example provied in `<pisces>/experiment_config/base_config.json`:\n",
    "```json\n",
    "{\n",
    "  \"data_config\": {\n",
    "    \"data_directory\": \"./data_sets\",\n",
    "    \"sets\": \"\"\n",
    "  },\n",
    "  \"validation\": {\n",
    "    \"method\": \"LEAVE_ONE_OUT\",\n",
    "    \"parameter\": null\n",
    "  },\n",
    "  \"models\": [\n",
    "    \"LOGISTIC\"\n",
    "  ],\n",
    "  \"features\": [\n",
    "    \"ACTIVITY\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "Let's break this down, to give you a sense for the kinds of experiments Pisces can run and how to modify this to suit your own investigations.\n",
    "\n",
    "* `\"data_config\"`: This is a dictionary that specifies the data sets to use. \n",
    "  * The `\"data_directory\": \"./data_sets` specifies that the data sets are located in the `data_sets` directory, which is a subdirectory of the current working directory.\n",
    "  * The `\"sets\": \"\"` key specifies the names of the data sets to use. Since this is an empty string, all data sets in the directory will be used. Otherwise, this would be a comma-separated list of data set names, which are folders inside the `\"data_directory\"` that have subdirectories matching `cleaned_*`.\n",
    "* `\"validation\"`: This is a dictionary that specifies the validation method to use.  \n",
    "  * The `\"method\"` key specifies the validation method to use. Here, we use `\"LEAVE_ONE_OUT\"`, which means that each data set will be used to train a model, and then the model will be tested on the same data set.\n",
    "  * The `\"parameter\"` key specifies any parameters that the validation method requires. `\"LEAVE_ONE_OUT\"`, then the parameter is the name of the column to use for the leave-one-out validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def random_string(length: int) -> str:\n",
    "    return ''.join(random.sample(string.ascii_lowercase, length))\n",
    "\n",
    "ids = [\n",
    "    \"abx123\",\n",
    "    \"bx887\",\n",
    "    \"MN3f23\"\n",
    "]\n",
    "acc_txts_1 = [\n",
    "    f\"{id}_cleaned_acc.txt\"\n",
    "    for id in ids\n",
    "]\n",
    "\n",
    "acc_txts_2 = [\n",
    "    f\"exported-{id}.{random_string(3)}\"\n",
    "    for id in ids\n",
    "]\n",
    "\n",
    "# just ID.csv, no pre/suffix\n",
    "acc_txts_3 = [\n",
    "    \"{id}.csv\"\n",
    "    for id in ids\n",
    "]\n",
    "\n",
    "\n",
    "# just ID, no pre/suffix\n",
    "acc_txts_4 = [\n",
    "    \"{id}\"\n",
    "    for id in ids\n",
    "]\n",
    "\n",
    "acc_txts = [acc_txts_1, acc_txts_2, acc_txts_3, acc_txts_4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import warnings\n",
    "\n",
    "class SimplifiablePrefixTree:\n",
    "    def __init__(self, delimiter: str = \"\", key: str = \"\"):\n",
    "        self.key = key\n",
    "        self.children: Dict[str, SimplifiablePrefixTree] = {}\n",
    "        self.is_end_of_word = False\n",
    "        self.delimiter = delimiter\n",
    "        self.print_spacer = \": \"\n",
    "    \n",
    "    def chars_from(self, word: str):\n",
    "        return word.split(self.delimiter) if self.delimiter else word\n",
    "\n",
    "    def insert(self, word: str):\n",
    "        node = self\n",
    "        for char in self.chars_from(word):\n",
    "            if char not in node.children:\n",
    "                node.children[char] = SimplifiablePrefixTree(self.delimiter, key=char)\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "\n",
    "    def search(self, word: str) -> bool:\n",
    "        node = self\n",
    "        for char in self.chars_from(word):\n",
    "            if char not in node.children:\n",
    "                return False\n",
    "            node = node.children[char]\n",
    "        return node.is_end_of_word\n",
    "    \n",
    "    def simplify(self):\n",
    "        if len(self.children) == 1 and not self.is_end_of_word:\n",
    "            child_key = list(self.children.keys())[0]\n",
    "            self.key += child_key\n",
    "            self.children = self.children[child_key].children\n",
    "            self.simplify()\n",
    "        else:\n",
    "            current_keys = list(self.children.keys())\n",
    "            for key in current_keys:\n",
    "                child = self.children.pop(key)\n",
    "                child.simplify()\n",
    "                self.children[child.key] = child\n",
    "        return self\n",
    "    \n",
    "    def reversed(self) -> 'SimplifiablePrefixTree':\n",
    "        rev_self = SimplifiablePrefixTree(self.delimiter, key=self.key[::-1])\n",
    "        rev_self.children = {k[::-1]: v.reversed() for k, v in self.children.items()}\n",
    "        return rev_self\n",
    "    \n",
    "    def flattened(self, max_depth: int = 1) -> 'SimplifiablePrefixTree':\n",
    "        \"\"\"Returns a Tree identical to `self` up to the given depth, but with all nodes at + below `max_depth` converted into leaves on the most recent acestor of lepth `max_depth - 1`.\n",
    "        \"\"\"\n",
    "        flat_self = SimplifiablePrefixTree(self.delimiter, key=self.key)\n",
    "        if max_depth == 0:\n",
    "            if not self.is_end_of_word:\n",
    "                warnings.warn(f\"max_depth is 0, but {self.key} is not a leaf.\")\n",
    "            return flat_self\n",
    "        if max_depth == 1:\n",
    "            for k, v in self.children.items():\n",
    "                if v.is_end_of_word:\n",
    "                    flat_self.children[k] = SimplifiablePrefixTree(self.delimiter, key=k)\n",
    "                else:\n",
    "                    # flattened_children = v._pushdown()\n",
    "                    for flattened_child in v._pushdown():\n",
    "                        flat_self.children[flattened_child.key] = flattened_child\n",
    "        else:\n",
    "            for k, v in self.children.items():\n",
    "                flat_self.children[k] = v.flattened(max_depth - 1)\n",
    "        return flat_self\n",
    "    \n",
    "    def _pushdown(self) -> List['SimplifiablePrefixTree']:\n",
    "        \"\"\"Returns a list corresponding to the children of `self`, with `self.key` prefixed to each child's key.\n",
    "        \"\"\"\n",
    "        pushed_down = [\n",
    "            c\n",
    "            for k in self.children.values()\n",
    "            for c in k._pushdown()\n",
    "        ]\n",
    "        for i in range(len(pushed_down)):\n",
    "            pushed_down[i].key = self.key + self.delimiter + pushed_down[i].key\n",
    "\n",
    "        if not pushed_down:\n",
    "            return [SimplifiablePrefixTree(self.delimiter, key=self.key)]\n",
    "        else:\n",
    "            return pushed_down\n",
    "            \n",
    "\n",
    "    def __str__(self):\n",
    "        # prints .children recursively with indentation\n",
    "        return self.key + \"\\n\" + self.print_tree()\n",
    "\n",
    "    def print_tree(self, indent=0) -> str:\n",
    "        result = \"\"\n",
    "        for key, child in self.children.items():\n",
    "            result +=  self.print_spacer * indent + \"( \" + child.key + \"\\n\"\n",
    "            result += SimplifiablePrefixTree.print_tree(child, indent + 1)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( a\n",
      ": ( b\n",
      ": : ( c\n",
      ": : : ( 1\n",
      ": : : : ( 2\n",
      ": : : : : ( 3\n",
      ": : : : : : ( X\n",
      ": : : : : : : ( Y\n",
      ": : : : : : : : ( Z\n",
      ": : : : : : : : ( &\n",
      ": : : : : : : : ( A\n",
      ": : : : : : : : ( B\n",
      ": : : : : ( M\n",
      ": : : : : : ( M\n",
      ": : : : : : : ( V\n",
      ": : : : : : : : ( Q\n",
      ": : : : : ( N\n",
      ": : : : : : ( M\n",
      ": : : : : : : ( V\n",
      ": : : : : : : : ( Q\n",
      "( x\n",
      ": ( y\n",
      ": : ( 9\n",
      ": : : ( 9\n",
      ": : : : ( 1\n",
      ": : : : : ( 4\n",
      ": : : : : : ( 9\n",
      ": : : : : : : ( 3\n",
      ": : : : : : : : ( 4\n",
      ": : ( 7\n",
      ": : : ( 2\n",
      ": : : : ( 7\n",
      ": : : : : ( 5\n",
      ": : : : : : ( 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entries = [\n",
    "    'abc123XYZ',\n",
    "    'abc123XY&',\n",
    "    'abc123XYA',\n",
    "    'abc123XYB',\n",
    "    'abc12MMVQ',\n",
    "    'abc12NMVQ',\n",
    "    'xy9914934',\n",
    "    'xy72758',\n",
    "]\n",
    "\n",
    "tree = SimplifiablePrefixTree()\n",
    "for entry in entries:\n",
    "    tree.insert(entry)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( abc12\n",
      ": ( 3XY\n",
      ": : ( Z\n",
      ": : ( &\n",
      ": : ( A\n",
      ": : ( B\n",
      ": ( MMVQ\n",
      ": ( NMVQ\n",
      "( xy\n",
      ": ( 9914934\n",
      ": ( 72758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = tree.simplify()\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( abc12\n",
      ": ( 3XY\n",
      ": : ( Z\n",
      ": : ( &\n",
      ": : ( A\n",
      ": : ( B\n",
      ": ( MMVQ\n",
      ": ( NMVQ\n",
      "( xy\n",
      ": ( 9914934\n",
      ": ( 72758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree.flattened(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import DefaultDict, Iterable\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "from pisces.utils import determine_header_rows_and_delimiter\n",
    "\n",
    "LOG_LEVEL = logging.INFO\n",
    "\n",
    "class DataSetObject:\n",
    "    FEATURE_PREFIX = \"cleaned_\"\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(LOG_LEVEL)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    def __init__(self, name: str, path: Path):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.ids: List[str] = []\n",
    "\n",
    "        # keeps track of the files for each feature and user\n",
    "        self._feature_map: DefaultDict[str, Dict[str, str]] = defaultdict(dict)\n",
    "        self._feature_cache: DefaultDict[str, Dict[str, pl.DataFrame]] = defaultdict(dict)\n",
    "    \n",
    "    @property\n",
    "    def features(self) -> List[str]:\n",
    "        return list(self._feature_map.keys())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.name}: {self.path}\"\n",
    "\n",
    "    def get_feature_data(self, feature: str, id: str) -> pl.DataFrame | None:\n",
    "        if feature not in self.features:\n",
    "            warnings.warn(f\"Feature {feature} not found in {self.name}. Returning None.\")\n",
    "            return None\n",
    "        if id not in self.ids:\n",
    "            warnings.warn(f\"ID {id} not found in {self.name}\")\n",
    "            return None\n",
    "        if (df := self._feature_cache[feature].get(id)) is None:\n",
    "            file = self.get_filename(feature, id)\n",
    "            self.logger.debug(f\"Loading {file}\")\n",
    "            try:\n",
    "                n_rows, delimiter = determine_header_rows_and_delimiter(file)\n",
    "                # self.logger.debug(f\"n_rows: {n_rows}, delimiter: {delimiter}\")\n",
    "                df = pl.read_csv(file, has_header=True if n_rows > 0 else False,\n",
    "                                 skip_rows=max(n_rows-1, 0), \n",
    "                                 separator=delimiter)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Error reading {file}:\\n{e}\")\n",
    "                return None\n",
    "            self._feature_cache[feature][id] = df\n",
    "        return df\n",
    "\n",
    "    def get_filename(self, feature: str, id: str) -> Path:\n",
    "        return self.get_feature_path(feature)\\\n",
    "            .joinpath(self._feature_map[feature][id])\n",
    "    \n",
    "    def get_feature_path(self, feature: str) -> Path:\n",
    "        return self.path.joinpath(self.FEATURE_PREFIX + feature)\n",
    "    \n",
    "    def _extract_ids(self, files: List[str]) -> List[str]:\n",
    "        ids_in_files_rev = SimplifiablePrefixTree()\n",
    "        for file in files:\n",
    "            ids_in_files_rev.insert(file[::-1])\n",
    "        return [\n",
    "            c.key for c in ids_in_files_rev\n",
    "                .simplify()\n",
    "                .reversed()\n",
    "                .flattened(1)\n",
    "                .children\n",
    "                .values()\n",
    "        ]\n",
    "    \n",
    "    def add_feature_files(self, feature: str, files: Iterable[str]):\n",
    "        if feature not in self.features:\n",
    "            self.logger.debug(f\"Adding feature {feature} to {self.name}\")\n",
    "            self._feature_map[feature] = {}\n",
    "        deduped_ids = set(self.ids)\n",
    "        for id, file in zip(self._extract_ids(files), files):\n",
    "            self._feature_map[feature][id] = file\n",
    "            deduped_ids.add(id)\n",
    "        self.ids = sorted(list(deduped_ids))\n",
    "    \n",
    "    def get_feature_files(self, feature: str) -> Dict[str, str]:\n",
    "        return {k: v for k, v in self._feature_map[feature].items()}\n",
    "    \n",
    "    def get_id_files(self, id: str) -> Dict[str, str]:\n",
    "        return {k: v[id] for k, v in self._feature_map.items()}\n",
    "    \n",
    "    def load_feature_data(self, feature: str | None, id: str | None) -> Dict[str, np.ndarray]:\n",
    "        if feature not in self.features:\n",
    "            raise ValueError(f\"Feature {feature} not found in {self.name}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def find_data_sets(cls, root: str | Path) -> Dict[str, 'DataSetObject']:\n",
    "        set_dir_regex = r\".*\" + cls.FEATURE_PREFIX + r\"(.+)\"\n",
    "        # this regex matches the feature directory name and the data set name\n",
    "        # but doesn't work on Windows (? maybe, cant test) because of the forward slashes\n",
    "        feature_dir_regex = r\".*/(.+)/\" + cls.FEATURE_PREFIX + r\"(.+)\"\n",
    "\n",
    "        data_sets: Dict[str, DataSetObject] = {}\n",
    "        for root, dirs, files in os.walk(root):\n",
    "            # check to see if the root is a feature directory,\n",
    "            # if it is, add that feature data to the data set object,\n",
    "            # creating a new data set object if necessary.\n",
    "            if (root_match := re.match(feature_dir_regex, root)):\n",
    "                cls.logger.debug(f\"Feature directory: {root}\")\n",
    "                cls.logger.debug(f\"data set name: {root_match.group(1)}\")\n",
    "                cls.logger.debug(f\"feature is: {root_match.group(2)}\", )\n",
    "                data_set_name = root_match.group(1)\n",
    "                feature_name = root_match.group(2)\n",
    "                if (data_set := data_sets.get(data_set_name)) is None:\n",
    "                    data_set = DataSetObject(root_match.group(1), Path(root).parent)\n",
    "                    data_sets[data_set.name] = data_set\n",
    "                data_set.add_feature_files(feature_name, files)\n",
    "        \n",
    "        return data_sets\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = DataSetObject.find_data_sets(\"../data_sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = sets['walch_et_al']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[ 0.35542702 -0.62391753  0.4841357 ]\n",
      " [-0.94990289  0.97888685 -1.07889197]]\n",
      "Intercept: \n",
      " [0.26968041 1.22598903]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "import numpy as np\n",
    "\n",
    "# X is your input data, a 2D array where each row is a sample and each column is a feature\n",
    "# y is your output data, a 1D array of target values\n",
    "X = np.random.rand(8, 3)\n",
    "y = np.random.rand(8, 2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', model.coef_)\n",
    "# The intercept\n",
    "print('Intercept: \\n', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psg_to_sleep_wake(psg: pl.DataFrame) -> np.array:\n",
    "    # map all positive classes to 1 (sleep)\n",
    "    # retain all 0 (wake) and -1 (mask) classes\n",
    "    return np.where(psg[:, 1] > 0, 1, psg[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def rolling_window(arr, window_size):\n",
    "    strided_axis_0 = max(arr.shape[0] - window_size + 1, 0)\n",
    "    arr_strided = as_strided(arr, shape=(strided_axis_0, window_size), strides=(arr.strides[0], arr.strides[0]))\n",
    "    return arr_strided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]\n",
      " [4 5 6]\n",
      " [5 6 7]\n",
      " [6 7 8]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "print(rolling_window(np.arange(10), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class SleepWakeClassifier:\n",
    "    \"\"\"Eventually, will be the base class. But for now, it will be the only example and it's logistic regression.\n",
    "\n",
    "    That is, this has a matrix A of shape (n_examples, n_features) and a 'bias' vector b of shape (n_examples, 1).\n",
    "\n",
    "    This model then learns A and b to optimize model.predict(x) = sigmoid(A @ x + b), where x is the input.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, lr: float = 0.01, input_dim: int = 7, output_dim: int = 1):\n",
    "        self.model = SGDClassifier(loss='log_loss', learning_rate='optimal', eta0=lr)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.window_step = 1\n",
    "\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        if examples_X or examples_y:\n",
    "            assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "        \n",
    "        training = []\n",
    "        training_iterator = iter(pairs_Xy) if pairs_Xy else zip(examples_X, examples_y)\n",
    "        for X, y in training_iterator:\n",
    "            try:\n",
    "                X_folded = self._fold(X)\n",
    "                y_trimmed = self._trim_labels(y)\n",
    "                if (X_folded.shape[0] != y_trimmed.shape[0]) or (X_folded.shape[0] == 0) or (y_trimmed.shape[0] == 0):\n",
    "                    continue\n",
    "                training.append((X_folded, y_trimmed))\n",
    "            except Exception as e:\n",
    "                print(f\"Error folding or trimming data: {e}\")\n",
    "                continue\n",
    "\n",
    "        for i in range(epochs):\n",
    "            for X, y in training:\n",
    "                if (X.shape[0] != y.shape[0]) or (X.shape[0] == 0) or (y.shape[0] == 0):\n",
    "                    print(f\"epoch {i} shape 0\")\n",
    "                    continue\n",
    "                n_sleep = np.sum(y > 0)\n",
    "                n_wake = np.sum(y == 0)\n",
    "                #\n",
    "                ignore_mask = y >= 0\n",
    "                balancing_weights_ignore_mask = np.where(y == 0, n_sleep / (n_wake + n_sleep), n_wake / (n_wake + n_sleep))\n",
    "\n",
    "                self.model.partial_fit(X, y, classes=[-1, 0, 1], \n",
    "                                       sample_weight=ignore_mask * balancing_weights_ignore_mask)\n",
    "    \n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict(self._fold(sample_X))\n",
    "    \n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict_proba(self._fold(sample_X))\n",
    "\n",
    "    def evaluate(self, examples_X_y: Tuple[np.ndarray, np.ndarray], method: str = 'auc') -> float:\n",
    "        trimmed_labels = self._trim_labels(examples_X_y[1])\n",
    "\n",
    "        if method == 'accuracy':\n",
    "            prediction = self.predict(examples_X_y[0])\n",
    "            return np.mean(prediction == trimmed_labels)\n",
    "        elif method == 'auc':\n",
    "            # get probability of sleep\n",
    "            prediction = self.predict_probabilities(examples_X_y[0])[:, -1]\n",
    "            # drop all -1 (mask) classes\n",
    "            non_mask = trimmed_labels >= 0\n",
    "            prediction = prediction[:len(non_mask)][non_mask]\n",
    "            trimmed_labels = trimmed_labels[non_mask]\n",
    "            auc = roc_auc_score(trimmed_labels, prediction)\n",
    "            print(f\"AUC: {auc}\")\n",
    "            return auc\n",
    "\n",
    "\n",
    "    def _fold(self, input_X: np.ndarray | pl.DataFrame) -> np.array:\n",
    "        if isinstance(input_X, pl.DataFrame):\n",
    "            xa = input_X.to_numpy()\n",
    "        return rolling_window(input_X, self.input_dim)\n",
    "    \n",
    "    def _trim_labels(self, labels_y: pl.DataFrame) -> np.ndarray:\n",
    "        start, end = self._indices_to_trim()\n",
    "        return labels_y[start:-end]\n",
    "        \n",
    "    def _indices_to_trim(self) -> Tuple[int, int]:\n",
    "        # ex: input_dim = 8 => (4, 3)\n",
    "        # ex: input_dim = 7 => (3, 3)\n",
    "        # ex: input_dim = 6 => (3, 2)\n",
    "        return (self.input_dim // 2, self.input_dim - (self.input_dim // 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_activity_X_PSG_y(data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    activity_0 = data_set.get_feature_data(\"activity\", id)\n",
    "    psg_0 = data_set.get_feature_data(\"psg\", id)\n",
    "\n",
    "    # trim the activity and psg data to both end when the 0th column (time) of either ends\n",
    "    end_time = min(activity_0[-1, 0], psg_0[-1, 0])\n",
    "    rows_retained = sum(activity_0[:, 0] <= end_time)\n",
    "    activity_0 = activity_0.filter(activity_0[:, 0] <= end_time)\n",
    "    psg_0 = psg_0.filter(psg_0[:, 0] <= end_time)\n",
    "\n",
    "    X = activity_0[:, 1].to_numpy()\n",
    "    # make the reshape(-1, 2) non-ragged \n",
    "    if res := X.shape[0] % 2:\n",
    "        X = X[:-res]\n",
    "    X = X.reshape((-1, 2)).sum(axis=1)\n",
    "    y = psg_to_sleep_wake(psg_0)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_activity_X_PSG_y(w, w.ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "train_indices, test_indices = [], []\n",
    "\n",
    "def run_split(train_index, test_index, preprocessed_data_set\n",
    "              ) -> Tuple[SleepWakeClassifier, \n",
    "                         List[Tuple[np.ndarray, np.ndarray]]]:\n",
    "    swc = SleepWakeClassifier()\n",
    "    training_pairs = [preprocessed_data_set[i] for i in train_index]\n",
    "    swc.train(pairs_Xy=training_pairs, epochs=100)\n",
    "\n",
    "    testing_pairs = [preprocessed_data_set[i] for i in test_index]\n",
    "\n",
    "    return swc#, [(swc.predict(X), y) for X, y in testing_pairs]\n",
    "\n",
    "split_models: List[SleepWakeClassifier] = []\n",
    "test_indices = []\n",
    "\n",
    "preprocessed_data = [get_activity_X_PSG_y(w, i) for i in w.ids]\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in tqdm(loo.split(w.ids)):\n",
    "    try:\n",
    "        model = run_split(train_index, test_index, preprocessed_data_set=preprocessed_data)\n",
    "        split_models.append(model)\n",
    "        test_indices.append(test_index[0])\n",
    "    except:\n",
    "        print(test_index[0], w.ids[test_index[0]],\"failed testing\")\n",
    "    i += 1\n",
    "    if i >= 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = split_models[0].predict_probabilities(preprocessed_data[test_indices[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.34029284164859\n",
      "AUC: 0.562686312920535\n"
     ]
    }
   ],
   "source": [
    "aucs = [split_models[i].evaluate(preprocessed_data[test_indices[i]]) for i in range(len(test_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pisces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
