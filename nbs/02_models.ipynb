{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Tuple\n",
    "from pisces.data_sets import DataSetObject, ModelInput1D, ModelInputSpectrogram, ModelOutputType, DataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import abc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SleepWakeClassifier(abc.ABC):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def get_needed_X_y(self, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        pass\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        pass\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SGDLogisticRegression(SleepWakeClassifier):\n",
    "    \"\"\"Uses Sk-Learn's `SGDCLassifier` to train a logistic regression model. The SGD aspect allows for online learning, or custom training regimes through the `partial_fit` method.\n",
    "     \n",
    "    The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a `StandardScaler` before being passed to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_processor: DataProcessor, lr: float = 0.15, ):\n",
    "        self.model = SGDClassifier(loss='log_loss',\n",
    "                                   learning_rate='adaptive',\n",
    "                                   penalty='l1',\n",
    "                                   eta0=lr,\n",
    "                                   class_weight='balanced',\n",
    "                                   warm_start=True)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pipeline = make_pipeline(self.scaler, self.model)\n",
    "        if not isinstance(data_processor.model_input, ModelInput1D):\n",
    "            raise ValueError(\"Model input must be set to 1D on the data processor\")\n",
    "        if not data_processor.model_output == ModelOutputType.SleepWake:\n",
    "            raise ValueError(\"Model output must be set to SleepWake on the data processor\")\n",
    "        self.data_processor = data_processor\n",
    "\n",
    "    def get_needed_X_y(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return self.data_processor.get_1D_X_y(id)\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[np.ndarray]=[], \n",
    "              examples_y: List[np.ndarray]=[], \n",
    "              pairs_Xy: List[Tuple[np.ndarray, np.ndarray]]=[], \n",
    "              ):\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y` \n",
    "        and ready to be passed to the model.\n",
    "        \"\"\"\n",
    "        if (examples_X and not examples_y) or (examples_y and not examples_X):\n",
    "            raise ValueError(\"If providing examples, must provide both X and y\")\n",
    "        else:\n",
    "            if examples_X and examples_y:\n",
    "                assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "\n",
    "        X = [self._input_preprocessing(example) for example in examples_X]\n",
    "\n",
    "        Xs = np.concatenate(X, axis=0)\n",
    "        ys = np.concatenate(examples_y, axis=0)\n",
    "\n",
    "        selector = ys >= 0\n",
    "        Xs = Xs[selector]\n",
    "        ys = ys[selector]\n",
    "\n",
    "        self.pipeline.fit(Xs, ys)\n",
    "    \n",
    "    def _input_preprocessing(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.scaler.transform(X)\n",
    "    \n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        return self.model.predict(self._input_preprocessing(sample_X))\n",
    "    \n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        return self.model.predict_proba(self._input_preprocessing(sample_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mads Olsen et all classifier\n",
    "\n",
    "We have downloaded the saved model weights from a [research repository from Mads Olsen's group](https://github.com/MADSOLSEN/SleepStagePrediction), and converted those into a saved Keras model to remove the need to re-define all of the layers. This conversion process is shown in `../analyses/convert_mads_olsen_model_to_keras.ipynb`.\n",
    "\n",
    "Thus, we have a TensorFlow model that we can run inference on, and we could train it if we wanted to.\n",
    "\n",
    "For simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs:\n",
    "1. 3-axis Accelerometer data\n",
    "2. PPG (photoplethysmogram) data\n",
    "\n",
    "Based on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tavel\\Desktop\\Internship-Arcascope\\2024_internship\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:187: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "\n",
    "from pisces.mads_olsen_support import *\n",
    "from pisces.utils import split_analysis\n",
    "\n",
    "\n",
    "class MOResUNetPretrained(SleepWakeClassifier):\n",
    "    tf_model = load_saved_keras()\n",
    "    config = MO_PREPROCESSING_CONFIG\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_hz: int = FS,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MOResUNetPretrained classifier.\n",
    "\n",
    "        Args:\n",
    "            sampling_hz (int, optional): The sampling frequency in Hz. Defaults to FS.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sampling_hz = sampling_hz\n",
    "\n",
    "    def prepare_set_for_training(self, \n",
    "                                 data_set: DataSetObject, ids: List[str] | None = None,\n",
    "                                 max_workers: int | None = None \n",
    "                                 ) -> List[Tuple[np.ndarray, np.ndarray] | None]:\n",
    "        \"\"\"\n",
    "        Prepare the data set for training.\n",
    "\n",
    "        Args:\n",
    "            data_set (DataSetObject): The data set to prepare for training.\n",
    "            ids (List[str], optional): The IDs to prepare. Defaults to None.\n",
    "            max_workers (int, optional): The number of workers to use for parallel processing. Defaults to None, which uses all available cores. Setting to a negative number leaves that many cores unused. For example, if my machine has 4 cores and I set max_workers to -1, then 3 = 4 - 1 cores will be used; if max_workers=-3 then 1 = 4 - 3 cores are used.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[np.ndarray, np.ndarray] | None]: A list of tuples, where each tuple is the result of `get_needed_X_y` for a given ID. An empty list indicates an error occurred during processing.\n",
    "        \"\"\"\n",
    "        if ids is None:\n",
    "            ids = data_set.ids\n",
    "        results = []\n",
    "        \n",
    "        if ids:\n",
    "            data_set_and_ids = [(data_set, id) for id in ids]\n",
    "            # Get the number of available CPU cores\n",
    "            num_cores = multiprocessing.cpu_count()\n",
    "            workers_to_use = max_workers if max_workers is not None else num_cores\n",
    "            if (workers_to_use > num_cores):\n",
    "                warnings.warn(f\"Attempting to use {max_workers} but only have {num_cores}. Running with {num_cores} workers.\")\n",
    "                workers_to_use = num_cores\n",
    "            if workers_to_use <= 0:\n",
    "                workers_to_use = num_cores + max_workers\n",
    "            if workers_to_use < 1:\n",
    "                # do this check second, NOT with elif, to verify we're still in a valid state\n",
    "                raise ValueError(f\"With `max_workers` == {max_workers}, we end up with max_workers + num_cores ({max_workers} + {num_cores}) which is less than 1. This is an error.\")\n",
    "\n",
    "            print(f\"Using {workers_to_use} of {num_cores} cores ({int(100 * workers_to_use / num_cores)}%) for parallel preprocessing.\")\n",
    "            print(f\"This can cause memory or heat issues if  is too high; if you run into problems, call prepare_set_for_training() again with max_workers = -1, going more negative if needed. (See the docstring for more info.)\")\n",
    "\n",
    "            # Create a pool of workers\n",
    "            with ProcessPoolExecutor(max_workers=workers_to_use) as executor:\n",
    "                results = list(\n",
    "                    executor.map(\n",
    "                        self.get_needed_X_y_from_pair, \n",
    "                        data_set_and_ids\n",
    "                    ))\n",
    "        else:\n",
    "            warnings.warn(\"No IDs found in the data set.\")\n",
    "            return results\n",
    "        return results\n",
    "    \n",
    "    def get_needed_X_y_from_pair(self, pair: Tuple[DataSetObject, str]) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        \"\"\"\n",
    "        Get the needed X and y data from a pair of data set and ID.\n",
    "\n",
    "        Args:\n",
    "            pair (Tuple[DataSetObject, str]): The pair of data set and ID.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray] | None: The X and y data as a tuple, or None if an error occurred.\n",
    "        \"\"\"\n",
    "        data_set, id = pair\n",
    "        print(f\"getting needed X, y for {id}\")\n",
    "        return self.get_needed_X_y(data_set, id)\n",
    "    \n",
    "    def get_needed_X_y(self, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        # WIP\n",
    "        \"\"\"\n",
    "        accelerometer = data_set.get_feature_data(\"accelerometer\", id)\n",
    "        psg = data_set.get_feature_data(\"psg\", id)\n",
    "\n",
    "        if accelerometer is None or psg is None:\n",
    "            print(f\"ID {id} {'psg' if psg is None else 'accelerometer'} not found in {data_set.name}\")\n",
    "            return None\n",
    "        \n",
    "        print(\"sampling hz:\", self.sampling_hz)\n",
    "        accelerometer = fill_gaps_in_accelerometer_data(accelerometer, smooth=False, final_sampling_rate_hz=self.sampling_hz)\n",
    "        stop_time = min(accelerometer[:, 0].max(), psg[:, 0].max())\n",
    "        accelerometer = accelerometer.filter(accelerometer[:, 0] <= stop_time)\n",
    "        psg = psg.filter(psg[:, 0] <= stop_time)\n",
    "\n",
    "        mirrored_spectro = self._input_preprocessing(accelerometer)\n",
    "\n",
    "        return mirrored_spectro, psg_to_sleep_wake(psg)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[pl.DataFrame] = [], \n",
    "              examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        \"\"\"Training is not implemented yet for this model. You can run inference, though, using `predict_probabilities` and `predict`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return np.argmax(self.predict_probabilities(sample_X), axis=1)\n",
    "\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        if isinstance(sample_X, pl.DataFrame):\n",
    "            sample_X = sample_X.to_numpy()\n",
    "        return self._evaluate_tf_model(sample_X)\n",
    "\n",
    "    def roc_curve(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        raise NotImplementedError\n",
    "    def roc_auc(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def _spectrogram_preprocessing(cls, acc_xyz: np.ndarray) -> np.ndarray:\n",
    "        return cls._preprocessing(acc_xyz)\n",
    "\n",
    "    @classmethod\n",
    "    def _input_preprocessing(\n",
    "        cls,\n",
    "        acc_xyz: pl.DataFrame | np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        spec = cls._spectrogram_preprocessing(acc_xyz)\n",
    "\n",
    "        # We will copy the spectrogram to both channels, flipping it on channel 1\n",
    "        input_shape = (1, *MO_UNET_CONFIG['input_shape'])\n",
    "        inputs_len = input_shape[1]\n",
    "\n",
    "        inputs = np.zeros(shape=input_shape, dtype=np.float32)\n",
    "        # We must do some careful work with indices to not overflow arrays\n",
    "        spec = spec[:inputs_len].astype(np.float32) # protect agains spec.len > input_shape\n",
    "\n",
    "        #! careful, order matters here. We first trim spec to make sure it'll fit into inputs,\n",
    "        # then compute the new length which we KNOW is <= inputs_len\n",
    "        spec_len = spec.shape[0]\n",
    "        # THEN we assign only as much inputs as spec covers\n",
    "        inputs[0, : spec_len, :, 0] = spec # protect agains spec_len < input_shape\n",
    "        inputs[0, : spec_len, :, 1] = spec[:, ::-1]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _evaluate_tf_model(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # set input tensor to FLOAT32\n",
    "        inputs = inputs.astype(np.float32)\n",
    "\n",
    "        # run inference\n",
    "        preds = self.tf_model.predict(inputs)\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    @classmethod\n",
    "    def _preprocessing(\n",
    "        cls,\n",
    "        acc: pl.DataFrame | np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The Mads Olsen repo uses a list of transformations\n",
    "        \"\"\"\n",
    "        if isinstance(acc, pl.DataFrame):\n",
    "            acc = acc.to_numpy()\n",
    "        x_ = acc[:, 0]\n",
    "        y_ = acc[:, 1]\n",
    "        z_ = acc[:, 2]\n",
    "        for step in cls.config[\"preprocessing\"]:\n",
    "            fn = eval(step[\"type\"])  # convert string version to function in environment\n",
    "            fn_args = partial(\n",
    "                fn, **step[\"args\"]\n",
    "            )  # fill in the args given, which must be everything besides numerical input\n",
    "\n",
    "            # apply\n",
    "            x_ = fn_args(x_)\n",
    "            y_ = fn_args(y_)\n",
    "            z_ = fn_args(z_)\n",
    "\n",
    "        spec = x_ + y_ + z_\n",
    "        spec /= 3.0\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def evaluate_data_set(self, data_set: DataSetObject, exclude: List[str] = [], max_workers: int = None) -> Tuple[Dict[str, dict], list]:\n",
    "        filtered_ids = [id for id in data_set.ids if id not in exclude]\n",
    "        mo_preprocessed_data = [\n",
    "            (d, i) \n",
    "            for (d, i) in zip(\n",
    "                self.prepare_set_for_training(data_set, filtered_ids, max_workers=max_workers),\n",
    "                filtered_ids) \n",
    "            if d is not None\n",
    "        ]\n",
    "\n",
    "        evaluations: Dict[str, dict] = {}\n",
    "        for i, ((X, y), id) in enumerate(mo_preprocessed_data):\n",
    "            y_hat_proba = self.predict_probabilities(X)\n",
    "            y_hat_sleep_proba = (1 - y_hat_proba[:, :, 0]).reshape(-1,)\n",
    "            analysis = split_analysis(y, y_hat_sleep_proba)\n",
    "            evaluations[id] = analysis\n",
    "            print(f\"Processing {i+1} of {len(mo_preprocessed_data)} ({id})... AUROC: {analysis['auc']}\")\n",
    "        return evaluations, mo_preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Type\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "\n",
    "class SplitMaker:\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LeaveOneOutSplitter(SplitMaker):\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        loo = LeaveOneOut()\n",
    "        return loo.split(ids)\n",
    "\n",
    "def run_split(train_indices, \n",
    "              preprocessed_data_set: List[Tuple[np.ndarray, np.ndarray]], \n",
    "              swc: SleepWakeClassifier) -> SleepWakeClassifier:\n",
    "    training_pairs = [\n",
    "        preprocessed_data_set[i][0]\n",
    "        for i in train_indices\n",
    "        if preprocessed_data_set[i][0] is not None\n",
    "    ]\n",
    "    swc.train(pairs_Xy=training_pairs)\n",
    "\n",
    "    return swc\n",
    "\n",
    "def run_splits(split_maker: SplitMaker, w: DataSetObject, swc_class: Type[SleepWakeClassifier], exclude: List[str] = []) -> Tuple[\n",
    "        List[SleepWakeClassifier], \n",
    "        List[np.ndarray],\n",
    "        List[List[List[int]]]]:\n",
    "    split_models: List[swc_class] = []\n",
    "    test_indices = []\n",
    "    splits = []\n",
    "\n",
    "    preprocessed_data = [(swc_class().get_needed_X_y(w, i), i) for i in w.ids if i not in exclude]\n",
    "\n",
    "    for train_index, test_index in tqdm(split_maker.split(w.ids)):\n",
    "        if preprocessed_data[test_index[0]][0] is None:\n",
    "            continue\n",
    "        model = run_split(train_indices=train_index,\n",
    "                        preprocessed_data_set=preprocessed_data,\n",
    "                        swc=swc_class())\n",
    "        split_models.append(model)\n",
    "        test_indices.append(test_index[0])\n",
    "        splits.append([train_index, test_index])\n",
    "        # break\n",
    "    \n",
    "    return split_models, preprocessed_data, splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
