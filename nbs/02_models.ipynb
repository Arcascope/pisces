{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Tuple\n",
    "from pisces.data_sets import DataSetObject, ModelInput1D, ModelInputSpectrogram, ModelOutputType, DataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import abc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SleepWakeClassifier(abc.ABC):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def get_needed_X_y(self, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        pass\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        pass\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SGDLogisticRegression(SleepWakeClassifier):\n",
    "    \"\"\"Uses Sk-Learn's `SGDCLassifier` to train a logistic regression model. The SGD aspect allows for online learning, or custom training regimes through the `partial_fit` method.\n",
    "     \n",
    "    The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a `StandardScaler` before being passed to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_processor: DataProcessor, lr: float = 0.15, ):\n",
    "        self.model = SGDClassifier(loss='log_loss',\n",
    "                                   learning_rate='adaptive',\n",
    "                                   penalty='l1',\n",
    "                                   eta0=lr,\n",
    "                                   class_weight='balanced',\n",
    "                                   warm_start=True)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pipeline = make_pipeline(self.scaler, self.model)\n",
    "        if not isinstance(data_processor.model_input, ModelInput1D):\n",
    "            raise ValueError(\"Model input must be set to 1D on the data processor\")\n",
    "        if not data_processor.model_output == ModelOutputType.SleepWake:\n",
    "            raise ValueError(\"Model output must be set to SleepWake on the data processor\")\n",
    "        self.data_processor = data_processor\n",
    "\n",
    "    def get_needed_X_y(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return self.data_processor.get_1D_X_y(id)\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[np.ndarray]=[], \n",
    "              examples_y: List[np.ndarray]=[], \n",
    "              pairs_Xy: List[Tuple[np.ndarray, np.ndarray]]=[], \n",
    "              ):\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y` \n",
    "        and ready to be passed to the model.\n",
    "        \"\"\"\n",
    "        if (examples_X and not examples_y) or (examples_y and not examples_X):\n",
    "            raise ValueError(\"If providing examples, must provide both X and y\")\n",
    "        else:\n",
    "            if examples_X and examples_y:\n",
    "                assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "\n",
    "        X = [self._input_preprocessing(example) for example in examples_X]\n",
    "\n",
    "        Xs = np.concatenate(X, axis=0)\n",
    "        ys = np.concatenate(examples_y, axis=0)\n",
    "\n",
    "        selector = ys >= 0\n",
    "        Xs = Xs[selector]\n",
    "        ys = ys[selector]\n",
    "\n",
    "        self.pipeline.fit(Xs, ys)\n",
    "    \n",
    "    def _input_preprocessing(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.scaler.transform(X)\n",
    "    \n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        return self.model.predict(self._input_preprocessing(sample_X))\n",
    "    \n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        return self.model.predict_proba(self._input_preprocessing(sample_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mads Olsen et all classifier\n",
    "\n",
    "We have downloaded the saved model weights from a [research repository from Mads Olsen's group](https://github.com/MADSOLSEN/SleepStagePrediction), and converted those into a saved Keras model to remove the need to re-define all of the layers. This conversion process is shown in `../analyses/convert_mads_olsen_model_to_keras.ipynb`.\n",
    "\n",
    "Thus, we have a TensorFlow model that we can run inference on, and we could train it if we wanted to.\n",
    "\n",
    "For simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs:\n",
    "1. 3-axis Accelerometer data\n",
    "2. PPG (photoplethysmogram) data\n",
    "\n",
    "Based on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tavel\\Desktop\\Internship-Arcascope\\2024_internship\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:187: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "\n",
    "from pisces.mads_olsen_support import *\n",
    "from pisces.utils import split_analysis\n",
    "\n",
    "\n",
    "class MOResUNetPretrained(SleepWakeClassifier):\n",
    "    tf_model = load_saved_keras()\n",
    "    config = MO_PREPROCESSING_CONFIG\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_hz: int = FS,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MOResUNetPretrained classifier.\n",
    "\n",
    "        Args:\n",
    "            sampling_hz (int, optional): The sampling frequency in Hz. Defaults to FS.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sampling_hz = sampling_hz\n",
    "\n",
    "    def prepare_set_for_training(self, \n",
    "                                 data_processor: DataProcessor, \n",
    "                                 ids: List[str],\n",
    "                                 max_workers: int | None = None \n",
    "                                 ) -> List[Tuple[np.ndarray, np.ndarray] | None]:\n",
    "        \"\"\"\n",
    "        Prepare the data set for training.\n",
    "\n",
    "        Args:\n",
    "            data_set (DataSetObject): The data set to prepare for training.\n",
    "            ids (List[str], optional): The IDs to prepare. Defaults to None.\n",
    "            max_workers (int, optional): The number of workers to use for parallel processing. Defaults to None, which uses all available cores. Setting to a negative number leaves that many cores unused. For example, if my machine has 4 cores and I set max_workers to -1, then 3 = 4 - 1 cores will be used; if max_workers=-3 then 1 = 4 - 3 cores are used.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[np.ndarray, np.ndarray] | None]: A list of tuples, where each tuple is the result of `get_needed_X_y` for a given ID. An empty list indicates an error occurred during processing.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        processor_and_ids = [(data_processor, id) for id in ids]\n",
    "        # Get the number of available CPU cores\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        workers_to_use = max_workers if max_workers is not None else num_cores\n",
    "        if (workers_to_use > num_cores):\n",
    "            warnings.warn(f\"Attempting to use {max_workers} but only have {num_cores}. Running with {num_cores} workers.\")\n",
    "            workers_to_use = num_cores\n",
    "        if workers_to_use <= 0:\n",
    "            workers_to_use = num_cores + max_workers\n",
    "        if workers_to_use < 1:\n",
    "            # do this check second, NOT with elif, to verify we're still in a valid state\n",
    "            raise ValueError(f\"With `max_workers` == {max_workers}, we end up with max_workers + num_cores ({max_workers} + {num_cores}) which is less than 1. This is an error.\")\n",
    "\n",
    "        print(f\"Using {workers_to_use} of {num_cores} cores ({int(100 * workers_to_use / num_cores)}%) for parallel preprocessing.\")\n",
    "        print(f\"This can cause memory or heat issues if  is too high; if you run into problems, call prepare_set_for_training() again with max_workers = -1, going more negative if needed. (See the docstring for more info.)\")\n",
    "        # Create a pool of workers\n",
    "        with ProcessPoolExecutor(max_workers=workers_to_use) as executor:\n",
    "            results = list(\n",
    "                executor.map(\n",
    "                    self.get_needed_X_y,\n",
    "                    processor_and_ids,\n",
    "                ))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_needed_X_y(self, data_processor: DataProcessor, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return data_processor.get_spectrogram(id)\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[pl.DataFrame] = [], \n",
    "              examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        \"\"\"Training is not implemented yet for this model. You can run inference, though, using `predict_probabilities` and `predict`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return np.argmax(self.predict_probabilities(sample_X), axis=1)\n",
    "\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        if isinstance(sample_X, pl.DataFrame):\n",
    "            sample_X = sample_X.to_numpy()\n",
    "        return self._evaluate_tf_model(sample_X)\n",
    "\n",
    "    def _evaluate_tf_model(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # set input tensor to FLOAT32\n",
    "        inputs = inputs.astype(np.float32)\n",
    "\n",
    "        # run inference\n",
    "        preds = self.tf_model.predict(inputs)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def evaluate_data_set(self, \n",
    "                          data_processor: DataProcessor, \n",
    "                          exclude: List[str] = [], \n",
    "                          max_workers: int = None) -> Tuple[Dict[str, dict], list]:\n",
    "        data_set = data_processor.data_set\n",
    "        filtered_ids = [id for id in data_set.ids if id not in exclude]\n",
    "        mo_preprocessed_data = [\n",
    "            (d, i) \n",
    "            for (d, i) in zip(\n",
    "                self.prepare_set_for_training(data_processor, filtered_ids, max_workers=max_workers),\n",
    "                filtered_ids) \n",
    "            if d is not None\n",
    "        ]\n",
    "\n",
    "        evaluations: Dict[str, dict] = {}\n",
    "        for i, ((X, y), id) in enumerate(mo_preprocessed_data):\n",
    "            y_hat_proba = self.predict_probabilities(X)\n",
    "            y_hat_sleep_proba = (1 - y_hat_proba[:, :, 0]).reshape(-1,)\n",
    "            analysis = split_analysis(y, y_hat_sleep_proba)\n",
    "            evaluations[id] = analysis\n",
    "            print(f\"Processing {i+1} of {len(mo_preprocessed_data)} ({id})... AUROC: {analysis['auc']}\")\n",
    "        return evaluations, mo_preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Type\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "\n",
    "class SplitMaker:\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LeaveOneOutSplitter(SplitMaker):\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        loo = LeaveOneOut()\n",
    "        return loo.split(ids)\n",
    "\n",
    "def run_split(train_indices, \n",
    "              preprocessed_data_set: List[Tuple[np.ndarray, np.ndarray]], \n",
    "              swc: SleepWakeClassifier) -> SleepWakeClassifier:\n",
    "    training_pairs = [\n",
    "        preprocessed_data_set[i][0]\n",
    "        for i in train_indices\n",
    "        if preprocessed_data_set[i][0] is not None\n",
    "    ]\n",
    "    swc.train(pairs_Xy=training_pairs)\n",
    "\n",
    "    return swc\n",
    "\n",
    "def run_splits(split_maker: SplitMaker, w: DataSetObject, swc_class: Type[SleepWakeClassifier], exclude: List[str] = []) -> Tuple[\n",
    "        List[SleepWakeClassifier], \n",
    "        List[np.ndarray],\n",
    "        List[List[List[int]]]]:\n",
    "    split_models: List[swc_class] = []\n",
    "    test_indices = []\n",
    "    splits = []\n",
    "\n",
    "    preprocessed_data = [(swc_class().get_needed_X_y(w, i), i) for i in w.ids if i not in exclude]\n",
    "\n",
    "    for train_index, test_index in tqdm(split_maker.split(w.ids)):\n",
    "        if preprocessed_data[test_index[0]][0] is None:\n",
    "            continue\n",
    "        model = run_split(train_indices=train_index,\n",
    "                        preprocessed_data_set=preprocessed_data,\n",
    "                        swc=swc_class())\n",
    "        split_models.append(model)\n",
    "        test_indices.append(test_index[0])\n",
    "        splits.append([train_index, test_index])\n",
    "        # break\n",
    "    \n",
    "    return split_models, preprocessed_data, splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
