{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Module of utility functions used throughout the package, for things like common preprocessing steps many models are likely to use, or useful for multiple points of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from enum import Enum, auto\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.metrics import auc as auc_score\n",
    "from typing import Dict, List, Optional, Tuple, Callable\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def determine_header_rows_and_delimiter(\n",
    "    filename: Path | str\n",
    ") -> Tuple[Optional[int], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Given a filename pointing at a CSV files, decides:\n",
    "     * how many header lines there are (based on first line starting with a digit)\n",
    "     * the delimiter-- right now tries whitespace and comma\n",
    "\n",
    "    Returns one of:\n",
    "     - (number of header rows, column delimiter),\n",
    "     - (number of header rows, None) if the delimiter could not be inferred,\n",
    "     - (None, None) if CSV has no numerical rows,\n",
    "\n",
    "    :param filename: CSV Path or filepath literal\n",
    "    :return: header and delimiter information, if possible.\n",
    "    \"\"\"\n",
    "    MAX_ROWS = 100  # if you don't have data within first 100 lines, exit.\n",
    "    header_row_count = 0\n",
    "    with open(filename) as f:\n",
    "        header_found = False\n",
    "        line = \"\"  # modified in while below\n",
    "\n",
    "        # Search over lines until we find one starting with a digit\n",
    "        while not header_found:\n",
    "            line = f.readline()\n",
    "            if line == \"\":  # last line of file reached\n",
    "                return None, None\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                int(line[0])\n",
    "                header_found = True\n",
    "            except ValueError:\n",
    "                header_row_count += 1\n",
    "            except IndexError:\n",
    "                header_row_count += 1\n",
    "\n",
    "            # guard against infinite loop\n",
    "            if header_row_count >= MAX_ROWS:\n",
    "                return None, None\n",
    "\n",
    "        # Now try splitting that first line of data\n",
    "        delim_guesses = [\" \", \",\", \", \"]\n",
    "\n",
    "        for guess in delim_guesses:\n",
    "            try:\n",
    "                comps = line.split(guess)  # whitespace separated?\n",
    "                float(comps[0])\n",
    "                return header_row_count, guess\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return header_row_count, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_ADS(\n",
    "    time_xyz: pl.DataFrame | np.ndarray,\n",
    "    resample_hz: float | None = None,\n",
    "    bin_size_seconds: float = 15,\n",
    "    prefix: str = \"\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"ADS algorithm for activity counts, developed by Arcascope with support from the NHRC.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "     - `time_xyz`: numpy array with shape (N_samples, 4) where the 4 coordinates are: [time, x, y, z] \n",
    "     - `resample_hz`: `float` sampling frequency of thetime_xyz \n",
    "    \"\"\"\n",
    "    data_shape_error = ValueError(\n",
    "            f\"`time_xyz` must have shape (N_samples, 4) but has shape {time_xyz.shape}\"\n",
    "        )\n",
    "    try:\n",
    "        assert (len(time_xyz.shape) == 2 and time_xyz.shape[1] == 4)\n",
    "    except AssertionError as exc:\n",
    "        raise data_shape_error from exc\n",
    "\n",
    "    if isinstance(time_xyz, pl.DataFrame):\n",
    "        time_data_raw = time_xyz[:, 0].to_numpy()\n",
    "        x_accel = time_xyz[:, 1].to_numpy()\n",
    "        y_accel = time_xyz[:, 2].to_numpy()\n",
    "        z_accel = time_xyz[:, 3].to_numpy()\n",
    "    elif isinstance(time_xyz, np.ndarray):\n",
    "        time_data_raw = time_xyz[:, 0]\n",
    "        x_accel = time_xyz[:, 1]\n",
    "        y_accel = time_xyz[:, 2]\n",
    "        z_accel = time_xyz[:, 3]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported type for `time_xyz`: {type(time_xyz)}\")\n",
    "\n",
    "    # Interpolate to sampling Hz\n",
    "    if resample_hz is not None:\n",
    "        time_values = np.arange(\n",
    "            np.amin(time_data_raw), np.amax(time_data_raw), 1 / resample_hz\n",
    "        )\n",
    "        # Must do each coordinate separately\n",
    "        x_data = np.interp(time_values, time_data_raw, x_accel)\n",
    "        y_data = np.interp(time_values, time_data_raw, y_accel)\n",
    "        z_data = np.interp(time_values, time_data_raw, z_accel)\n",
    "    else:\n",
    "        x_data = x_accel\n",
    "        y_data = y_accel\n",
    "        z_data = z_accel\n",
    "\n",
    "    # Calculate \"amplitude\" = timeseries of 2-norm of (x, y, z)\n",
    "    amplitude = np.linalg.norm(np.array([x_data, y_data, z_data]), axis = 0)\n",
    "\n",
    "    abs_amplitude_deriv = np.abs(np.diff(amplitude))\n",
    "    abs_amplitude_deriv = np.insert(abs_amplitude_deriv, 0, 0)\n",
    "\n",
    "    # Binning step\n",
    "    # Sum abs_amplitude_deriv in time-based windows\n",
    "    # ex: bin_size_seconds = 15\n",
    "    # Step from first to last time by 15 seconds\n",
    "    time_counts = np.arange(\n",
    "        np.amin(time_data_raw), np.amax(time_data_raw), bin_size_seconds\n",
    "    )\n",
    "\n",
    "    # Convert time at 50 hz to \"# of 15 second windows past start\"\n",
    "    bin_values = (time_values - time_values[0]).astype(int) // bin_size_seconds\n",
    "    sums_in_bins = np.bincount(bin_values, abs_amplitude_deriv)\n",
    "    sums_in_bins[sums_in_bins <= 0.05 * max(sums_in_bins)] = 0.0\n",
    "    return time_counts, sums_in_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_activity_counts_te_Lindert_et_al(\n",
    "    time_xyz, axis: int = 3, prefix: str = \"\"\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Implementation of the reverse-engineered activity count algorithm from\n",
    "    te Lindert BH, Van Someren EJ. Sleep. 2013\n",
    "    Sleep estimates using microelectromechanical systems (MEMS). \n",
    "    doi: 10.5665/sleep.2648\n",
    "    \n",
    "    :param time_xyz: `np.ndarray` loaded from timestamped triaxial accelerometer CSV. Shape (N, 4)\n",
    "    :return: (time, activity counts with 15 second epoch)\n",
    "    \"\"\"\n",
    "\n",
    "    # a helper function to calculate max over 2 epochs\n",
    "    def max2epochs(data, fs, epoch):\n",
    "        data = data.flatten()\n",
    "\n",
    "        seconds = int(np.floor(np.shape(data)[0] / fs))\n",
    "        data = np.abs(data)\n",
    "        data = data[0 : int(seconds * fs)]\n",
    "\n",
    "        data = data.reshape(fs, seconds, order=\"F\").copy()\n",
    "\n",
    "        data = data.max(0)\n",
    "        data = data.flatten()\n",
    "        N = np.shape(data)[0]\n",
    "        num_epochs = int(np.floor(N / epoch))\n",
    "        data = data[0 : (num_epochs * epoch)]\n",
    "\n",
    "        data = data.reshape(epoch, num_epochs, order=\"F\").copy()\n",
    "        epoch_data = np.sum(data, axis=0)\n",
    "        epoch_data = epoch_data.flatten()\n",
    "\n",
    "        return epoch_data\n",
    "    \n",
    "    # sort by time\n",
    "    sort_idx = np.argsort(time_xyz[:, 0])\n",
    "    time_xyz = time_xyz[sort_idx]\n",
    "    \n",
    "    fs = 50\n",
    "    time = np.arange(time_xyz[0, 0], time_xyz[-1, 0], 1.0 / fs)\n",
    "    z_data = np.interp(time, time_xyz[:, 0], time_xyz[:, axis])\n",
    "\n",
    "    cf_low = 3\n",
    "    cf_hi = 11\n",
    "    order = 5\n",
    "    w1 = cf_low / (fs / 2)\n",
    "    w2 = cf_hi / (fs / 2)\n",
    "    pass_band = [w1, w2]\n",
    "    b, a = butter(order, pass_band, \"bandpass\")\n",
    "\n",
    "    z_filt = filtfilt(b, a, z_data)\n",
    "    z_filt = np.abs(z_filt)\n",
    "\n",
    "    top_edge = 5\n",
    "    bottom_edge = 0\n",
    "    number_of_bins = 128\n",
    "\n",
    "    bin_edges = np.linspace(bottom_edge, top_edge, number_of_bins + 1)\n",
    "    binned = np.digitize(z_filt, bin_edges)\n",
    "    epoch = 15\n",
    "    counts = max2epochs(binned, fs, epoch)\n",
    "    counts = (counts - 18) * 3.07\n",
    "    counts[counts < 0] = 0\n",
    "\n",
    "    time_counts = np.linspace(time_xyz[0, 0], time_xyz[-1, 0], np.shape(counts)[0])\n",
    "    time_counts = np.expand_dims(time_counts, axis=1)\n",
    "    counts = np.expand_dims(counts, axis=1)\n",
    "\n",
    "    return time_counts, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ActivityCountAlgorithm(Enum):\n",
    "    te_Lindert_et_al = 0\n",
    "    ADS = 2\n",
    "\n",
    "\n",
    "def build_activity_counts(\n",
    "    data,\n",
    "    axis: int = 3,\n",
    "    prefix: str = \"\",\n",
    "    algorithm: ActivityCountAlgorithm = ActivityCountAlgorithm.ADS\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if algorithm == ActivityCountAlgorithm.ADS:\n",
    "        return build_ADS(data)\n",
    "    if algorithm == ActivityCountAlgorithm.te_Lindert_et_al:\n",
    "        return build_activity_counts_te_Lindert_et_al(data, axis, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_scores_CDF(scores: List[float], ax: plt.Axes = None):\n",
    "    \"\"\"Plot the cumulative dist function (CDF) of the scores.\"\"\"\n",
    "    # plt.figure(figsize=(20, 10))\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 1)\n",
    "    _ = ax.hist(scores,\n",
    "                cumulative=True,\n",
    "                density=True,\n",
    "                bins=100)\n",
    "\n",
    "\n",
    "def plot_scores_PDF(scores: List[float], ax: plt.Axes = None):\n",
    "    \"\"\"Plot the probability dist function (PDF) of the scores.\"\"\"\n",
    "    ax_ = ax\n",
    "    if ax is None:\n",
    "        _, ax_ = plt.subplots()\n",
    "    ax_.set_xlim(0, 1)\n",
    "    _ = ax_.hist(scores, bins=20)\n",
    "\n",
    "    # plot the mean as a vertical 'tab:orange' line\n",
    "    ax_.axvline(np.mean(scores), color='tab:orange', linestyle='--', label=f\"Mean: {np.mean(scores):.3f}\")\n",
    "    if ax is None:\n",
    "        ax_.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def constant_interp(\n",
    "    x: np.ndarray, xp: np.ndarray, yp: np.ndarray, side: str = \"right\"\n",
    ") -> np.ndarray:\n",
    "    # constant interpolation, from https://stackoverflow.com/a/39929401/3856731\n",
    "    indices = np.searchsorted(xp, x, side=side)\n",
    "    y2 = np.concatenate(([0], yp))\n",
    "\n",
    "    return y2[indices]\n",
    "\n",
    "def avg_steps(\n",
    "    xs: List[List[float]], ys: List[List[float]]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes average of step functions.\n",
    "\n",
    "    Each ys[j] is thought of as a right-continuous step function given by\n",
    "\n",
    "    `ys[j](x) = xs[j][i]`\n",
    "    for\n",
    "    `xs[j][i] <= x < xs[j][i+1]`\n",
    "\n",
    "    This function returns two NumPy arrays, `(inputs, outputs)`, giving the pointwise average\n",
    "    (see below) of these functions, one for inputs and one for outputs.\n",
    "    These output arrays can be considered to give another step function.\n",
    "\n",
    "    For a list of functions `[f_1, f_2, ..., f_n]`, their pointwise average\n",
    "    is the function `f_bar` defined by\n",
    "\n",
    "    `f_bar(x) = (1/n)(f_1(x) + f_2(x) + ... + f_n(x))`\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    `inputs`: `np.ndaray`\n",
    "        The union of all elements of all vectors in `xs`; this is the mutual domain\n",
    "        of the average function.\n",
    "    `outputs`: `np.ndarray`\n",
    "        The pointwise average of the `ys[j]`s, considered as step functions extended\n",
    "        to the full real line by assuming constant values for `x < min(xs[j])`\n",
    "        or `x > max(xs[j])`\n",
    "    \"\"\"\n",
    "    all_xs = []\n",
    "\n",
    "    # Start by removing extraneous dims\n",
    "    xs = [np.squeeze(x) for x in xs]\n",
    "    ys = [np.squeeze(y) for y in ys]\n",
    "\n",
    "    for j in range(len(xs)):\n",
    "        x = xs[j]\n",
    "        y = ys[j]\n",
    "        # union all x-values\n",
    "        all_xs += list(x)\n",
    "\n",
    "        # ensure array values are sorted\n",
    "        x_sort = np.argsort(x)\n",
    "        xs[j] = x[x_sort]\n",
    "        ys[j] = y[x_sort]\n",
    "\n",
    "    all_xs = list(set(all_xs))\n",
    "    all_xs.sort()\n",
    "\n",
    "    all_xs = np.array(all_xs)\n",
    "\n",
    "    # Holds constant-interpolated step fns as rows (axis 0).\n",
    "    # We \"evaluate\" ys[j] for every x-value in `all_xs`\n",
    "    # Easy to average via np.mean(all_curves, axis=0)\n",
    "    all_curves = np.zeros((len(xs), len(all_xs)))\n",
    "\n",
    "    for j, (x, y) in enumerate(zip(xs, ys)):\n",
    "        x, y = np.array(x), np.array(y)\n",
    "        all_curves[j] = constant_interp(all_xs, x, y, side=\"right\")\n",
    "\n",
    "    avg_curve = np.mean(all_curves, axis=0)\n",
    "\n",
    "    return all_xs, avg_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_rocs(fprs: List[np.ndarray],\n",
    "             tprs: List[np.ndarray],\n",
    "             x_class: str = \"SLEEP\",\n",
    "             y_class: str = \"WAKE\", \n",
    "             min_auc: float = 0.0,\n",
    "             avg_curve_color: str = \"tab:blue\",\n",
    "             specific_curve_color: str = \"tab:orange\",\n",
    "             roc_group_name: str = \"\", \n",
    "             ax: plt.Axes | None = None):\n",
    "    \"\"\"\n",
    "    Adds ROC curves to the given plot, or makes a new plot if ax is None.\n",
    "\n",
    "    if ax is None, we are making a new plot. We do additional formatting\n",
    "    in this case, such as adding the legend and showing the plot. \n",
    "    \n",
    "    When `ax` is provided, we expect the call site to do formatting.\n",
    "    \"\"\"\n",
    "    # don't overwrite ax, this lets us use the None info later on \n",
    "    # to automatically show the legend and do other formatting, \n",
    "    # which otherwise we'd expect the call site to peform on `ax`\n",
    "    resolved_ax = ax if ax is not None else plt.subplots()[1]\n",
    "    aucs = np.array([\n",
    "        auc_score(fpr, tpr)\n",
    "        for fpr, tpr in zip(fprs, tprs)\n",
    "    ])\n",
    "\n",
    "    all_fprs, avg_curve = avg_steps(\n",
    "            xs=[list(fprs[i]) for i in range(len(aucs)) if aucs[i] > min_auc],\n",
    "            ys=[list(tprs[i]) for i in range(len(aucs)) if aucs[i] > min_auc],\n",
    "        )\n",
    "\n",
    "    avg_auc = np.mean(aucs[aucs > min_auc])\n",
    "\n",
    "    resolved_ax.step(\n",
    "        all_fprs,\n",
    "        avg_curve,\n",
    "        c=avg_curve_color,\n",
    "        where=\"post\",\n",
    "        label=f\"{roc_group_name + ' ' * bool(roc_group_name)}All splits avg ROC-AUC: {avg_auc:0.3f}\",\n",
    "    )\n",
    "    for roc in zip(fprs, tprs):\n",
    "        resolved_ax.step(roc[0], roc[1], c=specific_curve_color, alpha=0.2, where=\"post\")\n",
    "    resolved_ax.plot([0, 1], [0, 1], \"-.\", c=\"black\")\n",
    "\n",
    "    resolved_ax.set_ylabel(f\"Fraction of {y_class} scored as {y_class}\")\n",
    "    resolved_ax.set_xlabel(f\"Fraction of {x_class} scored as {y_class}\")\n",
    "\n",
    "    resolved_ax.spines[\"top\"].set_visible(False)\n",
    "    resolved_ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    if ax is None:\n",
    "        # show the legend if we are making a new plot\n",
    "        # otherwise, the call site might want to make their own legend, leave it.\n",
    "        resolved_ax.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pad_to_hat(y: np.ndarray, y_hat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Adds zeros to the end of y to match the length of y_hat.\n",
    "\n",
    "    Useful when the inputs had to be padded with zeros to match shape requirements for dense layers.\n",
    "    \"\"\"\n",
    "    pad = y_hat.shape[-1] - y.shape[-1]\n",
    "    if pad < 0:\n",
    "        warnings.warn(f\"y_hat is shorter than y by {pad} elements, returning y unchanged\")\n",
    "        return y\n",
    "    y_padded = np.pad(y, (0, pad), constant_values=0)\n",
    "    return y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mae_func(\n",
    "    func: Callable[[np.ndarray], float],\n",
    "    trues: List[np.ndarray],\n",
    "    preds: List[np.ndarray],\n",
    ") -> float:\n",
    "    \"\"\"Computes Mean Absolute Error (MAE) for the numerical function `func` on the given lists.\n",
    "\n",
    "    This function is useful for computing MAE of statistical functions giving a single float\n",
    "    for every NumPy array.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "    `func`: callable `(np.ndarray) -> float`\n",
    "        The statistic we are computing for truth/prediction arrays. It is called on each element\n",
    "        of the lists of NumPy arrays, then MAE of the resulting statistic lists is computed.\n",
    "    `trues`: `list` of `np.ndarray`\n",
    "        The \"True\" labels, eg. This function is symmetric in `trues` and `preds`, and isn't specific\n",
    "        to classifiers, so the argument names are just mnemonics.\n",
    "    `preds`: `list` of `np.ndarray`\n",
    "        The \"Predicted\" labels, eg.\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    MAE of `func` applied to elements of `trues` and `preds`.\n",
    "    \"\"\"\n",
    "    assert len(trues) == len(preds)\n",
    "\n",
    "    # aes = (A)bsolute (E)rror(S)\n",
    "    # We will take the mean of this list for Mean Absolute Error\n",
    "    aes = list(\n",
    "        map(lambda ab: abs(ab[0] - ab[1]), zip(map(func, trues), map(func, preds)))\n",
    "    )\n",
    "\n",
    "    return sum(aes) / len(aes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "def resample_accel_data(accel_data: np.ndarray, original_fs: int, target_fs: int) -> np.ndarray:\n",
    "    # Calculate the greatest common divisor for up/down sampling rates\n",
    "    up = target_fs\n",
    "    down = original_fs\n",
    "    gcd = np.gcd(int(up), int(down))\n",
    "    up = int(up // gcd)\n",
    "    down = int(down // gcd)\n",
    "\n",
    "    accel_resampled_time = np.arange(accel_data[0, 0], accel_data[-1, 0], 1/original_fs)\n",
    "\n",
    "    accel_resampled = np.zeros((len(accel_resampled_time), accel_data.shape[1]))\n",
    "    accel_resampled[:, 0] = accel_resampled_time\n",
    "\n",
    "    for i in range(1, accel_data.shape[1]):\n",
    "        accel_resampled[:, i] = np.interp(accel_resampled_time, accel_data[:, 0], accel_data[:, i])\n",
    "    \n",
    "    # Resample data (excluding the time column)\n",
    "    resampled_data = resample_poly(accel_resampled[:, 1:], up, down, axis=0)\n",
    "    \n",
    "    # Recompute the time vector\n",
    "    duration = accel_data[-1, 0] - accel_data[0, 0]\n",
    "    num_samples = resampled_data.shape[0]\n",
    "    new_time = np.linspace(accel_resampled_time[0], accel_resampled_time[-1], num_samples)\n",
    "    \n",
    "    # Combine the new time vector with the resampled data\n",
    "    resampled_accel_data = np.column_stack((new_time, resampled_data))\n",
    "    return resampled_accel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Constants:\n",
    "    # WAKE_THRESHOLD = 0.3  # These values were used for scikit-learn 0.20.3, See:\n",
    "    # REM_THRESHOLD = 0.35  # https://scikit-learn.org/stable/whats_new.html#version-0-21-0\n",
    "    WAKE_THRESHOLD = 0.5  #\n",
    "    REM_THRESHOLD = 0.35\n",
    "\n",
    "    DEFAULT_EPOCH_DURATION_IN_SECONDS = 30\n",
    "    SECONDS_PER_MINUTE = 60\n",
    "    SECONDS_PER_DAY = 3600 * 24\n",
    "    SECONDS_PER_HOUR = 3600\n",
    "    VERBOSE = True\n",
    "\n",
    "class SleepMetricsCalculator:\n",
    "    @staticmethod\n",
    "    def get_tst(labels, epoch_seconds: float | None = 30.0):\n",
    "        tst = np.sum(labels > 0)\n",
    "        epoch_seconds = (\n",
    "            epoch_seconds\n",
    "            if epoch_seconds is not None\n",
    "            else Constants.DEFAULT_EPOCH_DURATION_IN_SECONDS\n",
    "        )\n",
    "        return tst * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wake_after_sleep_onset(labels, epoch_seconds: float | None = 30.0):\n",
    "        select = labels >= 0\n",
    "        labels = labels[select]\n",
    "        sleep_indices = np.argwhere(labels > 0)\n",
    "\n",
    "        epoch_seconds = (\n",
    "            epoch_seconds\n",
    "            if epoch_seconds is not None\n",
    "            else Constants.DEFAULT_EPOCH_DURATION_IN_SECONDS\n",
    "        )\n",
    "        if np.shape(sleep_indices)[0] > 0:\n",
    "            sol_index = np.amin(sleep_indices)\n",
    "            indices_where_wake_occurred = np.where(labels == 0)\n",
    "\n",
    "            waso_indices = np.where(indices_where_wake_occurred > sol_index)\n",
    "            waso_indices = waso_indices[1]\n",
    "            number_waso_indices = np.shape(waso_indices)[0]\n",
    "            return number_waso_indices * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "        else:\n",
    "            # print(\"*\" * 10 + \"get_wake_after_sleep_onset\" + \"*\" * 10)\n",
    "            # print(labels)\n",
    "            return len(labels) * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sleep_efficiency(labels):\n",
    "        sleep_indices = np.where(labels > 0)\n",
    "        sleep_efficiency = float(np.shape(sleep_indices)[1]) / float(\n",
    "            np.shape(labels)[0]\n",
    "        )\n",
    "        return sleep_efficiency\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sleep_onset_latency(labels, epoch_seconds: Optional[float]):\n",
    "        sleep_indices = np.argwhere(labels > 0)\n",
    "        epoch_seconds = (\n",
    "            epoch_seconds\n",
    "            if epoch_seconds is not None\n",
    "            else Constants.DEFAULT_EPOCH_DURATION_IN_SECONDS\n",
    "        )\n",
    "        if np.shape(sleep_indices)[0] > 0:\n",
    "            return np.amin(sleep_indices) * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "        else:\n",
    "            return len(labels) * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time_in_rem(labels, epoch_seconds: Optional[float]):\n",
    "        rem_epoch_indices = np.where(labels == 2)\n",
    "        rem_time = np.shape(rem_epoch_indices)[1]\n",
    "        return rem_time * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time_in_nrem(labels, epoch_seconds: Optional[float]):\n",
    "        rem_epoch_indices = np.where(labels == 1)\n",
    "        rem_time = np.shape(rem_epoch_indices)[1]\n",
    "        return rem_time * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @classmethod\n",
    "    def report_mae_tst_waso(\n",
    "        cls,\n",
    "        y_pred_y_true: List[Tuple[np.ndarray, np.ndarray]],\n",
    "        sleep_acc: float = 0.93,\n",
    "        epoch_seconds: Optional[float] = 30,\n",
    "    ) -> Dict[str, float]:\n",
    "        res = {\"mae_tst_minutes\": [], \"mae_waso_minutes\": []}\n",
    "        preds = []\n",
    "        trues = []\n",
    "        for pred, true in y_pred_y_true:\n",
    "            fprs, tprs, thresholds = roc_curve(true, pred)\n",
    "            threshold = thresholds[np.argmax(fprs <= (1 - sleep_acc))]\n",
    "            preds.append(pred >= threshold)\n",
    "            trues.append(true)\n",
    "\n",
    "        tst_func = partial(cls.get_tst, epoch_seconds=epoch_seconds)\n",
    "        waso_func = partial(cls.get_wake_after_sleep_onset, epoch_seconds=epoch_seconds)\n",
    "        res[\"mae_tst_minutes\"] = mae_func(tst_func, trues=trues, preds=preds)\n",
    "        res[\"mae_waso_minutes\"] = mae_func(waso_func, trues=trues, preds=preds)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "WASA_THRESHOLD = 0.93\n",
    "BALANCE_WEIGHTS = True\n",
    "\n",
    "def split_analysis(y, y_hat_sleep_proba, sleep_accuracy: float = WASA_THRESHOLD, balancing: bool = BALANCE_WEIGHTS):\n",
    "\n",
    "    y_flat = y.reshape(-1,)\n",
    "    n_sleep = np.sum(y_flat > 0)\n",
    "    n_wake = np.sum(y_flat == 0)\n",
    "    N = n_sleep + n_wake\n",
    "\n",
    "    balancing_weights_ignore_mask = np.where(y_flat > 0, N / n_sleep, N / n_wake) \\\n",
    "        if balancing else np.ones_like(y_flat)\n",
    "    balancing_weights_ignore_mask /= np.sum(balancing_weights_ignore_mask) # sums to 1.0\n",
    "\n",
    "    # adjust y to match the lenght of y_hat, which was padded to fit model constraints\n",
    "    y_padded = pad_to_hat(y_flat, y_hat_sleep_proba)\n",
    "    # make a mask to ignore the padded values, so they aren't counted against us\n",
    "    mask = pad_to_hat(balancing_weights_ignore_mask, y_hat_sleep_proba)\n",
    "\n",
    "    # also ignore any unscored or missing values.\n",
    "    y_to_score = pad_to_hat(y_flat >= 0, y_hat_sleep_proba)\n",
    "    mask *= y_to_score\n",
    "    # roc_auc will complain if -1 is in y_padded\n",
    "    y_padded *= y_to_score \n",
    "\n",
    "    # ROC analysis\n",
    "    fprs, tprs, thresholds = roc_curve(y_padded, y_hat_sleep_proba, sample_weight=mask)\n",
    "\n",
    "    # Sleep accuracy = (n sleep correct) / (n sleep) = TP/AP = TPR\n",
    "    wasa_threshold = thresholds[np.sum(tprs <= sleep_accuracy)]\n",
    "    y_guess = y_hat_sleep_proba > wasa_threshold\n",
    "\n",
    "    # # WASA X\n",
    "    guess_right = y_guess == y_padded\n",
    "    y_wake = y_padded == 0\n",
    "    wake_accuracy = np.sum(y_wake * guess_right * y_to_score) / np.sum(n_wake)\n",
    "     \n",
    "    return {\n",
    "        \"y_padded\": y_padded,\n",
    "        \"y_hat\": y_hat_sleep_proba,\n",
    "        \"mask\": mask,\n",
    "        \"kappa\": cohen_kappa_score(y_padded, y_guess, sample_weight=mask),\n",
    "        \"auc\": roc_auc_score(y_padded, y_hat_sleep_proba, sample_weight=mask),\n",
    "        \"roc_curve\": {\"tprs\": tprs,\n",
    "                      \"fprs\": fprs,\n",
    "                      \"thresholds\": thresholds\n",
    "        }, \n",
    "        f\"wasa{int(100 * sleep_accuracy)}_threshold\": wasa_threshold,\n",
    "        f\"wasa{int(100 * sleep_accuracy)}\": wake_accuracy, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pisces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
