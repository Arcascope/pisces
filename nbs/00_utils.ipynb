{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Module of utility functions used throughout the package, for things like common preprocessing steps many models are likely to use, or useful for multiple points of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def determine_header_rows_and_delimiter(\n",
    "    filename: Path | str\n",
    ") -> Tuple[Optional[int], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Given a filename pointing at a CSV files, decides:\n",
    "     * how many header lines there are (based on first line starting with a digit)\n",
    "     * the delimiter-- right now tries whitespace and comma\n",
    "\n",
    "    Returns one of:\n",
    "     - (number of header rows, column delimiter),\n",
    "     - (number of header rows, None) if the delimiter could not be inferred,\n",
    "     - (None, None) if CSV has no numerical rows,\n",
    "\n",
    "    :param filename: CSV Path or filepath literal\n",
    "    :return: header and delimiter information, if possible.\n",
    "    \"\"\"\n",
    "    MAX_ROWS = 100  # if you don't have data within first 100 lines, exit.\n",
    "    header_row_count = 0\n",
    "    with open(filename) as f:\n",
    "        header_found = False\n",
    "        line = \"\"  # modified in while below\n",
    "\n",
    "        # Search over lines until we find one starting with a digit\n",
    "        while not header_found:\n",
    "            line = f.readline()\n",
    "            if line == \"\":  # last line of file reached\n",
    "                return None, None\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                int(line[0])\n",
    "                header_found = True\n",
    "            except ValueError:\n",
    "                header_row_count += 1\n",
    "            except IndexError:\n",
    "                header_row_count += 1\n",
    "\n",
    "            # guard against infinite loop\n",
    "            if header_row_count >= MAX_ROWS:\n",
    "                return None, None\n",
    "\n",
    "        # Now try splitting that first line of data\n",
    "        delim_guesses = [\" \", \",\", \", \"]\n",
    "\n",
    "        for guess in delim_guesses:\n",
    "            try:\n",
    "                comps = line.split(guess)  # whitespace separated?\n",
    "                float(comps[0])\n",
    "                return header_row_count, guess\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return header_row_count, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ActivityCountAlgorithm(Enum):\n",
    "    te_Lindert_et_al = 0\n",
    "    ActiGraphOfficial = 1\n",
    "    ADS = 2\n",
    "\n",
    "\n",
    "def build_activity_counts(\n",
    "    data,\n",
    "    axis: int = 3,\n",
    "    prefix: str = \"\",\n",
    "    algorithm: ActivityCountAlgorithm = ActivityCountAlgorithm.ADS\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if algorithm == ActivityCountAlgorithm.ActiGraphOfficial:\n",
    "        return build_ActiGraph_official(data)\n",
    "    if algorithm == ActivityCountAlgorithm.ADS:\n",
    "        return build_ADS(data)\n",
    "    if algorithm == ActivityCountAlgorithm.te_Lindert_et_al:\n",
    "        return build_activity_counts_te_Lindert_et_al(data, axis, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def build_ADS(\n",
    "    time_xyz: np.ndarray,\n",
    "    sampling_hz: float = 50.0,\n",
    "    bin_size_seconds: float = 15,\n",
    "    prefix: str = \"\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"ADS algorithm for activity counts, developed by Arcascope with support from the NHRC.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "     - `time_xyz`: numpy array with shape (N_samples, 4) where the 4 coordinates are: [time, x, y, z] \n",
    "     - `sampling_hz`: `float` sampling frequency of thetime_xyz \n",
    "    \"\"\"\n",
    "    data_shape_error = ValueError(\n",
    "            f\"`time_xyz` must have shape (N_samples, 4) but has shape {time_xyz.shape}\"\n",
    "        )\n",
    "    try:\n",
    "        assert (len(time_xyz.shape) == 2 and time_xyz.shape[1] == 4)\n",
    "    except AssertionError:\n",
    "        raise data_shape_error\n",
    "\n",
    "    time_data_raw = time_xyz[:, 0]\n",
    "    x_accel = time_xyz[:, 1]\n",
    "    y_accel = time_xyz[:, 2]\n",
    "    z_accel = time_xyz[:, 3]\n",
    "\n",
    "    # Interpolate to sampling Hz\n",
    "    time_values = np.arange(\n",
    "        np.amin(time_data_raw), np.amax(time_data_raw), 1 / sampling_hz\n",
    "    )\n",
    "    # Must do each coordinate separately\n",
    "    x_data = np.interp(time_values, time_data_raw, x_accel)\n",
    "    y_data = np.interp(time_values, time_data_raw, y_accel)\n",
    "    z_data = np.interp(time_values, time_data_raw, z_accel)\n",
    "\n",
    "    # Calculate \"amplitude\" = timeseries of 2-norm of (x, y, z)\n",
    "    amplitude = np.linalg.norm(np.array([x_data, y_data, z_data]), axis = 0)\n",
    "\n",
    "    abs_amplitude_deriv = np.abs(np.diff(amplitude))\n",
    "    abs_amplitude_deriv = np.insert(abs_amplitude_deriv, 0, 0)\n",
    "\n",
    "    # Binning step\n",
    "    # Sum abs_amplitude_deriv in time-based windows\n",
    "    # ex: bin_size_seconds = 15\n",
    "    # Step from first to last time by 15 seconds\n",
    "    time_counts = np.arange(\n",
    "        np.amin(time_data_raw), np.amax(time_data_raw), bin_size_seconds\n",
    "    )\n",
    "\n",
    "    # Convert time at 50 hz to \"# of 15 second windows past start\"\n",
    "    bin_values = (time_values - time_values[0]).astype(int) // bin_size_seconds\n",
    "    sums_in_bins = np.bincount(bin_values, abs_amplitude_deriv)\n",
    "    sums_in_bins[sums_in_bins <= 0.05 * max(sums_in_bins)] = 0.0\n",
    "    return time_counts, sums_in_bins\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def build_activity_counts_te_Lindert_et_al(\n",
    "    time_xyz, axis: int = 3, prefix: str = \"\"\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Implementation of the reverse-engineered activity count algorithm from\n",
    "    te Lindert BH, Van Someren EJ. Sleep. 2013\n",
    "    Sleep estimates using microelectromechanical systems (MEMS). \n",
    "    doi: 10.5665/sleep.2648\n",
    "    \n",
    "    :param time_xyz: `np.ndarray` loaded from timestamped triaxial accelerometer CSV. Shape (N, 4)\n",
    "    :return: (time, activity counts with 15 second epoch)\n",
    "    \"\"\"\n",
    "\n",
    "    # a helper function to calculate max over 2 epochs\n",
    "    def max2epochs(data, fs, epoch):\n",
    "        data = data.flatten()\n",
    "\n",
    "        seconds = int(np.floor(np.shape(data)[0] / fs))\n",
    "        data = np.abs(data)\n",
    "        data = data[0 : int(seconds * fs)]\n",
    "\n",
    "        data = data.reshape(fs, seconds, order=\"F\").copy()\n",
    "\n",
    "        data = data.max(0)\n",
    "        data = data.flatten()\n",
    "        N = np.shape(data)[0]\n",
    "        num_epochs = int(np.floor(N / epoch))\n",
    "        data = data[0 : (num_epochs * epoch)]\n",
    "\n",
    "        data = data.reshape(epoch, num_epochs, order=\"F\").copy()\n",
    "        epoch_data = np.sum(data, axis=0)\n",
    "        epoch_data = epoch_data.flatten()\n",
    "\n",
    "        return epoch_data\n",
    "    \n",
    "    fs = 50\n",
    "    time = np.arange(np.amin(time_xyz[:, 0]), np.amax(time_xyz[:, 0]), 1.0 / fs)\n",
    "    z_data = np.interp(time, time_xyz[:, 0], time_xyz[:, axis])\n",
    "\n",
    "    cf_low = 3\n",
    "    cf_hi = 11\n",
    "    order = 5\n",
    "    w1 = cf_low / (fs / 2)\n",
    "    w2 = cf_hi / (fs / 2)\n",
    "    pass_band = [w1, w2]\n",
    "    b, a = butter(order, pass_band, \"bandpass\")\n",
    "\n",
    "    z_filt = filtfilt(b, a, z_data)\n",
    "    z_filt = np.abs(z_filt)\n",
    "\n",
    "    top_edge = 5\n",
    "    bottom_edge = 0\n",
    "    number_of_bins = 128\n",
    "\n",
    "    bin_edges = np.linspace(bottom_edge, top_edge, number_of_bins + 1)\n",
    "    binned = np.digitize(z_filt, bin_edges)\n",
    "    epoch = 15\n",
    "    counts = max2epochs(binned, fs, epoch)\n",
    "    counts = (counts - 18) * 3.07\n",
    "    counts[counts < 0] = 0\n",
    "\n",
    "    time_counts = np.linspace(np.min(time_xyz[:, 0]), max(time_xyz[:, 0]), np.shape(counts)[0])\n",
    "    time_counts = np.expand_dims(time_counts, axis=1)\n",
    "    counts = np.expand_dims(counts, axis=1)\n",
    "\n",
    "    return time_counts, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from agcounts.extract import get_counts\n",
    "\n",
    "def build_ActiGraph_official(time_xyz, axis: int = 3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    freq = 50\n",
    "    counts = get_counts(time_xyz[:, 1:], freq=freq, epoch=15)[:, axis - 1]\n",
    "    times = np.linspace(time_xyz[0, 0], time_xyz[-1, 0], len(counts))\n",
    "\n",
    "    return times, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def constant_interp(\n",
    "    x: np.ndarray, xp: np.ndarray, yp: np.ndarray, side: str = \"right\"\n",
    ") -> np.ndarray:\n",
    "    # constant interpolation, from https://stackoverflow.com/a/39929401/3856731\n",
    "    indices = np.searchsorted(xp, x, side=side)\n",
    "    y2 = np.concatenate(([0], yp))\n",
    "\n",
    "    return y2[indices]\n",
    "\n",
    "def avg_steps(\n",
    "    xs: List[List[float]], ys: List[List[float]]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes average of step functions.\n",
    "\n",
    "    Each ys[j] is thought of as a right-continuous step function given by\n",
    "\n",
    "    `ys[j](x) = xs[j][i]`\n",
    "    for\n",
    "    `xs[j][i] <= x < xs[j][i+1]`\n",
    "\n",
    "    This function returns two NumPy arrays, `(inputs, outputs)`, giving the pointwise average\n",
    "    (see below) of these functions, one for inputs and one for outputs.\n",
    "    These output arrays can be considered to give another step function.\n",
    "\n",
    "    For a list of functions `[f_1, f_2, ..., f_n]`, their pointwise average\n",
    "    is the function `f_bar` defined by\n",
    "\n",
    "    `f_bar(x) = (1/n)(f_1(x) + f_2(x) + ... + f_n(x))`\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    `inputs`: `np.ndaray`\n",
    "        The union of all elements of all vectors in `xs`; this is the mutual domain\n",
    "        of the average function.\n",
    "    `outputs`: `np.ndarray`\n",
    "        The pointwise average of the `ys[j]`s, considered as step functions extended\n",
    "        to the full real line by assuming constant values for `x < min(xs[j])`\n",
    "        or `x > max(xs[j])`\n",
    "    \"\"\"\n",
    "    all_xs = []\n",
    "\n",
    "    # Start by removing extraneous dims\n",
    "    xs = [np.squeeze(x) for x in xs]\n",
    "    ys = [np.squeeze(y) for y in ys]\n",
    "\n",
    "    for j in range(len(xs)):\n",
    "        x = xs[j]\n",
    "        y = ys[j]\n",
    "        # union all x-values\n",
    "        all_xs += list(x)\n",
    "\n",
    "        # ensure array values are sorted\n",
    "        x_sort = np.argsort(x)\n",
    "        xs[j] = x[x_sort]\n",
    "        ys[j] = y[x_sort]\n",
    "\n",
    "    all_xs = list(set(all_xs))\n",
    "    all_xs.sort()\n",
    "\n",
    "    all_xs = np.array(all_xs)\n",
    "\n",
    "    # Holds constant-interpolated step fns as rows (axis 0).\n",
    "    # We \"evaluate\" ys[j] for every x-value in `all_xs`\n",
    "    # Easy to average via np.mean(all_curves, axis=0)\n",
    "    all_curves = np.zeros((len(xs), len(all_xs)))\n",
    "\n",
    "    for j, (x, y) in enumerate(zip(xs, ys)):\n",
    "        x, y = np.array(x), np.array(y)\n",
    "        all_curves[j] = constant_interp(all_xs, x, y, side=\"right\")\n",
    "\n",
    "    avg_curve = np.mean(all_curves, axis=0)\n",
    "\n",
    "    return all_xs, avg_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "def pad_to_hat(y: np.ndarray, y_hat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Adds zeros to the end of y to match the length of y_hat.\n",
    "\n",
    "    Useful when the inputs had to be padded with zeros to match shape requirements for dense layers.\n",
    "    \"\"\"\n",
    "    pad = y_hat.shape[-1] - y.shape[-1]\n",
    "    if pad < 0:\n",
    "        warnings.warn(f\"y_hat is shorter than y by {pad} elements, returning y unchanged\")\n",
    "        return y\n",
    "    y_padded = np.pad(y, (0, pad), constant_values=0)\n",
    "    return y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
