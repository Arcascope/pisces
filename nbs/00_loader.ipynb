{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loader\n",
    "\n",
    "> Module for loading in new datasets and performing validation checks on them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "import csv\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "from sqlalchemy import Boolean, Column, Integer, String, create_engine, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class StudyRecord(Base):\n",
    "    \"\"\"\n",
    "    Class modeling the records in our sqlite database.\n",
    "\n",
    "    The attributes of this class must match the column names in NavySleepResearch/data_sets.csv. Using\n",
    "    Python's built-in CSVReader class, we get an array of dicts that are easy to turn into StudyRecord\n",
    "    objects via dict unpacking (** operator) and kwarg matching, which is a fancy way to say:\n",
    "      new_record = StudyRecord(**csv_row_dict)\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"data_sets\"\n",
    "\n",
    "    id = Column(\n",
    "        Integer, primary_key=True\n",
    "    )  # database id column, not id of patient within study\n",
    "\n",
    "    # Identification of patient and data set\n",
    "    subject_id = Column(String)\n",
    "    data_set_name = Column(String)\n",
    "\n",
    "    # Patient metadata\n",
    "    age = Column(Integer)\n",
    "    sex = Column(Integer)  # Integer: maps to Enum\n",
    "    race = Column(Integer)  # Integer: maps to Enum\n",
    "    has_apnea = Column(Boolean)\n",
    "    has_idiopathic_hypersomnia = Column(Boolean)\n",
    "    has_sleep_paralysis = Column(Boolean)\n",
    "    has_restless_leg_syndrome = Column(Boolean)\n",
    "    has_insomnia = Column(Boolean)\n",
    "    has_parasomnia = Column(Boolean)\n",
    "    has_rem_sleep_disorder = Column(Boolean)\n",
    "    has_narcolepsy = Column(Boolean)\n",
    "    has_nocturia = Column(Boolean)\n",
    "\n",
    "    # Identify what data is available and where it is\n",
    "    psg_location = Column(String)\n",
    "    accelerometer_location = Column(String)\n",
    "    heartrate_location = Column(String)\n",
    "    activity_location = Column(String)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.subject_id}:{self.data_set_name}\"\n",
    "\n",
    "    def diagnosis(self) -> str:\n",
    "        known_conditions = [\n",
    "            \"apnea\",\n",
    "            \"idiopathic_hypersomnia\",\n",
    "            \"sleep_paralysis\",\n",
    "            \"restless_leg_syndrome\",\n",
    "            \"insomnia\",\n",
    "            \"parasomnia\",\n",
    "            \"rem_sleep_disorder\",\n",
    "            \"narcolepsy\",\n",
    "            \"nocturia\",\n",
    "        ]\n",
    "\n",
    "        self_dict = (\n",
    "            self.__dict__\n",
    "        )  # lets us paste known_conditions into dict keys for easy iteration\n",
    "\n",
    "        found_conditions = \"\"\n",
    "\n",
    "        for condition in known_conditions:\n",
    "            if self_dict[f\"has_{condition}\"]:\n",
    "                found_conditions += f\" -- {condition.replace('_', ' ')}\\n\"\n",
    "        return found_conditions\n",
    "\n",
    "\n",
    "class DatabaseLiaison:\n",
    "    def __init__(self, db_filename: str):\n",
    "        self.engine = create_engine(f\"sqlite:///{db_filename}\")\n",
    "        self.Session_maker = sessionmaker(bind=self.engine)\n",
    "        Base.metadata.create_all(\n",
    "            bind=self.engine\n",
    "        )  # makes the actual tables in database, if they don't exist\n",
    "\n",
    "    def add(self, record: StudyRecord):\n",
    "        session = self.Session_maker()\n",
    "        session.add(record)\n",
    "        session.commit()\n",
    "\n",
    "    def add_all(self, records: List[StudyRecord]):\n",
    "        session = self.Session_maker()\n",
    "        session.add_all(records)\n",
    "        session.commit()\n",
    "\n",
    "    def select(self, selector: str = \"\") -> List[StudyRecord]:\n",
    "        \"\"\"\n",
    "        Runs raw SQL on database; selector adds a WHERE clause to SELECT * FROM data_sets\n",
    "        The default \"\" selects everything, equivalent to '''SELECT * FROM data_sets'''\n",
    "        :param selector: WHERE clause contents\n",
    "        :return: List of study records matching the selector.\n",
    "        \"\"\"\n",
    "        session = self.Session_maker()\n",
    "        return session.query(StudyRecord).filter(text(selector)).all()\n",
    "\n",
    "    def to_csvDictWriterDicts(self) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Converts database to a list of dicts ready to be sent into a CSV by csv.DictWriter.\n",
    "        :return: `[dict]` one per row with keys being the columns.\n",
    "        \"\"\"\n",
    "        session = self.Session_maker()\n",
    "        records = [r.__dict__ for r in session.query(StudyRecord).all()]\n",
    "\n",
    "        for r in records:\n",
    "            # pop those keys that are not part of CSV\n",
    "            r.pop(\"id\")  # db PK, dont persist this\n",
    "            r.pop(\"_sa_instance_state\")  # artifact of SQLAlchemy class.\n",
    "\n",
    "        return records\n",
    "\n",
    "    def to_csv(self, filename: str):\n",
    "        records = self.to_csvDictWriterDicts()\n",
    "        with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "            # This must be hardcoded to ensure deterministic order, AFAIK\n",
    "            fieldnames = [\n",
    "                \"subject_id\",\n",
    "                \"data_set_name\",\n",
    "                \"age\",\n",
    "                \"sex\",\n",
    "                \"race\",\n",
    "                \"has_apnea\",\n",
    "                \"has_idiopathic_hypersomnia\",\n",
    "                \"has_sleep_paralysis\",\n",
    "                \"has_restless_leg_syndrome\",\n",
    "                \"has_insomnia\",\n",
    "                \"has_parasomnia\",\n",
    "                \"has_rem_sleep_disorder\",\n",
    "                \"has_narcolepsy\",\n",
    "                \"has_nocturia\",\n",
    "                \"psg_location\",\n",
    "                \"accelerometer_location\",\n",
    "                \"heartrate_location\",\n",
    "                \"activity_location\",\n",
    "            ]\n",
    "\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for rec in records:\n",
    "                writer.writerow(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnownFeatures(Enum):\n",
    "    \"\"\"\n",
    "    Expresses to the system which features from data to include.\n",
    "    This lives here for cyclic import reasons...\n",
    "    \"\"\"\n",
    "\n",
    "    ACTIVITY = auto()\n",
    "    ACCELEROMETER = auto()\n",
    "    HEARTRATE = auto()\n",
    "    PSG = auto()\n",
    "\n",
    "    # Not settable, derived\n",
    "    ACCEL_SPECTRO = auto()\n",
    "    MO_SPECTRO = auto()  # Mads Olsen preprocessing\n",
    "\n",
    "    @property\n",
    "    def base_feature(self) -> \"KnownFeatures\":\n",
    "        if self.is_accelerometer_based:\n",
    "            return KnownFeatures.ACCELEROMETER\n",
    "        elif self.is_activity_based:\n",
    "            return KnownFeatures.ACTIVITY\n",
    "        elif self.is_heartrate_based:\n",
    "            return KnownFeatures.HEARTRATE\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "    @property\n",
    "    def is_derived(self) -> bool:\n",
    "        match self:\n",
    "            case KnownFeatures.ACTIVITY | KnownFeatures.ACCELEROMETER | KnownFeatures.HEARTRATE | KnownFeatures.PSG:\n",
    "                return False\n",
    "            case _:\n",
    "                return True\n",
    "\n",
    "    @property\n",
    "    def is_spectral(self) -> bool:\n",
    "        return self in [KnownFeatures.ACCEL_SPECTRO, KnownFeatures.MO_SPECTRO]\n",
    "\n",
    "    @property\n",
    "    def is_accelerometer_based(self) -> bool:\n",
    "        # Convenience + readability helper\n",
    "        return self in [\n",
    "            KnownFeatures.ACCELEROMETER,\n",
    "            KnownFeatures.ACCEL_SPECTRO,\n",
    "            KnownFeatures.MO_SPECTRO,\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def is_activity_based(self) -> bool:\n",
    "        return self in [KnownFeatures.ACTIVITY]\n",
    "\n",
    "    @property\n",
    "    def is_heartrate_based(self) -> bool:\n",
    "        return self in [KnownFeatures.HEARTRATE]\n",
    "\n",
    "\n",
    "# Determines the names of folders inside a data set's root\n",
    "# where we expect to find specific data CSVs.\n",
    "ACCELEROMETER_FOLDER_NAME = \"cleaned_accelerometer\"\n",
    "HEARTRATE_FOLDER_NAME = \"cleaned_heartrate\"\n",
    "PSG_FOLDER_NAME = \"cleaned_psg\"\n",
    "ACTIGRAPHY_FOLDER_NAME = \"cleaned_activity\"\n",
    "\n",
    "\n",
    "# class DataRecord:\n",
    "#     \"\"\"\n",
    "#     Class that understands how to take study database records and provide data to the pipeline.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, study_record: StudyRecord, data_folder: Optional[str | Path] = None\n",
    "#     ):\n",
    "#         self.record = study_record\n",
    "\n",
    "#         self.data_root = (\n",
    "#             Path(__file__).parent.parent.parent.joinpath(\"data\")\n",
    "#             if data_folder is None\n",
    "#             else Path(data_folder)\n",
    "#         )\n",
    "#         self.data_folder = self.data_root.joinpath(str(self.record.data_set_name))\n",
    "#         # These will be populated by actual data that can be operated on by classifiers\n",
    "#         self._data: Dict[KnownFeatures, TimeseriesRecording] = {\n",
    "#             KnownFeatures.PSG: None,\n",
    "#             KnownFeatures.ACCELEROMETER: None,\n",
    "#             KnownFeatures.HEARTRATE: None,\n",
    "#             KnownFeatures.ACTIVITY: None,\n",
    "#         }\n",
    "\n",
    "#     @property\n",
    "#     def psg_data(self) -> Union[PSGStagingRecord, PSGSleepWakeRecord] | None:\n",
    "#         return self._data[KnownFeatures.PSG]\n",
    "\n",
    "#     @psg_data.setter\n",
    "#     def psg_data(self, value):\n",
    "#         assert (value is None) or issubclass(type(value), PSGRecord)\n",
    "#         self._data[KnownFeatures.PSG] = value\n",
    "\n",
    "#     @property\n",
    "#     def acceleration_data(self) -> AccelerometerRecord | None:\n",
    "#         return self._data[KnownFeatures.ACCELEROMETER]\n",
    "\n",
    "#     @acceleration_data.setter\n",
    "#     def acceleration_data(self, value):\n",
    "#         assert (value is None) or isinstance(value, AccelerometerRecord)\n",
    "#         self._data[KnownFeatures.ACCELEROMETER] = value\n",
    "\n",
    "#     @property\n",
    "#     def heartrate_data(self) -> HeartrateRecord | None:\n",
    "#         return self._data[KnownFeatures.HEARTRATE]\n",
    "\n",
    "#     @heartrate_data.setter\n",
    "#     def heartrate_data(self, value):\n",
    "#         assert (value is None) or isinstance(value, HeartrateRecord)\n",
    "#         self._data[KnownFeatures.HEARTRATE] = value\n",
    "\n",
    "#     @property\n",
    "#     def activity_count_data(self) -> ActivityCountRecord | None:\n",
    "#         return self._data[KnownFeatures.ACTIVITY]\n",
    "\n",
    "#     @activity_count_data.setter\n",
    "#     def activity_count_data(self, value):\n",
    "#         assert (value is None) or isinstance(value, ActivityCountRecord)\n",
    "#         self._data[KnownFeatures.ACTIVITY] = value\n",
    "\n",
    "#     @property\n",
    "#     def data_set_name(self):\n",
    "#         return self.record.data_set_name\n",
    "\n",
    "#     @property\n",
    "#     def subject_id(self):\n",
    "#         return self.record.subject_id\n",
    "\n",
    "#     @property\n",
    "#     def absolute_accelerometer_path(self) -> str:\n",
    "#         return self.data_folder.joinpath(ACCELEROMETER_FOLDER_NAME).joinpath(\n",
    "#             self.record.accelerometer_location\n",
    "#         )\n",
    "\n",
    "#     def max_time(self, over: List[KnownFeatures] = list(KnownFeatures)) -> float:\n",
    "#         times = []\n",
    "#         for feature in over:\n",
    "#             feat = self._data.get(feature)\n",
    "#             if feat is not None:\n",
    "#                 times += [feat.time.max()]\n",
    "#         # times is array of maximum time on each non-None feature\n",
    "#         # min of maximum times gives maximum time intersection\n",
    "#         return min(times) if times else np.inf\n",
    "\n",
    "#     def min_time(self, over: List[KnownFeatures] = list(KnownFeatures)) -> float:\n",
    "#         times = []\n",
    "#         for feature in over:\n",
    "#             feat = self._data.get(feature)\n",
    "#             if feat is not None:\n",
    "#                 times += [feat.time.min()]\n",
    "#         # times is array of minimum time on each non-None feature\n",
    "#         # max of minimum times gives minimum time intersection\n",
    "#         return max(times) if times else -1.0 * np.inf\n",
    "\n",
    "#     def trim(self, start: float, end: float):\n",
    "#         \"\"\"\n",
    "#         Trims all of the timeseries loaded on this object to have .time values\n",
    "#         between `start` and `end`, inclusive. (`start` <= `t` <= `end` for all times `t` in self)\n",
    "#         \"\"\"\n",
    "#         start = floor(start)\n",
    "#         end = ceil(end)\n",
    "#         for k in self._data:\n",
    "#             if self._data[k] is not None:\n",
    "#                 self._data[k].trim(start, end)\n",
    "\n",
    "#     def samples(self, feature: KnownFeatures) -> Optional[np.ndarray]:\n",
    "#         \"\"\"\n",
    "#         Get inputs/data for samples in this record\n",
    "#         :param feature: `KnownFeatures` element you want\n",
    "#         :return: np.ndarray of shape (n_samples,), or (n_samples, 3) for accel, or `None` if this data not loaded.\n",
    "#         \"\"\"\n",
    "#         data_obj = self._data.get(feature.base_feature)\n",
    "#         if data_obj is None:\n",
    "#             return None\n",
    "\n",
    "#         if feature.is_activity_based:\n",
    "#             return data_obj.count\n",
    "#         elif feature.is_heartrate_based:\n",
    "#             return data_obj.heartrate\n",
    "#         elif feature.is_accelerometer_based:\n",
    "#             data_obj: AccelerometerRecord\n",
    "#             return data_obj.xyz\n",
    "\n",
    "#     def times(self, feature: KnownFeatures) -> Optional[np.ndarray]:\n",
    "#         \"\"\"\n",
    "#         Get time axis array for the given feature on this record.\n",
    "#         \"\"\"\n",
    "\n",
    "#         feature_data = self._feature_select(feature)\n",
    "#         if feature_data is None:\n",
    "#             return None\n",
    "#         return feature_data.time\n",
    "\n",
    "#     def _feature_select(self, feature: KnownFeatures) -> Optional[FeatureRecord]:\n",
    "#         if feature.is_activity_based:\n",
    "#             return self.activity_count_data\n",
    "#         elif feature.is_heartrate_based:\n",
    "#             return self.heartrate_data\n",
    "#         elif feature.is_accelerometer_based:\n",
    "#             return self.acceleration_data\n",
    "#         else:\n",
    "#             return None\n",
    "\n",
    "#     def load_data(self, load_accel: bool = False, verbose: bool = False):\n",
    "#         \"\"\"\n",
    "#         Mutating function. Loads data from locations we have. Empty CSV cells are loaded as '' which is Falsey.\n",
    "#         :param load_accel: `bool` controlling if we load accelerometer data or not. This is often huge, might crash your computer.\n",
    "#         :return: `None` since this is a self-mutation method\n",
    "#         \"\"\"\n",
    "\n",
    "#         # PSG\n",
    "#         start = time.time()\n",
    "#         if verbose:\n",
    "#             print(\"Loading PSG...\")\n",
    "#         worked = True\n",
    "#         try:\n",
    "#             self._load_psg()\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(e)\n",
    "#                 print(\"didnt load\", self.record.psg_location)\n",
    "#             worked = False\n",
    "#         if verbose and worked:\n",
    "#             print(f\"PSG loaded, took {time.time() - start:0.2f} seconds\")\n",
    "#         # Heart Rate\n",
    "#         start = time.time()\n",
    "#         if verbose:\n",
    "#             print(\"Loading Heart rate...\")\n",
    "#         worked = True\n",
    "#         try:\n",
    "#             self._load_heartrate()\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(e)\n",
    "#                 print(\"didnt load\", self.record.heartrate_location)\n",
    "#             worked = False\n",
    "#         if verbose and worked:\n",
    "#             print(f\"Heartrate loaded, took {time.time() - start:0.2f} seconds\")\n",
    "\n",
    "#         # Actigraphy\n",
    "#         start = time.time()\n",
    "#         if verbose:\n",
    "#             print(\"Loading activity counts...\")\n",
    "#         worked = True\n",
    "#         try:\n",
    "#             self._load_activity()\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(e)\n",
    "#                 print(\"didnt load\", self.record.activity_location)\n",
    "#             worked = False\n",
    "#         if verbose and worked:\n",
    "#             print(f\"Activity counts loaded, took {time.time() - start:0.2f} seconds\")\n",
    "\n",
    "#         # High-resolution Accelerometer\n",
    "#         if load_accel:\n",
    "#             start = time.time()\n",
    "#             if verbose:\n",
    "#                 print(\"Loading Accelerometer...\")\n",
    "#             try:\n",
    "#                 self._load_accel()\n",
    "#                 worked = True\n",
    "#             except Exception as e:\n",
    "#                 if verbose:\n",
    "#                     print(type(e))\n",
    "#                     print(e)\n",
    "#                     print(\"didnt load\", self.record.accelerometer_location)\n",
    "#                 worked = False\n",
    "#             if verbose and worked:\n",
    "#                 print(f\"Accelerometer loaded, took {time.time() - start:0.2f} seconds\")\n",
    "\n",
    "#     def _load_accel(self):\n",
    "#         \"\"\"\n",
    "#         Store logic for how to load a triaxial accelerometer file here!\n",
    "#         :return: AccelerometerRecord with data loaded as appropriate\n",
    "#         \"\"\"\n",
    "#         specific_path = self.data_folder.joinpath(ACCELEROMETER_FOLDER_NAME)\n",
    "#         if not self.record.accelerometer_location:\n",
    "#             # Attempt to infer based on specific path\n",
    "#             for file in os.listdir(specific_path):\n",
    "#                 if self.subject_id in file:\n",
    "#                     self.record.accelerometer_location = file\n",
    "#                     break\n",
    "#         location = specific_path.joinpath(str(self.record.accelerometer_location))\n",
    "#         # print(location)\n",
    "#         header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "#         if header_count is None or delim is None:\n",
    "#             raise ValueError(\n",
    "#                 \"The location provided could not be parsed as CSV\"\n",
    "#                 + f\"\\npath: {location.absolute()}\"\n",
    "#             )\n",
    "\n",
    "#         array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "#         self.acceleration_data = AccelerometerRecord(\n",
    "#             time=array_data[:, 0],\n",
    "#             _xyz=array_data[:, 1:],\n",
    "#         )\n",
    "\n",
    "#         return self.acceleration_data\n",
    "\n",
    "#     def _load_heartrate(self):\n",
    "#         \"\"\"\n",
    "#         Store logic for how to load a heart rate file here!\n",
    "#         :return: HeartrateRecord with data loaded as appropriate\n",
    "#         \"\"\"\n",
    "#         specific_path = self.data_folder.joinpath(HEARTRATE_FOLDER_NAME)\n",
    "#         if not self.record.heartrate_location:\n",
    "#             # Attempt to infer based on specific path\n",
    "#             for file in os.listdir(specific_path):\n",
    "#                 if self.subject_id in file:\n",
    "#                     self.record.heartrate_location = file\n",
    "#                     break\n",
    "#         location = specific_path.joinpath(str(self.record.heartrate_location))\n",
    "#         header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "#         if header_count is None or delim is None:\n",
    "#             raise ValueError(\n",
    "#                 \"The location provided could not be parsed as CSV\"\n",
    "#                 + f\"\\npath: {location.absolute()}\"\n",
    "#             )\n",
    "\n",
    "#         array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "#         self.heartrate_data = HeartrateRecord(\n",
    "#             time=array_data[:, 0], heartrate=array_data[:, 1]\n",
    "#         )\n",
    "\n",
    "#         return self.heartrate_data\n",
    "\n",
    "#     def _load_psg(self):\n",
    "#         \"\"\"\n",
    "#         Store logic for how to load a PSG 4 or 5 stage file here!\n",
    "#         :return: PSGStagingRecord with data loaded as appropriate\n",
    "#         \"\"\"\n",
    "#         specific_path = Path(self.data_folder).joinpath(PSG_FOLDER_NAME)\n",
    "#         if not self.record.psg_location:\n",
    "#             # Attempt to infer based on specific path\n",
    "#             files = os.listdir(specific_path)\n",
    "#             for file in files:\n",
    "#                 if self.subject_id in file:\n",
    "#                     self.record.psg_location = file\n",
    "#                     break\n",
    "\n",
    "#         location = specific_path.joinpath(str(self.record.psg_location))\n",
    "#         header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "#         if header_count is None or delim is None:\n",
    "#             raise ValueError(\n",
    "#                 \"The location provided could not be parsed as CSV\"\n",
    "#                 + f\"\\npath: {location.absolute()}\"\n",
    "#             )\n",
    "\n",
    "#         array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "\n",
    "#         self.psg_data = PSGStagingRecord(\n",
    "#             time=array_data[:, 0],\n",
    "#             # Safer to assume SleepStages, since W=0/S=1 still loads with .value > 0 meaning sleep,\n",
    "#             # so when we call .to_sleep_wake the correct labels will be decided.\n",
    "#             psg=[\n",
    "#                 SleepStages(int(d)) for d in array_data[:, 1]\n",
    "#             ],  # int(d) to safeguard against eg 1 vs 1.0\n",
    "#         )\n",
    "\n",
    "#         return self.psg_data\n",
    "\n",
    "#     def _load_activity(self):\n",
    "#         \"\"\"\n",
    "#         Store logic for how to load an activity count file here!\n",
    "#         :return: AccelerometerRecord with data loaded as appropriate\n",
    "#         \"\"\"\n",
    "#         specific_path = self.data_folder.joinpath(ACTIGRAPHY_FOLDER_NAME)\n",
    "#         if not self.record.activity_location:\n",
    "#             files = os.listdir(specific_path)\n",
    "#             # Attempt to infer based on specific path\n",
    "#             for file in files:\n",
    "#                 if self.subject_id in file:\n",
    "#                     self.record.activity_location = file\n",
    "#                     break\n",
    "#         location = specific_path.joinpath(str(self.record.activity_location))\n",
    "#         header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "#         if header_count is None or delim is None:\n",
    "#             raise ValueError(\n",
    "#                 \"The location provided could not be parsed as CSV\"\n",
    "#                 + f\"\\npath: {location.absolute()}\"\n",
    "#             )\n",
    "\n",
    "#         array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "#         self.activity_count_data = ActivityCountRecord(\n",
    "#             time=array_data[:, 0], count=array_data[:, 1]\n",
    "#         )\n",
    "\n",
    "#         return self.activity_count_data\n",
    "\n",
    "#     def fill_gaps(self, in_: List[KnownFeatures] = []):\n",
    "#         if len(self.psg_data.time) < 2:\n",
    "#             return\n",
    "#         psg_dt = self.psg_data.time[1] - self.psg_data.time[0]\n",
    "#         self.psg_data.fill_gaps(max_gap_seconds=psg_dt, fill_dt=psg_dt)\n",
    "\n",
    "#         in_ = [i.base_feature for i in in_]\n",
    "\n",
    "#         for feature in in_:\n",
    "#             if feature == KnownFeatures.PSG:\n",
    "#                 continue  # already done\n",
    "#             f_data = self._data.get(feature)\n",
    "#             if f_data is None:\n",
    "#                 continue\n",
    "#             for j, t in enumerate(self.psg_data.time):\n",
    "#                 # We dont just want to look for gaps,\n",
    "#                 # we want to find gaps in f_data that contain an entire PSG epoch\n",
    "#                 t_plus = f_data.time[f_data.time >= t]\n",
    "#                 has_gap_at_t = (len(t_plus) == 0) or ((t_plus[0] - t) > psg_dt)\n",
    "\n",
    "#                 if has_gap_at_t:\n",
    "#                     psg_type = type(self.psg_data.psg[j])\n",
    "#                     self.psg_data.psg[j] = psg_type.UNSCORED\n",
    "#             self._data[feature].fill_gaps(\n",
    "#                 max_gap_seconds=psg_dt, fill_dt=np.diff(f_data.time).mean()\n",
    "#             )\n",
    "\n",
    "#         for feature in self._data:\n",
    "#             # Any other data should be discarded, it is now totally out of sync with transformations\n",
    "#             if feature in [*in_, KnownFeatures.PSG]:\n",
    "#                 continue\n",
    "#             self._data[feature] = None\n",
    "\n",
    "#     def tidy(\n",
    "#         self,\n",
    "#         verbose: bool = False,\n",
    "#         acc_Hz: int = 50,\n",
    "#         mask_gaps_in: List[KnownFeatures] = [],\n",
    "#     ):\n",
    "#         # Align and aggregate activity counts, PSG, heartrate\n",
    "#         mask_gaps_in = [m.base_feature for m in mask_gaps_in]\n",
    "\n",
    "#         mask_gaps_in = list(set([KnownFeatures.PSG] + mask_gaps_in))\n",
    "\n",
    "#         if self.psg_data is None:\n",
    "#             return\n",
    "#         # All data should be conformed to PSG\n",
    "#         start_time = min(self.psg_data.time)\n",
    "#         end_time = max(self.psg_data.time)\n",
    "\n",
    "#         self.trim(start_time, end_time)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(\"start_time\", start_time)\n",
    "#             print(\"end_time\", end_time)\n",
    "#             print((end_time - start_time) // 3600, \"hours\")\n",
    "\n",
    "#         # Identify gaps in model input data, match to epochs of PSG and sub -1\n",
    "#         self.fill_gaps(in_=mask_gaps_in)\n",
    "\n",
    "#         self.bin_to_psg()\n",
    "#         if self._data.get(KnownFeatures.ACCELEROMETER) is not None:\n",
    "#             self._data[KnownFeatures.ACCELEROMETER].resample(Hz=acc_Hz)\n",
    "\n",
    "#     def bin_to_psg(self):\n",
    "#         if self.psg_data is None:\n",
    "#             raise ValueError(\"No psg data found!\")\n",
    "#         for f in self._data:\n",
    "#             f_data = self._data.get(f)\n",
    "#             if f_data is None:\n",
    "#                 continue\n",
    "#             self._data[f].bin_based_on(self.psg_data.time)\n",
    "\n",
    "#     def __str__(self):\n",
    "#         return str(self.record)\n",
    "\n",
    "#     def __getstate__(self) -> dict:\n",
    "#         # Returns lightened version of self. Sets all the data to None, retains loading info.\n",
    "#         return DataRecord(study_record=self.record).__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pisces.records import PSGStagingRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pisces\n",
    "import pkgutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  [name for _, name, _ in pkgutil.iter_modules(['pisces'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pisces:\n",
      "\n",
      "NAME\n",
      "    pisces\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _modidx\n",
      "    coooooore\n",
      "    core\n",
      "    load\n",
      "\n",
      "VERSION\n",
      "    0.0.1\n",
      "\n",
      "FILE\n",
      "    /Users/ojwalch/Documents/pisces/pisces/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pisces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
