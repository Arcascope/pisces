{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loader\n",
    "\n",
    "> Module for associating data on the disk with database entries to be queried for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "from sqlalchemy import Boolean, Column, Integer, String, create_engine, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pisces.enums import KnownFeatures\n",
    "from pisces.records import (\n",
    "    AccelerometerRecord, \n",
    "    ActivityCountRecord, \n",
    "    HeartRateRecord,\n",
    "    PSGRecord,\n",
    "    PSGStagingRecord,\n",
    "    PSGSleepWakeRecord,\n",
    "    SleepStages,\n",
    "    TimeseriesRecording\n",
    ")\n",
    "from pisces.utils import determine_header_rows_and_delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal representation of study data\n",
    "\n",
    "This pipeline is designed to load and manage the data from research studies. This data is by default located at `<pisces repo>/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Determines the names of folders inside a data set's root\n",
    "# where we expect to find specific data CSVs.\n",
    "ACCELEROMETER_FOLDER_NAME = \"cleaned_accelerometer\"\n",
    "HEARTRATE_FOLDER_NAME = \"cleaned_heartrate\"\n",
    "PSG_FOLDER_NAME = \"cleaned_psg\"\n",
    "ACTIGRAPHY_FOLDER_NAME = \"cleaned_activity\"\n",
    "\n",
    "FOLDER_NAMES = [\n",
    "    ACCELEROMETER_FOLDER_NAME,\n",
    "    HEARTRATE_FOLDER_NAME,\n",
    "    PSG_FOLDER_NAME,\n",
    "    ACTIGRAPHY_FOLDER_NAME\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class StudyRecord(Base):\n",
    "    \"\"\"\n",
    "    Class modeling the records in our sqlite database.\n",
    "\n",
    "    The attributes of this class must match the column names in NavySleepResearch/data_sets.csv. Using\n",
    "    Python's built-in CSVReader class, we get an array of dicts that are easy to turn into StudyRecord\n",
    "    objects via dict unpacking (** operator) and kwarg matching, which is a fancy way to say:\n",
    "      new_record = StudyRecord(**csv_row_dict)\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"data_sets\"\n",
    "\n",
    "    id = Column(\n",
    "        Integer, primary_key=True\n",
    "    )  # database id column, not id of patient within study\n",
    "\n",
    "    # Identification of patient and data set\n",
    "    subject_id = Column(String)\n",
    "    data_set_name = Column(String)\n",
    "\n",
    "    # Identify what data is available and where it is\n",
    "    psg_location = Column(String)\n",
    "    accelerometer_location = Column(String)\n",
    "    heartrate_location = Column(String)\n",
    "    activity_location = Column(String)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.subject_id}:{self.data_set_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DatabaseLiaison:\n",
    "    def __init__(self):\n",
    "        self.engine = create_engine(f\"sqlite:///:memory:\")\n",
    "        self.Session_maker = sessionmaker(bind=self.engine)\n",
    "        Base.metadata.create_all(\n",
    "            bind=self.engine\n",
    "        )  # makes the actual tables in database, if they don't exist\n",
    "\n",
    "    def add(self, record: StudyRecord):\n",
    "        session = self.Session_maker()\n",
    "        session.add(record)\n",
    "        session.commit()\n",
    "\n",
    "    def add_all(self, records: List[StudyRecord]):\n",
    "        session = self.Session_maker()\n",
    "        session.add_all(records)\n",
    "        session.commit()\n",
    "\n",
    "    def select(self, selector: str = \"\") -> List[StudyRecord]:\n",
    "        \"\"\"\n",
    "        Runs raw SQL on database; selector adds a WHERE clause to SELECT * FROM data_sets\n",
    "        The default \"\" selects everything, equivalent to '''SELECT * FROM data_sets'''\n",
    "        :param selector: WHERE clause contents\n",
    "        :return: List of study records matching the selector.\n",
    "        \"\"\"\n",
    "        session = self.Session_maker()\n",
    "        return session.query(StudyRecord).filter(text(selector)).all()\n",
    "\n",
    "    def to_csvDictWriterDicts(self) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Converts database to a list of dicts ready to be sent into a CSV by csv.DictWriter.\n",
    "        :return: `[dict]` one per row with keys being the columns.\n",
    "        \"\"\"\n",
    "        session = self.Session_maker()\n",
    "        records = [r.__dict__ for r in session.query(StudyRecord).all()]\n",
    "\n",
    "        for r in records:\n",
    "            # pop those keys that are not part of CSV\n",
    "            r.pop(\"id\")  # db PK, dont persist this\n",
    "            r.pop(\"_sa_instance_state\")  # artifact of SQLAlchemy class.\n",
    "\n",
    "        return records\n",
    "\n",
    "    def to_csv(self, filename: str):\n",
    "        records = self.to_csvDictWriterDicts()\n",
    "        with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "            fieldnames = [\n",
    "                \"subject_id\",\n",
    "                \"data_set_name\",\n",
    "                \"psg_location\",\n",
    "                \"accelerometer_location\",\n",
    "                \"heartrate_location\",\n",
    "                \"activity_location\",\n",
    "            ]\n",
    "\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for rec in records:\n",
    "                writer.writerow(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DataRecord:\n",
    "    \"\"\"\n",
    "    Class that understands how to take study database records and provide data to the pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, study_record: StudyRecord, data_folder: Optional[str | Path] = None\n",
    "    ):\n",
    "        self.record = study_record\n",
    "\n",
    "        self.data_root = (\n",
    "            Path(__file__).parent.parent.parent.joinpath(\"data\")\n",
    "            if data_folder is None\n",
    "            else Path(data_folder)\n",
    "        )\n",
    "        self.data_folder = self.data_root.joinpath(str(self.record.data_set_name))\n",
    "        # These will be populated by actual data that can be operated on by classifiers\n",
    "        self._data: Dict[KnownFeatures, TimeseriesRecording] = {\n",
    "            KnownFeatures.PSG: None,\n",
    "            KnownFeatures.ACCELEROMETER: None,\n",
    "            KnownFeatures.HEARTRATE: None,\n",
    "            KnownFeatures.ACTIVITY: None,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def psg_data(self) -> Union[PSGStagingRecord, PSGSleepWakeRecord] | None:\n",
    "        return self._data[KnownFeatures.PSG]\n",
    "\n",
    "    @psg_data.setter\n",
    "    def psg_data(self, value):\n",
    "        assert (value is None) or issubclass(type(value), PSGRecord)\n",
    "        self._data[KnownFeatures.PSG] = value\n",
    "\n",
    "    @property\n",
    "    def acceleration_data(self) -> AccelerometerRecord | None:\n",
    "        return self._data[KnownFeatures.ACCELEROMETER]\n",
    "\n",
    "    @acceleration_data.setter\n",
    "    def acceleration_data(self, value):\n",
    "        assert (value is None) or isinstance(value, AccelerometerRecord)\n",
    "        self._data[KnownFeatures.ACCELEROMETER] = value\n",
    "\n",
    "    @property\n",
    "    def heartrate_data(self) -> HeartRateRecord | None:\n",
    "        return self._data[KnownFeatures.HEARTRATE]\n",
    "\n",
    "    @heartrate_data.setter\n",
    "    def heartrate_data(self, value):\n",
    "        assert (value is None) or isinstance(value, HeartRateRecord)\n",
    "        self._data[KnownFeatures.HEARTRATE] = value\n",
    "\n",
    "    @property\n",
    "    def activity_count_data(self) -> ActivityCountRecord | None:\n",
    "        return self._data[KnownFeatures.ACTIVITY]\n",
    "\n",
    "    @activity_count_data.setter\n",
    "    def activity_count_data(self, value):\n",
    "        assert (value is None) or isinstance(value, ActivityCountRecord)\n",
    "        self._data[KnownFeatures.ACTIVITY] = value\n",
    "\n",
    "    @property\n",
    "    def data_set_name(self):\n",
    "        return self.record.data_set_name\n",
    "\n",
    "    @property\n",
    "    def subject_id(self):\n",
    "        return self.record.subject_id\n",
    "\n",
    "    @property\n",
    "    def absolute_accelerometer_path(self) -> str:\n",
    "        return self.data_folder.joinpath(ACCELEROMETER_FOLDER_NAME).joinpath(\n",
    "            self.record.accelerometer_location\n",
    "        )\n",
    "\n",
    "    def max_time(self, over: List[KnownFeatures] = list(KnownFeatures)) -> float:\n",
    "        times = []\n",
    "        for feature in over:\n",
    "            feat = self._data.get(feature)\n",
    "            if feat is not None:\n",
    "                times += [feat.time.max()]\n",
    "        # times is array of maximum time on each non-None feature\n",
    "        # min of maximum times gives maximum time intersection\n",
    "        return min(times) if times else np.inf\n",
    "\n",
    "    def min_time(self, over: List[KnownFeatures] = list(KnownFeatures)) -> float:\n",
    "        times = []\n",
    "        for feature in over:\n",
    "            feat = self._data.get(feature)\n",
    "            if feat is not None:\n",
    "                times += [feat.time.min()]\n",
    "        # times is array of minimum time on each non-None feature\n",
    "        # max of minimum times gives minimum time intersection\n",
    "        return max(times) if times else -1.0 * np.inf\n",
    "\n",
    "    def trim(self, start: float, end: float):\n",
    "        \"\"\"\n",
    "        Trims all of the timeseries loaded on this object to have .time values\n",
    "        between `start` and `end`, inclusive. (`start` <= `t` <= `end` for all times `t` in self)\n",
    "        \"\"\"\n",
    "        start = floor(start)\n",
    "        end = ceil(end)\n",
    "        for k in self._data:\n",
    "            if self._data[k] is not None:\n",
    "                self._data[k].trim(start, end)\n",
    "\n",
    "    def samples(self, feature: KnownFeatures) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get inputs/data for samples in this record\n",
    "        :param feature: `KnownFeatures` element you want\n",
    "        :return: np.ndarray of shape (n_samples,), or (n_samples, 3) for accel, or `None` if this data not loaded.\n",
    "        \"\"\"\n",
    "        data_obj = self._data.get(feature.base_feature)\n",
    "        if data_obj is None:\n",
    "            return None\n",
    "\n",
    "        if feature.is_activity_based:\n",
    "            return data_obj.count\n",
    "        elif feature.is_heartrate_based:\n",
    "            return data_obj.heartrate\n",
    "        elif feature.is_accelerometer_based:\n",
    "            data_obj: AccelerometerRecord\n",
    "            return data_obj.xyz\n",
    "\n",
    "    def times(self, feature: KnownFeatures) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get time axis array for the given feature on this record.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_data = self._feature_select(feature)\n",
    "        if feature_data is None:\n",
    "            return None\n",
    "        return feature_data.time\n",
    "\n",
    "    def _feature_select(self, feature: KnownFeatures) -> Optional[TimeseriesRecording]:\n",
    "        if feature.is_activity_based:\n",
    "            return self.activity_count_data\n",
    "        elif feature.is_heartrate_based:\n",
    "            return self.heartrate_data\n",
    "        elif feature.is_accelerometer_based:\n",
    "            return self.acceleration_data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def load_data(self, load_accel: bool = False, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Mutating function. Loads data from locations we have. Empty CSV cells are loaded as '' which is Falsey.\n",
    "        :param load_accel: `bool` controlling if we load accelerometer data or not. This is often huge, might crash your computer.\n",
    "        :return: `None` since this is a self-mutation method\n",
    "        \"\"\"\n",
    "\n",
    "        # PSG\n",
    "        start = time.time()\n",
    "        if verbose:\n",
    "            print(\"Loading PSG...\")\n",
    "        worked = True\n",
    "        try:\n",
    "            self._load_psg()\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(e)\n",
    "                print(\"didnt load\", self.record.psg_location)\n",
    "            worked = False\n",
    "        if verbose and worked:\n",
    "            print(f\"PSG loaded, took {time.time() - start:0.2f} seconds\")\n",
    "        # Heart Rate\n",
    "        start = time.time()\n",
    "        if verbose:\n",
    "            print(\"Loading Heart rate...\")\n",
    "        worked = True\n",
    "        try:\n",
    "            self._load_heartrate()\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(e)\n",
    "                print(\"didnt load\", self.record.heartrate_location)\n",
    "            worked = False\n",
    "        if verbose and worked:\n",
    "            print(f\"Heartrate loaded, took {time.time() - start:0.2f} seconds\")\n",
    "\n",
    "        # Actigraphy\n",
    "        start = time.time()\n",
    "        if verbose:\n",
    "            print(\"Loading activity counts...\")\n",
    "        worked = True\n",
    "        try:\n",
    "            self._load_activity()\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(e)\n",
    "                print(\"didnt load\", self.record.activity_location)\n",
    "            worked = False\n",
    "        if verbose and worked:\n",
    "            print(f\"Activity counts loaded, took {time.time() - start:0.2f} seconds\")\n",
    "\n",
    "        # High-resolution Accelerometer\n",
    "        if load_accel:\n",
    "            start = time.time()\n",
    "            if verbose:\n",
    "                print(\"Loading Accelerometer...\")\n",
    "            try:\n",
    "                self._load_accel()\n",
    "                worked = True\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(type(e))\n",
    "                    print(e)\n",
    "                    print(\"didnt load\", self.record.accelerometer_location)\n",
    "                worked = False\n",
    "            if verbose and worked:\n",
    "                print(f\"Accelerometer loaded, took {time.time() - start:0.2f} seconds\")\n",
    "\n",
    "    def _load_accel(self):\n",
    "        \"\"\"\n",
    "        Store logic for how to load a triaxial accelerometer file here!\n",
    "        :return: AccelerometerRecord with data loaded as appropriate\n",
    "        \"\"\"\n",
    "        specific_path = self.data_folder.joinpath(ACCELEROMETER_FOLDER_NAME)\n",
    "        if not self.record.accelerometer_location:\n",
    "            # Attempt to infer based on specific path\n",
    "            for file in os.listdir(specific_path):\n",
    "                if self.subject_id in file:\n",
    "                    self.record.accelerometer_location = file\n",
    "                    break\n",
    "        location = specific_path.joinpath(str(self.record.accelerometer_location))\n",
    "        # print(location)\n",
    "        header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "        if header_count is None or delim is None:\n",
    "            raise ValueError(\n",
    "                \"The location provided could not be parsed as CSV\"\n",
    "                + f\"\\npath: {location.absolute()}\"\n",
    "            )\n",
    "\n",
    "        array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "        self.acceleration_data = AccelerometerRecord(\n",
    "            time=array_data[:, 0],\n",
    "            _xyz=array_data[:, 1:],\n",
    "        )\n",
    "\n",
    "        return self.acceleration_data\n",
    "\n",
    "    def _load_heartrate(self):\n",
    "        \"\"\"\n",
    "        Store logic for how to load a heart rate file here!\n",
    "        :return: HeartrateRecord with data loaded as appropriate\n",
    "        \"\"\"\n",
    "        specific_path = self.data_folder.joinpath(HEARTRATE_FOLDER_NAME)\n",
    "        if not self.record.heartrate_location:\n",
    "            # Attempt to infer based on specific path\n",
    "            for file in os.listdir(specific_path):\n",
    "                if self.subject_id in file:\n",
    "                    self.record.heartrate_location = file\n",
    "                    break\n",
    "        location = specific_path.joinpath(str(self.record.heartrate_location))\n",
    "        header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "        if header_count is None or delim is None:\n",
    "            raise ValueError(\n",
    "                \"The location provided could not be parsed as CSV\"\n",
    "                + f\"\\npath: {location.absolute()}\"\n",
    "            )\n",
    "\n",
    "        array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "        self.heartrate_data = HeartRateRecord(\n",
    "            time=array_data[:, 0], heartrate=array_data[:, 1]\n",
    "        )\n",
    "\n",
    "        return self.heartrate_data\n",
    "\n",
    "    def _load_psg(self):\n",
    "        \"\"\"\n",
    "        Store logic for how to load a PSG 4 or 5 stage file here!\n",
    "        :return: PSGStagingRecord with data loaded as appropriate\n",
    "        \"\"\"\n",
    "        specific_path = Path(self.data_folder).joinpath(PSG_FOLDER_NAME)\n",
    "        if not self.record.psg_location:\n",
    "            # Attempt to infer based on specific path\n",
    "            files = os.listdir(specific_path)\n",
    "            for file in files:\n",
    "                if self.subject_id in file:\n",
    "                    self.record.psg_location = file\n",
    "                    break\n",
    "\n",
    "        location = specific_path.joinpath(str(self.record.psg_location))\n",
    "        header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "        if header_count is None or delim is None:\n",
    "            raise ValueError(\n",
    "                \"The location provided could not be parsed as CSV\"\n",
    "                + f\"\\npath: {location.absolute()}\"\n",
    "            )\n",
    "\n",
    "        array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "\n",
    "        self.psg_data = PSGStagingRecord(\n",
    "            time=array_data[:, 0],\n",
    "            # Safer to assume SleepStages, since W=0/S=1 still loads with .value > 0 meaning sleep,\n",
    "            # so when we call .to_sleep_wake the correct labels will be decided.\n",
    "            psg=[\n",
    "                SleepStages(int(d)) for d in array_data[:, 1]\n",
    "            ],  # int(d) to safeguard against eg 1 vs 1.0\n",
    "        )\n",
    "\n",
    "        return self.psg_data\n",
    "\n",
    "    def _load_activity(self):\n",
    "        \"\"\"\n",
    "        Store logic for how to load an activity count file here!\n",
    "        :return: AccelerometerRecord with data loaded as appropriate\n",
    "        \"\"\"\n",
    "        specific_path = self.data_folder.joinpath(ACTIGRAPHY_FOLDER_NAME)\n",
    "        if not self.record.activity_location:\n",
    "            files = os.listdir(specific_path)\n",
    "            # Attempt to infer based on specific path\n",
    "            for file in files:\n",
    "                if self.subject_id in file:\n",
    "                    self.record.activity_location = file\n",
    "                    break\n",
    "        location = specific_path.joinpath(str(self.record.activity_location))\n",
    "        header_count, delim = determine_header_rows_and_delimiter(location)\n",
    "        if header_count is None or delim is None:\n",
    "            raise ValueError(\n",
    "                \"The location provided could not be parsed as CSV\"\n",
    "                + f\"\\npath: {location.absolute()}\"\n",
    "            )\n",
    "\n",
    "        array_data = np.loadtxt(location, skiprows=header_count, delimiter=delim)\n",
    "        self.activity_count_data = ActivityCountRecord(\n",
    "            time=array_data[:, 0], count=array_data[:, 1]\n",
    "        )\n",
    "\n",
    "        return self.activity_count_data\n",
    "\n",
    "    def fill_gaps(self, in_: List[KnownFeatures] = []):\n",
    "        if len(self.psg_data.time) < 2:\n",
    "            return\n",
    "        psg_dt = self.psg_data.time[1] - self.psg_data.time[0]\n",
    "        self.psg_data.fill_gaps(max_gap_seconds=psg_dt, fill_dt=psg_dt)\n",
    "\n",
    "        in_ = [i.base_feature for i in in_]\n",
    "\n",
    "        for feature in in_:\n",
    "            if feature == KnownFeatures.PSG:\n",
    "                continue  # already done\n",
    "            f_data = self._data.get(feature)\n",
    "            if f_data is None:\n",
    "                continue\n",
    "            for j, t in enumerate(self.psg_data.time):\n",
    "                # We dont just want to look for gaps,\n",
    "                # we want to find gaps in f_data that contain an entire PSG epoch\n",
    "                t_plus = f_data.time[f_data.time >= t]\n",
    "                has_gap_at_t = (len(t_plus) == 0) or ((t_plus[0] - t) > psg_dt)\n",
    "\n",
    "                if has_gap_at_t:\n",
    "                    psg_type = type(self.psg_data.psg[j])\n",
    "                    self.psg_data.psg[j] = psg_type.UNSCORED\n",
    "            self._data[feature].fill_gaps(\n",
    "                max_gap_seconds=psg_dt, fill_dt=np.diff(f_data.time).mean()\n",
    "            )\n",
    "\n",
    "        for feature in self._data:\n",
    "            # Any other data should be discarded, it is now totally out of sync with transformations\n",
    "            if feature in [*in_, KnownFeatures.PSG]:\n",
    "                continue\n",
    "            self._data[feature] = None\n",
    "\n",
    "    def tidy(\n",
    "        self,\n",
    "        verbose: bool = False,\n",
    "        acc_Hz: int = 50,\n",
    "        mask_gaps_in: List[KnownFeatures] = [],\n",
    "    ):\n",
    "        # Align and aggregate activity counts, PSG, heartrate\n",
    "        mask_gaps_in = [m.base_feature for m in mask_gaps_in]\n",
    "\n",
    "        mask_gaps_in = list(set([KnownFeatures.PSG] + mask_gaps_in))\n",
    "\n",
    "        if self.psg_data is None:\n",
    "            return\n",
    "        # All data should be conformed to PSG\n",
    "        start_time = min(self.psg_data.time)\n",
    "        end_time = max(self.psg_data.time)\n",
    "\n",
    "        self.trim(start_time, end_time)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"start_time\", start_time)\n",
    "            print(\"end_time\", end_time)\n",
    "            print((end_time - start_time) // 3600, \"hours\")\n",
    "\n",
    "        # Identify gaps in model input data, match to epochs of PSG and sub -1\n",
    "        self.fill_gaps(in_=mask_gaps_in)\n",
    "\n",
    "        self.bin_to_psg()\n",
    "        if self._data.get(KnownFeatures.ACCELEROMETER) is not None:\n",
    "            self._data[KnownFeatures.ACCELEROMETER].resample(Hz=acc_Hz)\n",
    "\n",
    "    def bin_to_psg(self):\n",
    "        if self.psg_data is None:\n",
    "            raise ValueError(\"No psg data found!\")\n",
    "        for f in self._data:\n",
    "            f_data = self._data.get(f)\n",
    "            if f_data is None:\n",
    "                continue\n",
    "            self._data[f].bin_based_on(self.psg_data.time)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.record)\n",
    "\n",
    "    def __getstate__(self) -> dict:\n",
    "        # Returns lightened version of self. Sets all the data to None, retains loading info.\n",
    "        return DataRecord(study_record=self.record).__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class DataPool:\n",
    "    \"\"\"\n",
    "    Class handling loading of records, providing them in a prefix-searchable way to other resources.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, records: List[DataRecord], load: bool = False):\n",
    "        self.records: List[DataRecord] = records\n",
    "\n",
    "        self.is_loaded = False\n",
    "        if load:\n",
    "            self.load()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        tidy: bool = True,\n",
    "        load_accel: bool = False,\n",
    "        verbose: bool = False,\n",
    "        acc_Hz: int = 50,\n",
    "        tidy_features: List[KnownFeatures] = [],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loads the data into memory from the disk locations specified in self.records\n",
    "        :param load_accel: Do you want to load the accelerometer data? this is huge, slow, might crash your computer\n",
    "        :return: `None` just self-mutation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        n_records = len(self.records)\n",
    "        for j in range(n_records):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"({time.time() - start:0.3f} sec) Loading {j+1}/{n_records} ({self.records[j]})\"\n",
    "                )\n",
    "            self.records[j].load_data(load_accel=load_accel, verbose=verbose)\n",
    "            if verbose:\n",
    "                print(\"=====================================\\n\")\n",
    "        self.is_loaded = True\n",
    "\n",
    "        if tidy:\n",
    "            self.tidy(verbose=verbose, acc_Hz=acc_Hz)\n",
    "\n",
    "    def tidy(self, verbose: bool = False, acc_Hz=50):\n",
    "        n_records = len(self.records)\n",
    "        for j in range(n_records):\n",
    "            self.records[j].tidy(\n",
    "                verbose,\n",
    "                acc_Hz=acc_Hz,\n",
    "                # Just do it for everything we've got, always.\n",
    "                mask_gaps_in=list(KnownFeatures),\n",
    "            )\n",
    "\n",
    "    def _align_times_by_index(self, idxs: List[int]):\n",
    "        if len(set(idxs)) <= 1:\n",
    "            # empty/Only one distinct index, already aligned with itself\n",
    "            return\n",
    "        if max(idxs) >= len(self.records):\n",
    "            raise IndexError(f\"max(idxs) = {max(idxs)} >= {len(self.records)}\")\n",
    "        min_time = max([self.records[idx].min_time for idx in idxs])\n",
    "        max_time = min([self.records[idx].max_time for idx in idxs])\n",
    "        for idx in idxs:\n",
    "            self.records[idx].trim(min_time, max_time)\n",
    "\n",
    "    def align_subject_versions(self, subject_id: str):\n",
    "        self._align_times_by_index(\n",
    "            [\n",
    "                j\n",
    "                for j in range(len(self.records))\n",
    "                if self.records[j].subject_id == subject_id\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_record(self, subject_id: str) -> Optional[DataRecord]:\n",
    "        \"\"\"\n",
    "        Allows caller to get record for provided subject_id,\n",
    "        or get None when that record is not found.\n",
    "        :param subject_id: The subject id as found in data sets CSV, eg 1234 or 1234:some_data\n",
    "        :return: `DataRecord` for that subject if found, or `None`\n",
    "        \"\"\"\n",
    "        for record in self.records:\n",
    "            if str(record).startswith(subject_id):\n",
    "                return record\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "        A pool of data with {len(self.records)} distinct people from the following known data sets:\n",
    "        {\", \".join(list(set([record.data_set_name for record in self.records])))}\n",
    "        totaling {sum([len(x.psg_data) for x in self.records if x.psg_data is not None])} labeled PSG epochs.\n",
    "        \"\"\"\n",
    "\n",
    "    def __getstate__(self) -> dict:\n",
    "        selfCopy = self\n",
    "        selfCopy.is_loaded = False\n",
    "\n",
    "        return selfCopy.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#| export\n",
    "\n",
    "def make_datapool(data_folder: str) -> DataPool:\n",
    "    # Instantiate the tools for database management.\n",
    "    liaison =  make_database()\n",
    "\n",
    "    records_to_use = liaison.records\n",
    "\n",
    "    return DataPool(\n",
    "        records=[DataRecord(rec, data_folder=data_folder) for rec in records_to_use],\n",
    "        load=False,\n",
    "    )\n",
    "\n",
    "def figure_out_data_sets_paths(data_folder: str) -> List[dict]:\n",
    "    data_sets_found = []\n",
    "    for root, dirs, files in os.walk(data_folder):\n",
    "        if any(folder_name in dirs for folder_name in FOLDER_NAMES):\n",
    "            data_sets_found.append(root)\n",
    "            # Prevent descending into subdirectories\n",
    "            del dirs[:]\n",
    "    return data_sets_found\n",
    "\n",
    "def figure_out_data_set_individuals(data_sets_found: List[str], data_folder) -> List[dict]:\n",
    "    for dataset in data_sets_found:\n",
    "        path_for_set = Path(data_folder).joinpath(dataset)\n",
    "        \n",
    "\n",
    "\n",
    "def make_database(data_folder: str) -> DatabaseLiaison:\n",
    "\n",
    "    data_sets_found = figure_out_data_sets_paths(data_folder)\n",
    "    rows = figure_out_data_set_individuals(data_sets_found, data_folder)\n",
    "\n",
    "    # \n",
    "    liaison = DatabaseLiaison()\n",
    "\n",
    "\n",
    "    records = [StudyRecord(**row) for row in rows]\n",
    "\n",
    "    liaison.add_all(records=records)\n",
    "\n",
    "    return liaison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Function to create directories and files\n",
    "def create_structure(base, structure):\n",
    "    for dir_name, content in structure.items():\n",
    "        # Create directory\n",
    "        dir_path = os.path.join(base, dir_name)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        for subdir in content['subdirs']:\n",
    "            os.makedirs(os.path.join(dir_path, subdir), exist_ok=True)\n",
    "        \n",
    "        # Create files\n",
    "        for subdir, files in content['files'].items():\n",
    "            subdir_path = os.path.join(dir_path, subdir)\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(subdir_path, file_name)\n",
    "                # Simply create the file\n",
    "                with open(file_path, 'w') as f:\n",
    "                    f.write('')  # Write an empty string to create the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "class DataSetScanner:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_sets, self.data_set_csvs = self._scan_data_sets()\n",
    "    \n",
    "    def _scan_data_sets(self):\n",
    "        data_sets = []\n",
    "        data_set_csvs = []\n",
    "        for root_dir, dirs, files in os.walk(self.data_dir):\n",
    "            # Check if root_dir is a data set directory\n",
    "            if self._is_data_set_dir(root_dir):\n",
    "                # print(f\"Found data set directory: {root_dir}\")\n",
    "                data_sets.append(root_dir)\n",
    "                for sub_dir in dirs:\n",
    "                    if sub_dir.startswith(\"cleaned_\"):\n",
    "                        cleaned_dir = os.path.join(root_dir, sub_dir)\n",
    "                        csv_files = [os.path.join(cleaned_dir, f) for f in os.listdir(cleaned_dir) if f.endswith('.csv')]\n",
    "                        data_set_csvs.extend(csv_files)\n",
    "            else:\n",
    "                pass\n",
    "                # print(f\"Skipping {root_dir} as it does not appear to be a data set directory\")\n",
    "        return data_sets, data_set_csvs\n",
    "\n",
    "    def _is_data_set_dir(self, dir_path):\n",
    "        required_dirs = [\"cleaned_psg\"]\n",
    "        optional_dirs = [\"cleaned_activity\", \"cleaned_motion\"]\n",
    "        has_required = True\n",
    "        has_optional = True\n",
    "\n",
    "        sub_dirs = next(os.walk(dir_path))[1]\n",
    "        for required_dir in required_dirs:\n",
    "            if required_dir not in sub_dirs:\n",
    "                # print(f\"didn't Find required directory {required_dir}\")\n",
    "                has_required = False \n",
    "            else:\n",
    "                break\n",
    "                # print(f\"Found required directory {required_dir}\")\n",
    "\n",
    "        # for optional_dir in optional_dirs:\n",
    "        #     if optional_dir in sub_dirs:\n",
    "        #         has_optional = True\n",
    "        #         break\n",
    "\n",
    "        return has_required\n",
    "\n",
    "    def get_data_sets(self):\n",
    "        return self.data_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test \"find data sets in a directory\"\n",
    "\n",
    "# Define the base path\n",
    "test_dir_name = \"/test_psg_required_data\"\n",
    "base_path = os.getcwd() + test_dir_name\n",
    "\n",
    "# Define the structure of directories and files to be created\n",
    "structure = {\n",
    "    'study_x123': {\n",
    "        'subdirs': ['cleaned_psg', 'cleaned_activity', 'cleaned_ekg'],\n",
    "        'files': {\n",
    "            'cleaned_psg': ['data1.csv', 'data2.csv'],\n",
    "            'cleaned_activity': ['activity1.csv'],\n",
    "            'cleaned_ekg': []\n",
    "        }\n",
    "    },\n",
    "    'study_x456': {\n",
    "        'subdirs': ['cleaned_psg'],\n",
    "        'files': {\n",
    "            'cleaned_psg': ['data3.csv']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# Create the directory and file structure\n",
    "create_structure(base_path, structure)\n",
    "\n",
    "scanner = DataSetScanner(base_path)\n",
    "data_sets = scanner.get_data_sets()\n",
    "\n",
    "# Remove the created directory and files\n",
    "shutil.rmtree(base_path)\n",
    "expected_len = 2\n",
    "try:\n",
    "    assert len(data_sets) == expected_len  # Adjust assertion as necessary\n",
    "except AssertionError:\n",
    "    print(data_sets)\n",
    "    raise AssertionError(f\"Expected {expected_len} data sets, but got {len(data_sets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test \"skip data sets in a directory\"\n",
    "\n",
    "# Define the base path\n",
    "test_dir_name = \"/test_skip_data\"\n",
    "base_path = os.getcwd() + test_dir_name\n",
    "\n",
    "# Define the structure of directories and files to be created\n",
    "structure = {\n",
    "    'study_x123': {\n",
    "        'subdirs': ['cleaned_psg', 'cleaned_activity', 'cleaned_ekg'],\n",
    "        'files': {\n",
    "            'cleaned_psg': ['data1.csv', 'data2.csv'],\n",
    "            'cleaned_activity': ['activity1.csv'],\n",
    "            'cleaned_ekg': []\n",
    "        }\n",
    "    },\n",
    "    'study_x456': {\n",
    "        'subdirs': ['cleaned_activity'],\n",
    "        'files': {\n",
    "            'cleaned_activity': ['data3.csv']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# Create the directory and file structure\n",
    "create_structure(base_path, structure)\n",
    "\n",
    "scanner = DataSetScanner(base_path)\n",
    "data_sets = scanner.get_data_sets()\n",
    "\n",
    "# Remove the created directory and files\n",
    "shutil.rmtree(base_path)\n",
    "expected_len = 1\n",
    "try:\n",
    "    assert len(data_sets) == expected_len  # Adjust assertion as necessary\n",
    "except AssertionError:\n",
    "    print(data_sets)\n",
    "    raise AssertionError(f\"Expected {expected_len} data sets, but got {len(data_sets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
