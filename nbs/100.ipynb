{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models\n",
    "\n",
    "> This module defines the base interface of machine learning models in this library. To add a model architecture, or modify an existing one, subclass `MLModelWithTransforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class TorchDeviceEnum(Enum):\n",
    "    cpu = auto()\n",
    "    cuda = auto()\n",
    "    metal = auto()\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        if self == TorchDeviceEnum.cpu:\n",
    "            return torch.device(\"cpu\")\n",
    "        elif self == TorchDeviceEnum.cuda:\n",
    "            return torch.device(\"cuda:0\")\n",
    "        elif self == TorchDeviceEnum.metal:\n",
    "            return torch.device(\"mps\")\n",
    "\n",
    "\n",
    "class AutoConfiguringTorchDevice:\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self._dev_enum = TorchDeviceEnum.cuda\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self._dev_enum = TorchDeviceEnum.metal\n",
    "        else:\n",
    "            self._dev_enum = TorchDeviceEnum.cpu\n",
    "\n",
    "        self.device = self._dev_enum.device\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> torch.device:\n",
    "        return self.device\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self._dev_enum == TorchDeviceEnum.cpu:\n",
    "            return \"CPU\"\n",
    "        elif self._dev_enum == TorchDeviceEnum.cuda:\n",
    "            return \"CUDA-enabled GPU (NVidia)\"\n",
    "        elif self._dev_enum == TorchDeviceEnum.metal:\n",
    "            return \"Metal-enabled GPU (Apple)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pisces.enums import KnownModel\n",
    "\n",
    "\n",
    "class MLModelWithTransforms:\n",
    "    def __init__(self) -> None:\n",
    "        self.device = AutoConfiguringTorchDevice().device\n",
    "\n",
    "    @property\n",
    "    def model_type(self) -> KnownModel:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "m = MLModelWithTransforms()\n",
    "print(m.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
