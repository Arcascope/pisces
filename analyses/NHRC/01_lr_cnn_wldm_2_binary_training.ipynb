{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation, data loading, and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Mads Olsen model to Keras...\n",
      "Model saved at /Users/eric/Engineering/Work/pisces/pisces/cached_models/mo_resunet.keras\n"
     ]
    }
   ],
   "source": [
    "!pisces_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_KERNEL_SIZE: 61\n"
     ]
    }
   ],
   "source": [
    "from nhrc_utils.analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import tensorflow as tf\n",
    "import pisces.models as pm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CWD = Path(os.getcwd())\n",
    "# NB! these are different from preprocess_data.ipynb\n",
    "save_path = CWD.joinpath(\"fine_tuning\")\n",
    "\n",
    "hybrid_path = save_path.joinpath(\"hybrid\")\n",
    "os.makedirs(hybrid_path, exist_ok=True)\n",
    "disordered_path = save_path.joinpath(\"disordered\")\n",
    "os.makedirs(disordered_path, exist_ok=True)\n",
    "walch_path = save_path.joinpath(\"walch\")\n",
    "os.makedirs(walch_path, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data tensors for training + evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    }
   ],
   "source": [
    "dataset = \"stationary\"\n",
    "acc_hz = \"50\"\n",
    "walch_preprocessed_data = np.load(f'./pre_processed_data/{dataset}/{dataset}_preprocessed_data_{acc_hz}.npy',\n",
    "                                   allow_pickle=True).item()\n",
    "walch_keys = list(walch_preprocessed_data.keys())\n",
    "walch_data_bundle = prepare_data(walch_preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([28, 1024, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walch_data_bundle.mo_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([28, 15360, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walch_data_bundle.spectrogram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LR CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Set up separate log directories for each model\n",
    "log_dir_lr = f\"./logs/lr_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "log_dir_cnn = f\"./logs/cnn_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Configure TensorBoard callback\n",
    "cnn_tensorboard_callback = TensorBoard(log_dir=log_dir_cnn, histogram_freq=1)\n",
    "lr_tensorboard_callback = TensorBoard(log_dir=log_dir_lr, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LR CNN models...\n",
      "Saving models to /Users/eric/Engineering/Work/pisces/analyses/NHRC/evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Next split: 100%|██████████| 28/28 [01:46<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from analyses.NHRC.nhrc_utils.model_definitions import LR_CNN_NAME,  LABEL_SHAPE, LR_INPUT_SHAPE, WeightedModel, build_lr_cnn\n",
    "\n",
    "\n",
    "split_maker = pm.LeaveOneOutSplitter()\n",
    "\n",
    "training_results = []\n",
    "lr_predictors = []\n",
    "\n",
    "print(f\"Training {LR_CNN_NAME} models...\")\n",
    "print(\"Saving models to\", DEFAULT_EVALUATION_DIR)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "for k_train, k_test in tqdm(split_maker.split(walch_keys), desc=\"Next split\", total=len(walch_keys)):\n",
    "    # Convert indices to tensors\n",
    "    train_idx_tensor = tf.constant(k_train, dtype=tf.int32)\n",
    "    test_idx_tensor = tf.constant(k_test, dtype=tf.int32)\n",
    "\n",
    "    # Gather the training and validation data using tf.gather\n",
    "    # training\n",
    "    train_data = tf.reshape(\n",
    "        tf.gather(walch_data_bundle.activity, train_idx_tensor),\n",
    "        LR_INPUT_SHAPE)\n",
    "    train_labels = tf.reshape(\n",
    "        tf.gather(walch_data_bundle.true_labels, train_idx_tensor),\n",
    "        LABEL_SHAPE)\n",
    "    train_sample_weights = tf.reshape(\n",
    "        tf.gather(walch_data_bundle.sample_weights, train_idx_tensor),\n",
    "        LABEL_SHAPE)\n",
    "\n",
    "    # make the labels binary, -1 -> 0\n",
    "    # since we incorporate the mask in the sample weights, we can just set the labels to 0\n",
    "    train_labels_masked = tf.where(train_sample_weights > 0, train_labels, 0.0)\n",
    "\n",
    "    # z-normalize input data\n",
    "    train_data = (train_data - tf.reduce_mean(train_data)) / np.std(train_data)\n",
    "\n",
    "\n",
    "    # Custom loss function that includes the sample weights\n",
    "    lr_cnn = build_lr_cnn()\n",
    "    weighted_lr_cnn = WeightedModel(lr_cnn)\n",
    "    weighted_lr_cnn.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3),\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_data, train_labels_masked, train_sample_weights))\n",
    "    dataset = dataset.batch(32)\n",
    "    training_results.append(weighted_lr_cnn.fit(\n",
    "        dataset,\n",
    "        epochs=350, \n",
    "        verbose=0,\n",
    "        callbacks=[lr_tensorboard_callback]\n",
    "    ))\n",
    "\n",
    "\n",
    "    lr_predictors.append(lr_cnn)\n",
    "    lr_path = make_lr_filename(walch_keys[k_test[0]])\n",
    "    lr_cnn.save(lr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fine Tuning models...\n",
      "Saving models to /Users/eric/Engineering/Work/pisces/analyses/NHRC/evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Next split: 100%|██████████| 28/28 [04:42<00:00, 10.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from analyses.NHRC.nhrc_utils.model_definitions import FINETUNING_INPUT_SHAPE, build_finetuning_model, EXTRA_LAYERS_NAME\n",
    "\n",
    "split_maker = pm.LeaveOneOutSplitter()\n",
    "\n",
    "training_results = []\n",
    "evaluations = []\n",
    "cnn_predictors = []\n",
    "\n",
    "print(f\"Training {EXTRA_LAYERS_NAME} models...\")\n",
    "print(\"Saving models to\", DEFAULT_EVALUATION_DIR)\n",
    "\n",
    "def finetuning_gather_reshape(data_bundle: PreparedData, train_idx_tensor: tf.Tensor, input_shape: tuple = FINETUNING_INPUT_SHAPE, output_shape: tuple = LABEL_SHAPE) -> tuple | None:\n",
    "    train_data = tf.reshape(\n",
    "        tf.gather(data_bundle.mo_predictions, train_idx_tensor),\n",
    "        input_shape\n",
    "        )\n",
    "    train_labels = tf.reshape(\n",
    "        tf.gather(data_bundle.true_labels, train_idx_tensor),\n",
    "        output_shape)\n",
    "    train_sample_weights = tf.reshape(\n",
    "        tf.gather(data_bundle.sample_weights, train_idx_tensor),\n",
    "        output_shape)\n",
    "    return train_data, train_labels, train_sample_weights\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "for k_train, k_test in tqdm(split_maker.split(walch_keys), desc=\"Next split\", total=len(walch_keys)):\n",
    "    # Convert indices to tensors\n",
    "    train_idx_tensor = tf.constant(k_train, dtype=tf.int32)\n",
    "    test_idx_tensor = tf.constant(k_test, dtype=tf.int32)\n",
    "\n",
    "    # Gather the training and validation data using tf.gather\n",
    "    # training\n",
    "    train_data, train_labels, train_sample_weights = finetuning_gather_reshape(walch_data_bundle, train_idx_tensor)\n",
    "\n",
    "    # make the labels binary, -1 -> 0\n",
    "    # since we incorporate the mask in the sample weights, we can just set the labels to 0\n",
    "    train_labels_masked = tf.where(train_sample_weights > 0, train_labels, 0.0)\n",
    "\n",
    "    # Train the model on the training set\n",
    "    cnn = build_finetuning_model(FINETUNING_INPUT_SHAPE[1:])\n",
    "    \n",
    "    cnn.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=5e-4),\n",
    "    )\n",
    "\n",
    "    # gives weight 0 to -1 \"mask\" intervals, 1 to the rest\n",
    "\n",
    "\n",
    "    # make the labels binary, -1 -> 0\n",
    "    # since we incorporate the mask in the sample weights,\n",
    "    # we can just set the labels to 0\n",
    "    train_labels_masked = np.where(train_sample_weights, train_labels, 0)\n",
    "\n",
    "    training_results.append(cnn.fit(\n",
    "        train_data, train_labels_masked, \n",
    "        epochs=100, \n",
    "        validation_split=0.0, \n",
    "        batch_size=1,\n",
    "        sample_weight=train_sample_weights,\n",
    "        verbose=0,\n",
    "        callbacks=[cnn_tensorboard_callback]\n",
    "    ))\n",
    "\n",
    "\n",
    "    cnn_predictors.append(cnn)\n",
    "    cnn_path = make_finetuning_filename(walch_keys[k_test[0]])\n",
    "\n",
    "    # save the trained model weights\n",
    "    cnn.save(cnn_path)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pisces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
